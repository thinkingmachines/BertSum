{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTSUM_fine-tuning_forwardpass_api_function.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinkingmachines/BertSum/blob/master/experiments/BERTSUM_fine_tuning_forwardpass_api_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re1fJcNGNeLA",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Ci0MfBqiL8",
        "colab_type": "code",
        "outputId": "ca64324a-6e10-48cb-b3ae-fe942531772e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install googledrivedownloader\n",
        "!pip install pyrouge\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.157)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.157 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.157)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.157->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.157->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.157->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.4)\n",
            "Collecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXP9_LQCZ5J9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_from_gdrive(file_id, destination_path, unzip=False):\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "    gdd.download_file_from_google_drive(file_id=file_id,\n",
        "                                        dest_path=destination_path,\n",
        "                                        unzip=unzip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJLajp7X1fPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6TrpXrOvcGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78GtKGHuotRU",
        "colab_type": "code",
        "outputId": "ed2bec06-75a0-4e23-fab5-54f1dd72338c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /home"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AG506Xso0yG",
        "colab_type": "code",
        "outputId": "535c7547-7c6f-4e75-ec2d-f1f7cd3ff7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/thinkingmachines/BertSum.git"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'BertSum' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uoSgF4e_PCb",
        "colab_type": "text"
      },
      "source": [
        "## pyrouge setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl9kUGHasNFq",
        "colab_type": "code",
        "outputId": "07bfc582-1b04-459f-8b00-0cabc62e356b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /home"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIFVL8kGzLi_",
        "colab_type": "code",
        "outputId": "f1f468fb-94fe-4578-fa50-5825dcb8d30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/andersjo/pyrouge.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyrouge'...\n",
            "remote: Enumerating objects: 393, done.\u001b[K\n",
            "Receiving objects:   0% (1/393)   \rReceiving objects:   1% (4/393)   \rReceiving objects:   2% (8/393)   \rReceiving objects:   3% (12/393)   \rReceiving objects:   4% (16/393)   \rReceiving objects:   5% (20/393)   \rReceiving objects:   6% (24/393)   \rReceiving objects:   7% (28/393)   \rReceiving objects:   8% (32/393)   \rReceiving objects:   9% (36/393)   \rReceiving objects:  10% (40/393)   \rReceiving objects:  11% (44/393)   \rReceiving objects:  12% (48/393)   \rReceiving objects:  13% (52/393)   \rReceiving objects:  14% (56/393)   \rReceiving objects:  15% (59/393)   \rReceiving objects:  16% (63/393)   \rReceiving objects:  17% (67/393)   \rReceiving objects:  18% (71/393)   \rReceiving objects:  19% (75/393)   \rReceiving objects:  20% (79/393)   \rReceiving objects:  21% (83/393)   \rReceiving objects:  22% (87/393)   \rReceiving objects:  23% (91/393)   \rReceiving objects:  24% (95/393)   \rReceiving objects:  25% (99/393)   \rReceiving objects:  26% (103/393)   \rReceiving objects:  27% (107/393)   \rReceiving objects:  28% (111/393)   \rReceiving objects:  29% (114/393)   \rReceiving objects:  30% (118/393)   \rReceiving objects:  31% (122/393)   \rReceiving objects:  32% (126/393)   \rReceiving objects:  33% (130/393)   \rReceiving objects:  34% (134/393)   \rReceiving objects:  35% (138/393)   \rReceiving objects:  36% (142/393)   \rReceiving objects:  37% (146/393)   \rReceiving objects:  38% (150/393)   \rReceiving objects:  39% (154/393)   \rReceiving objects:  40% (158/393)   \rReceiving objects:  41% (162/393)   \rReceiving objects:  42% (166/393)   \rReceiving objects:  43% (169/393)   \rReceiving objects:  44% (173/393)   \rReceiving objects:  45% (177/393)   \rReceiving objects:  46% (181/393)   \rReceiving objects:  47% (185/393)   \rReceiving objects:  48% (189/393)   \rReceiving objects:  49% (193/393)   \rReceiving objects:  50% (197/393)   \rReceiving objects:  51% (201/393)   \rReceiving objects:  52% (205/393)   \rReceiving objects:  53% (209/393)   \rReceiving objects:  54% (213/393)   \rReceiving objects:  55% (217/393)   \rReceiving objects:  56% (221/393)   \rReceiving objects:  57% (225/393)   \rReceiving objects:  58% (228/393)   \rReceiving objects:  59% (232/393)   \rReceiving objects:  60% (236/393)   \rReceiving objects:  61% (240/393)   \rReceiving objects:  62% (244/393)   \rReceiving objects:  63% (248/393)   \rReceiving objects:  64% (252/393)   \rReceiving objects:  65% (256/393)   \rReceiving objects:  66% (260/393)   \rReceiving objects:  67% (264/393)   \rReceiving objects:  68% (268/393)   \rReceiving objects:  69% (272/393)   \rReceiving objects:  70% (276/393)   \rReceiving objects:  71% (280/393)   \rReceiving objects:  72% (283/393)   \rReceiving objects:  73% (287/393)   \rReceiving objects:  74% (291/393)   \rReceiving objects:  75% (295/393)   \rReceiving objects:  76% (299/393)   \rReceiving objects:  77% (303/393)   \rReceiving objects:  78% (307/393)   \rReceiving objects:  79% (311/393)   \rReceiving objects:  80% (315/393)   \rReceiving objects:  81% (319/393)   \rReceiving objects:  82% (323/393)   \rReceiving objects:  83% (327/393)   \rReceiving objects:  84% (331/393)   \rReceiving objects:  85% (335/393)   \rReceiving objects:  86% (338/393)   \rReceiving objects:  87% (342/393)   \rReceiving objects:  88% (346/393)   \rReceiving objects:  89% (350/393)   \rReceiving objects:  90% (354/393)   \rReceiving objects:  91% (358/393)   \rremote: Total 393 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
            "Receiving objects:  92% (362/393)   \rReceiving objects:  93% (366/393)   \rReceiving objects:  94% (370/393)   \rReceiving objects:  95% (374/393)   \rReceiving objects:  96% (378/393)   \rReceiving objects:  97% (382/393)   \rReceiving objects:  98% (386/393)   \rReceiving objects:  99% (390/393)   \rReceiving objects: 100% (393/393)   \rReceiving objects: 100% (393/393), 298.73 KiB | 7.11 MiB/s, done.\n",
            "Resolving deltas:   0% (0/109)   \rResolving deltas:   2% (3/109)   \rResolving deltas:   3% (4/109)   \rResolving deltas:   4% (5/109)   \rResolving deltas:   5% (6/109)   \rResolving deltas:   6% (7/109)   \rResolving deltas:   9% (10/109)   \rResolving deltas:  10% (11/109)   \rResolving deltas:  11% (12/109)   \rResolving deltas:  12% (14/109)   \rResolving deltas:  13% (15/109)   \rResolving deltas:  14% (16/109)   \rResolving deltas:  15% (17/109)   \rResolving deltas:  17% (19/109)   \rResolving deltas:  20% (22/109)   \rResolving deltas:  21% (23/109)   \rResolving deltas:  22% (24/109)   \rResolving deltas:  23% (26/109)   \rResolving deltas:  24% (27/109)   \rResolving deltas:  27% (30/109)   \rResolving deltas:  28% (31/109)   \rResolving deltas:  33% (36/109)   \rResolving deltas:  34% (38/109)   \rResolving deltas:  35% (39/109)   \rResolving deltas:  37% (41/109)   \rResolving deltas:  41% (45/109)   \rResolving deltas:  42% (46/109)   \rResolving deltas:  44% (49/109)   \rResolving deltas:  45% (50/109)   \rResolving deltas:  47% (52/109)   \rResolving deltas:  49% (54/109)   \rResolving deltas:  51% (56/109)   \rResolving deltas:  54% (59/109)   \rResolving deltas:  55% (60/109)   \rResolving deltas:  57% (63/109)   \rResolving deltas:  59% (65/109)   \rResolving deltas:  60% (66/109)   \rResolving deltas:  61% (67/109)   \rResolving deltas:  62% (68/109)   \rResolving deltas:  63% (69/109)   \rResolving deltas:  64% (70/109)   \rResolving deltas:  65% (71/109)   \rResolving deltas:  66% (72/109)   \rResolving deltas:  67% (74/109)   \rResolving deltas:  68% (75/109)   \rResolving deltas:  70% (77/109)   \rResolving deltas:  71% (78/109)   \rResolving deltas:  72% (79/109)   \rResolving deltas:  73% (80/109)   \rResolving deltas:  74% (81/109)   \rResolving deltas:  75% (82/109)   \rResolving deltas:  76% (83/109)   \rResolving deltas:  77% (84/109)   \rResolving deltas:  78% (86/109)   \rResolving deltas:  79% (87/109)   \rResolving deltas:  80% (88/109)   \rResolving deltas:  81% (89/109)   \rResolving deltas:  83% (91/109)   \rResolving deltas:  85% (93/109)   \rResolving deltas:  86% (94/109)   \rResolving deltas:  87% (95/109)   \rResolving deltas:  88% (96/109)   \rResolving deltas:  90% (99/109)   \rResolving deltas:  91% (100/109)   \rResolving deltas:  92% (101/109)   \rResolving deltas:  93% (102/109)   \rResolving deltas:  94% (103/109)   \rResolving deltas:  97% (106/109)   \rResolving deltas: 100% (109/109)   \rResolving deltas: 100% (109/109), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQiCNMdEzQST",
        "colab_type": "code",
        "outputId": "5edb8c88-58fc-45c8-90fd-b37ffcbab49d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd pyrouge"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/pyrouge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMl8VHRc2AvF",
        "colab_type": "code",
        "outputId": "71519561-17a3-4ae3-bb0f-400a9ff89715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/pyrouge/tools/ROUGE-1.5.5/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  README.txt  RELEASE-NOTE.txt  \u001b[01;32mROUGE-1.5.5.pl\u001b[0m*  \u001b[01;32mrunROUGE-test.pl\u001b[0m*  \u001b[01;34mXML\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvheSmwIzQd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't need to run assuming pyrogue is already installed \n",
        "#!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY_CG_mqzqC6",
        "colab_type": "code",
        "outputId": "b7cfe8f9-7385-4dc7-d02a-0ff69d8b7412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pyrouge_set_rouge_path /home/pyrouge/tools/ROUGE-1.5.5/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-02 12:00:34,729 [MainThread  ] [INFO ]  Set ROUGE home directory to /home/pyrouge/tools/ROUGE-1.5.5/.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2cd081d9-6551-4609-c46d-cd58ae971d9a",
        "id": "JVLoN0L2E7_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3369
        }
      },
      "source": [
        "# This makes sure that XML/Parser.pm is installed (required by pyrouge)\n",
        "!sudo apt-get install libxml-parser-perl"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl netbase perl-openssl-defaults\n",
            "Suggested packages:\n",
            "  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libauthen-ntlm-perl\n",
            "The following NEW packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl libxml-parser-perl netbase perl-openssl-defaults\n",
            "0 upgraded, 31 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 1,710 kB of archives.\n",
            "After this operation, 5,567 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 netbase all 5.4 [12.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdata-dump-perl all 1.23-1 [27.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-listing-perl all 6.04-1 [9,774 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfont-afm-perl all 1.20-2 [13.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-form-perl all 6.03-1 [23.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tree-perl all 5.07-1 [200 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-format-perl all 2.12-1 [41.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-cookies-perl all 6.04-1 [17.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-daemon-perl all 6.01-1 [17.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-negotiate-perl all 6.00-2 [13.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 perl-openssl-defaults amd64 3build1 [7,012 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-ssleay-perl amd64 1.84-1build1 [282 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-socket-ssl-perl all 2.056-1 [172 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-http-perl all 6.17-1 [22.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtry-tiny-perl all 0.30-1 [20.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwww-robotrules-perl all 6.01-1 [14.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwww-perl all 6.31-1 [137 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-protocol-https-perl all 6.07-2 [8,284 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5,948 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmailtools-perl all 2.18-1 [74.0 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxml-parser-perl amd64 2.44-2build3 [199 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 libauthen-sasl-perl all 2.1600-1 [48.7 kB]\n",
            "Fetched 1,710 kB in 1s (1,875 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 31.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 130911 files and directories currently installed.)\n",
            "Preparing to unpack .../00-netbase_5.4_all.deb ...\n",
            "Unpacking netbase (5.4) ...\n",
            "Selecting previously unselected package libdata-dump-perl.\n",
            "Preparing to unpack .../01-libdata-dump-perl_1.23-1_all.deb ...\n",
            "Unpacking libdata-dump-perl (1.23-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../02-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../03-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../04-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libfile-listing-perl.\n",
            "Preparing to unpack .../05-libfile-listing-perl_6.04-1_all.deb ...\n",
            "Unpacking libfile-listing-perl (6.04-1) ...\n",
            "Selecting previously unselected package libfont-afm-perl.\n",
            "Preparing to unpack .../06-libfont-afm-perl_1.20-2_all.deb ...\n",
            "Unpacking libfont-afm-perl (1.20-2) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../07-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../08-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../09-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../10-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../11-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../12-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libhtml-form-perl.\n",
            "Preparing to unpack .../13-libhtml-form-perl_6.03-1_all.deb ...\n",
            "Unpacking libhtml-form-perl (6.03-1) ...\n",
            "Selecting previously unselected package libhtml-tree-perl.\n",
            "Preparing to unpack .../14-libhtml-tree-perl_5.07-1_all.deb ...\n",
            "Unpacking libhtml-tree-perl (5.07-1) ...\n",
            "Selecting previously unselected package libhtml-format-perl.\n",
            "Preparing to unpack .../15-libhtml-format-perl_2.12-1_all.deb ...\n",
            "Unpacking libhtml-format-perl (2.12-1) ...\n",
            "Selecting previously unselected package libhttp-cookies-perl.\n",
            "Preparing to unpack .../16-libhttp-cookies-perl_6.04-1_all.deb ...\n",
            "Unpacking libhttp-cookies-perl (6.04-1) ...\n",
            "Selecting previously unselected package libhttp-daemon-perl.\n",
            "Preparing to unpack .../17-libhttp-daemon-perl_6.01-1_all.deb ...\n",
            "Unpacking libhttp-daemon-perl (6.01-1) ...\n",
            "Selecting previously unselected package libhttp-negotiate-perl.\n",
            "Preparing to unpack .../18-libhttp-negotiate-perl_6.00-2_all.deb ...\n",
            "Unpacking libhttp-negotiate-perl (6.00-2) ...\n",
            "Selecting previously unselected package perl-openssl-defaults:amd64.\n",
            "Preparing to unpack .../19-perl-openssl-defaults_3build1_amd64.deb ...\n",
            "Unpacking perl-openssl-defaults:amd64 (3build1) ...\n",
            "Selecting previously unselected package libnet-ssleay-perl.\n",
            "Preparing to unpack .../20-libnet-ssleay-perl_1.84-1build1_amd64.deb ...\n",
            "Unpacking libnet-ssleay-perl (1.84-1build1) ...\n",
            "Selecting previously unselected package libio-socket-ssl-perl.\n",
            "Preparing to unpack .../21-libio-socket-ssl-perl_2.056-1_all.deb ...\n",
            "Unpacking libio-socket-ssl-perl (2.056-1) ...\n",
            "Selecting previously unselected package libnet-http-perl.\n",
            "Preparing to unpack .../22-libnet-http-perl_6.17-1_all.deb ...\n",
            "Unpacking libnet-http-perl (6.17-1) ...\n",
            "Selecting previously unselected package libtry-tiny-perl.\n",
            "Preparing to unpack .../23-libtry-tiny-perl_0.30-1_all.deb ...\n",
            "Unpacking libtry-tiny-perl (0.30-1) ...\n",
            "Selecting previously unselected package libwww-robotrules-perl.\n",
            "Preparing to unpack .../24-libwww-robotrules-perl_6.01-1_all.deb ...\n",
            "Unpacking libwww-robotrules-perl (6.01-1) ...\n",
            "Selecting previously unselected package libwww-perl.\n",
            "Preparing to unpack .../25-libwww-perl_6.31-1_all.deb ...\n",
            "Unpacking libwww-perl (6.31-1) ...\n",
            "Selecting previously unselected package liblwp-protocol-https-perl.\n",
            "Preparing to unpack .../26-liblwp-protocol-https-perl_6.07-2_all.deb ...\n",
            "Unpacking liblwp-protocol-https-perl (6.07-2) ...\n",
            "Selecting previously unselected package libnet-smtp-ssl-perl.\n",
            "Preparing to unpack .../27-libnet-smtp-ssl-perl_1.04-1_all.deb ...\n",
            "Unpacking libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Selecting previously unselected package libmailtools-perl.\n",
            "Preparing to unpack .../28-libmailtools-perl_2.18-1_all.deb ...\n",
            "Unpacking libmailtools-perl (2.18-1) ...\n",
            "Selecting previously unselected package libxml-parser-perl.\n",
            "Preparing to unpack .../29-libxml-parser-perl_2.44-2build3_amd64.deb ...\n",
            "Unpacking libxml-parser-perl (2.44-2build3) ...\n",
            "Selecting previously unselected package libauthen-sasl-perl.\n",
            "Preparing to unpack .../30-libauthen-sasl-perl_2.1600-1_all.deb ...\n",
            "Unpacking libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libtry-tiny-perl (0.30-1) ...\n",
            "Setting up libfont-afm-perl (1.20-2) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up perl-openssl-defaults:amd64 (3build1) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libdata-dump-perl (1.23-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libnet-http-perl (6.17-1) ...\n",
            "Setting up libwww-robotrules-perl (6.01-1) ...\n",
            "Setting up libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up netbase (5.4) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libnet-ssleay-perl (1.84-1build1) ...\n",
            "Setting up libio-socket-ssl-perl (2.056-1) ...\n",
            "Setting up libhtml-tree-perl (5.07-1) ...\n",
            "Setting up libfile-listing-perl (6.04-1) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libhttp-negotiate-perl (6.00-2) ...\n",
            "Setting up libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Setting up libhtml-format-perl (2.12-1) ...\n",
            "Setting up libhttp-cookies-perl (6.04-1) ...\n",
            "Setting up libhttp-daemon-perl (6.01-1) ...\n",
            "Setting up libhtml-form-perl (6.03-1) ...\n",
            "Setting up libmailtools-perl (2.18-1) ...\n",
            "Setting up liblwp-protocol-https-perl (6.07-2) ...\n",
            "Setting up libwww-perl (6.31-1) ...\n",
            "Setting up libxml-parser-perl (2.44-2build3) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a526e47a-abd9-4976-80c6-56508a91dac6",
        "id": "c5SOZl3GE7_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /home/pyrouge/tools/ROUGE-1.5.5/data/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/pyrouge/tools/ROUGE-1.5.5/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oS6WoMWgE7_r",
        "colab": {}
      },
      "source": [
        "rm WordNet-2.0.exc.db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rlw70vWyE7_s",
        "colab": {}
      },
      "source": [
        "!./WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPhO1bu411g7",
        "colab_type": "code",
        "outputId": "083e5dbf-c1f0-4fb3-f984-e03da2f77ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2604
        }
      },
      "source": [
        "!python -m pyrouge.test"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-02 12:00:53,696 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmp53uan0cv/rouge_conf.xml\n",
            "F2019-06-02 12:00:53,801 [MainThread  ] [INFO ]  Processing files in data/SL2003_models_plain_text.\n",
            "2019-06-02 12:00:53,801 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-08.html.\n",
            "2019-06-02 12:00:53,801 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-12.html.\n",
            "2019-06-02 12:00:53,801 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-24.html.\n",
            "2019-06-02 12:00:53,801 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-19.html.\n",
            "2019-06-02 12:00:53,802 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-16.html.\n",
            "2019-06-02 12:00:53,802 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-03.html.\n",
            "2019-06-02 12:00:53,802 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-23.html.\n",
            "2019-06-02 12:00:53,802 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-09.html.\n",
            "2019-06-02 12:00:53,802 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-04.html.\n",
            "2019-06-02 12:00:53,803 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-06.html.\n",
            "2019-06-02 12:00:53,803 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-02.html.\n",
            "2019-06-02 12:00:53,803 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-18.html.\n",
            "2019-06-02 12:00:53,803 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-14.html.\n",
            "2019-06-02 12:00:53,803 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-10.html.\n",
            "2019-06-02 12:00:53,803 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-17.html.\n",
            "2019-06-02 12:00:53,804 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-15.html.\n",
            "2019-06-02 12:00:53,804 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-21.html.\n",
            "2019-06-02 12:00:53,804 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-22.html.\n",
            "2019-06-02 12:00:53,804 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-07.html.\n",
            "2019-06-02 12:00:53,804 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-25.html.\n",
            "2019-06-02 12:00:53,805 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-13.html.\n",
            "2019-06-02 12:00:53,805 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-01.html.\n",
            "2019-06-02 12:00:53,805 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-05.html.\n",
            "2019-06-02 12:00:53,805 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-20.html.\n",
            "2019-06-02 12:00:53,805 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-11.html.\n",
            "2019-06-02 12:00:53,805 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp24cu1i1o.\n",
            ".2019-06-02 12:00:53,822 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmpt0o9qyk5/rouge_conf.xml\n",
            "2019-06-02 12:00:53,822 [MainThread  ] [INFO ]  Running ROUGE with command /home/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /home/pyrouge/tools/ROUGE-1.5.5/data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m /tmp/tmpt0o9qyk5/rouge_conf.xml\n",
            "F2019-06-02 12:00:55,872 [MainThread  ] [INFO ]  Processing files in data/SL2003_models_rouge_format.\n",
            "2019-06-02 12:00:55,872 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-08.html.\n",
            "/usr/local/bin/pyrouge_convert_rouge_format_to_plain_text:14: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 14 of the file /usr/local/bin/pyrouge_convert_rouge_format_to_plain_text. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(html)\n",
            "2019-06-02 12:00:55,874 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-12.html.\n",
            "2019-06-02 12:00:55,875 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-24.html.\n",
            "2019-06-02 12:00:55,875 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-19.html.\n",
            "2019-06-02 12:00:55,876 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-16.html.\n",
            "2019-06-02 12:00:55,877 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-03.html.\n",
            "2019-06-02 12:00:55,877 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-23.html.\n",
            "2019-06-02 12:00:55,878 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-09.html.\n",
            "2019-06-02 12:00:55,879 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-04.html.\n",
            "2019-06-02 12:00:55,879 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-06.html.\n",
            "2019-06-02 12:00:55,880 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-02.html.\n",
            "2019-06-02 12:00:55,881 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-18.html.\n",
            "2019-06-02 12:00:55,881 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-14.html.\n",
            "2019-06-02 12:00:55,882 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-10.html.\n",
            "2019-06-02 12:00:55,883 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-17.html.\n",
            "2019-06-02 12:00:55,883 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-15.html.\n",
            "2019-06-02 12:00:55,884 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-21.html.\n",
            "2019-06-02 12:00:55,885 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-22.html.\n",
            "2019-06-02 12:00:55,885 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-07.html.\n",
            "2019-06-02 12:00:55,886 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-25.html.\n",
            "2019-06-02 12:00:55,887 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-13.html.\n",
            "2019-06-02 12:00:55,887 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-01.html.\n",
            "2019-06-02 12:00:55,888 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-05.html.\n",
            "2019-06-02 12:00:55,889 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-20.html.\n",
            "2019-06-02 12:00:55,889 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-11.html.\n",
            "2019-06-02 12:00:55,890 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp4pd2wndd.\n",
            ".E.2019-06-02 12:00:56,018 [MainThread  ] [INFO ]  Writing summaries.\n",
            "2019-06-02 12:00:56,019 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /tmp/tmp1rtp3hv3/system and model files to /tmp/tmp1rtp3hv3/model.\n",
            "2019-06-02 12:00:56,019 [MainThread  ] [INFO ]  Processing files in data/systems_plain.\n",
            "2019-06-02 12:00:56,020 [MainThread  ] [INFO ]  Processing D30003.M.100.T.A.\n",
            "2019-06-02 12:00:56,020 [MainThread  ] [INFO ]  Processing D30005.M.100.T.A.\n",
            "2019-06-02 12:00:56,020 [MainThread  ] [INFO ]  Processing D30001.M.100.T.A.\n",
            "2019-06-02 12:00:56,020 [MainThread  ] [INFO ]  Processing D30002.M.100.T.A.\n",
            "2019-06-02 12:00:56,020 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp1rtp3hv3/system.\n",
            "2019-06-02 12:00:56,021 [MainThread  ] [INFO ]  Processing files in data/models_plain.\n",
            "2019-06-02 12:00:56,021 [MainThread  ] [INFO ]  Processing D30005.M.100.T.B.\n",
            "2019-06-02 12:00:56,021 [MainThread  ] [INFO ]  Processing D30005.M.100.T.G.\n",
            "2019-06-02 12:00:56,021 [MainThread  ] [INFO ]  Processing D30002.M.100.T.E.\n",
            "2019-06-02 12:00:56,021 [MainThread  ] [INFO ]  Processing D30001.M.100.T.D.\n",
            "2019-06-02 12:00:56,021 [MainThread  ] [INFO ]  Processing D30005.M.100.T.C.\n",
            "2019-06-02 12:00:56,022 [MainThread  ] [INFO ]  Processing D30003.M.100.T.F.\n",
            "2019-06-02 12:00:56,022 [MainThread  ] [INFO ]  Processing D30001.M.100.T.B.\n",
            "2019-06-02 12:00:56,022 [MainThread  ] [INFO ]  Processing D30003.M.100.T.C.\n",
            "2019-06-02 12:00:56,022 [MainThread  ] [INFO ]  Processing D30003.M.100.T.B.\n",
            "2019-06-02 12:00:56,022 [MainThread  ] [INFO ]  Processing D30002.M.100.T.B.\n",
            "2019-06-02 12:00:56,023 [MainThread  ] [INFO ]  Processing D30001.M.100.T.C.\n",
            "2019-06-02 12:00:56,023 [MainThread  ] [INFO ]  Processing D30002.M.100.T.C.\n",
            "2019-06-02 12:00:56,023 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp1rtp3hv3/model.\n",
            "2019-06-02 12:00:56,024 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmpjnuyllbk/rouge_conf.xml\n",
            "2019-06-02 12:00:56,024 [MainThread  ] [INFO ]  Running ROUGE with command /home/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /home/pyrouge/tools/ROUGE-1.5.5/data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m /tmp/tmpjnuyllbk/rouge_conf.xml\n",
            "F.E..\n",
            "======================================================================\n",
            "ERROR: test_options (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 218, in test_options\n",
            "    pyrouge_output = check_output_clean(pyrouge_command)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 17, in <lambda>\n",
            "    check_output_clean = lambda c: check_output(c).decode(\"UTF-8\").strip()\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 336, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 403, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 709, in __init__\n",
            "    restore_signals, start_new_session)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyrouge_evaluate_plain_text_files.py': 'pyrouge_evaluate_plain_text_files.py'\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_write_config (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 197, in test_write_config\n",
            "    check_output(command.split())\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 336, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 403, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 709, in __init__\n",
            "    restore_signals, start_new_session)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyrouge_write_config_file.py': 'pyrouge_write_config_file.py'\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_config_file (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 147, in test_config_file\n",
            "    add_data_path(\"ROUGE-test_11.xml\")))\n",
            "AssertionError: False is not true\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_evaluation (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 164, in test_evaluation\n",
            "    self.assertEqual(pyrouge_output, orig_rouge_output)\n",
            "AssertionError: '----[62 chars]R: 0.22552 (95%-conf.int. 0.18148 - 0.27060)\\n[1839 chars]467)' != '----[62 chars]R: 0.77625 (95%-conf.int. 0.76493 - 0.78766)\\n[1839 chars]402)'\n",
            "Diff is 5944 characters long. Set self.maxDiff to None to see it.\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_rouge_for_plain_text (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 183, in test_rouge_for_plain_text\n",
            "    self.assertEqual(pyrouge_output, orig_rouge_output)\n",
            "AssertionError: '----[61 chars]R: 0.39343 (95%-conf.int. 0.37315 - 0.40507)\\n[1816 chars]275)' != '----[61 chars]R: 0.60964 (95%-conf.int. 0.56136 - 0.66437)\\n[1816 chars]163)'\n",
            "Diff is 6000 characters long. Set self.maxDiff to None to see it.\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 11 tests in 5.315s\n",
            "\n",
            "FAILED (failures=3, errors=2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NCtuJElbaI_",
        "colab_type": "text"
      },
      "source": [
        "### Load model and sample data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HjaAwgXSp90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dl_gcs(object_fp, dest_fp, bucket_name='nlp-experiment-datasets', project_id='bert-experiments'):\n",
        "    from googleapiclient.discovery import build\n",
        "    gcs_service = build('storage', 'v1')\n",
        "    from apiclient.http import MediaIoBaseDownload\n",
        "    with open(dest_fp, 'wb') as f:\n",
        "        # Download the file from a given Google Cloud Storage bucket.\n",
        "        request = gcs_service.objects().get_media(bucket=bucket_name,\n",
        "                                            object=f'{object_fp}')\n",
        "        media = MediaIoBaseDownload(f, request)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "            _, done = media.next_chunk()        \n",
        "    print('Download complete')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wHkLLz6TaVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS_DIR = '/home/models/'\n",
        "!mkdir {MODELS_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYP8zKcwePNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL = 'model_step_43000.pt'\n",
        "MODEL_FP = f'{MODELS_DIR}{MODEL}'\n",
        "SAMPLE_DATA_FP = '/home/sample_data/cnndm.test.0.bert.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGfZ_jMAEe35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /home/sample_data/ # Source of data\n",
        "!mkdir /home/bert_results/ # Destination of results\n",
        "!mkdir /home/temp/ \n",
        "!mkdir /home/logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfrsJH-rTPI4",
        "colab_type": "code",
        "outputId": "a0fce195-36fe-4c05-b07a-2da5c21f89cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Download trained model\n",
        "dl_gcs('bertsum/cnn_dailymail/models/model_step_43000.pt', MODEL_FP)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzYKPnYtEEzv",
        "colab_type": "code",
        "outputId": "193eb832-1f8e-4286-e037-5cd2491ad11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Download sample test data\n",
        "dl_gcs('bertsum/cnn_dailymail/sample_data/cnndm.test.0.bert.pt', SAMPLE_DATA_FP)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQifkIF3UyNR",
        "colab_type": "code",
        "outputId": "33600887-7d4f-425a-df4e-5b04ce65e9c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls {MODELS_DIR}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_step_43000.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3QDd4C0lWDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls /home/sample_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJB92Mg3rKXX",
        "colab_type": "text"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCUp_r4rrvdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From src/models/optimizers.py\n",
        "\"\"\" Optimizers class \"\"\"\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "# from onmt.utils import use_gpu\n",
        "\n",
        "\n",
        "def use_gpu(opt):\n",
        "    \"\"\"\n",
        "    Creates a boolean if gpu used\n",
        "    \"\"\"\n",
        "    return (hasattr(opt, 'gpu_ranks') and len(opt.gpu_ranks) > 0) or \\\n",
        "           (hasattr(opt, 'gpu') and opt.gpu > -1)\n",
        "\n",
        "def build_optim(model, opt, checkpoint):\n",
        "    \"\"\" Build optimizer \"\"\"\n",
        "    saved_optimizer_state_dict = None\n",
        "\n",
        "    if opt.train_from:\n",
        "        optim = checkpoint['optim']\n",
        "        # We need to save a copy of optim.optimizer.state_dict() for setting\n",
        "        # the, optimizer state later on in Stage 2 in this method, since\n",
        "        # the method optim.set_parameters(model.parameters()) will overwrite\n",
        "        # optim.optimizer, and with ith the values stored in\n",
        "        # optim.optimizer.state_dict()\n",
        "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
        "    else:\n",
        "        optim = Optimizer(\n",
        "            opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
        "            lr_decay=opt.learning_rate_decay,\n",
        "            start_decay_steps=opt.start_decay_steps,\n",
        "            decay_steps=opt.decay_steps,\n",
        "            beta1=opt.adam_beta1,\n",
        "            beta2=opt.adam_beta2,\n",
        "            adagrad_accum=opt.adagrad_accumulator_init,\n",
        "            decay_method=opt.decay_method,\n",
        "            warmup_steps=opt.warmup_steps)\n",
        "\n",
        "    # Stage 1:\n",
        "    # Essentially optim.set_parameters (re-)creates and optimizer using\n",
        "    # model.paramters() as parameters that will be stored in the\n",
        "    # optim.optimizer.param_groups field of the torch optimizer class.\n",
        "    # Importantly, this method does not yet load the optimizer state, as\n",
        "    # essentially it builds a new optimizer with empty optimizer state and\n",
        "    # parameters from the model.\n",
        "    optim.set_parameters(model.named_parameters())\n",
        "\n",
        "    if opt.train_from:\n",
        "        # Stage 2: In this stage, which is only performed when loading an\n",
        "        # optimizer from a checkpoint, we load the saved_optimizer_state_dict\n",
        "        # into the re-created optimizer, to set the optim.optimizer.state\n",
        "        # field, which was previously empty. For this, we use the optimizer\n",
        "        # state saved in the \"saved_optimizer_state_dict\" variable for\n",
        "        # this purpose.\n",
        "        # See also: https://github.com/pytorch/pytorch/issues/2830\n",
        "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
        "        # Convert back the state values to cuda type if applicable\n",
        "        if use_gpu(opt):\n",
        "            for state in optim.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.cuda()\n",
        "\n",
        "        # We want to make sure that indeed we have a non-empty optimizer state\n",
        "        # when we loaded an existing model. This should be at least the case\n",
        "        # for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state\n",
        "        # (Exponential moving average of gradient and squared gradient values)\n",
        "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
        "            raise RuntimeError(\n",
        "                \"Error: loaded Adam optimizer from existing model\" +\n",
        "                \" but optimizer state is empty\")\n",
        "\n",
        "    return optim\n",
        "\n",
        "\n",
        "class MultipleOptimizer(object):\n",
        "    \"\"\" Implement multiple optimizers needed for sparse adam \"\"\"\n",
        "\n",
        "    def __init__(self, op):\n",
        "        \"\"\" ? \"\"\"\n",
        "        self.optimizers = op\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        for op in self.optimizers:\n",
        "            op.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        for op in self.optimizers:\n",
        "            op.step()\n",
        "\n",
        "    @property\n",
        "    def state(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        return {k: v for op in self.optimizers for k, v in op.state.items()}\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        return [op.state_dict() for op in self.optimizers]\n",
        "\n",
        "    def load_state_dict(self, state_dicts):\n",
        "        \"\"\" ? \"\"\"\n",
        "        assert len(state_dicts) == len(self.optimizers)\n",
        "        for i in range(len(state_dicts)):\n",
        "            self.optimizers[i].load_state_dict(state_dicts[i])\n",
        "\n",
        "\n",
        "class Optimizer(object):\n",
        "    \"\"\"\n",
        "    Controller class for optimization. Mostly a thin\n",
        "    wrapper for `optim`, but also useful for implementing\n",
        "    rate scheduling beyond what is currently available.\n",
        "    Also implements necessary methods for training RNNs such\n",
        "    as grad manipulations.\n",
        "    Args:\n",
        "      method (:obj:`str`): one of [sgd, adagrad, adadelta, adam]\n",
        "      lr (float): learning rate\n",
        "      lr_decay (float, optional): learning rate decay multiplier\n",
        "      start_decay_steps (int, optional): step to start learning rate decay\n",
        "      beta1, beta2 (float, optional): parameters for adam\n",
        "      adagrad_accum (float, optional): initialization parameter for adagrad\n",
        "      decay_method (str, option): custom decay options\n",
        "      warmup_steps (int, option): parameter for `noam` decay\n",
        "    We use the default parameters for Adam that are suggested by\n",
        "    the original paper https://arxiv.org/pdf/1412.6980.pdf\n",
        "    These values are also used by other established implementations,\n",
        "    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
        "    https://keras.io/optimizers/\n",
        "    Recently there are slightly different values used in the paper\n",
        "    \"Attention is all you need\"\n",
        "    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n",
        "    was used there however, beta2=0.999 is still arguably the more\n",
        "    established value, so we use that here as well\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, method, learning_rate, max_grad_norm,\n",
        "                 lr_decay=1, start_decay_steps=None, decay_steps=None,\n",
        "                 beta1=0.9, beta2=0.999,\n",
        "                 adagrad_accum=0.0,\n",
        "                 decay_method=None,\n",
        "                 warmup_steps=4000\n",
        "                 ):\n",
        "        self.last_ppl = None\n",
        "        self.learning_rate = learning_rate\n",
        "        self.original_lr = learning_rate\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.method = method\n",
        "        self.lr_decay = lr_decay\n",
        "        self.start_decay_steps = start_decay_steps\n",
        "        self.decay_steps = decay_steps\n",
        "        self.start_decay = False\n",
        "        self._step = 0\n",
        "        self.betas = [beta1, beta2]\n",
        "        self.adagrad_accum = adagrad_accum\n",
        "        self.decay_method = decay_method\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        \"\"\" ? \"\"\"\n",
        "        self.params = []\n",
        "        self.sparse_params = []\n",
        "        for k, p in params:\n",
        "            if p.requires_grad:\n",
        "                if self.method != 'sparseadam' or \"embed\" not in k:\n",
        "                    self.params.append(p)\n",
        "                else:\n",
        "                    self.sparse_params.append(p)\n",
        "        if self.method == 'sgd':\n",
        "            self.optimizer = optim.SGD(self.params, lr=self.learning_rate)\n",
        "        elif self.method == 'adagrad':\n",
        "            self.optimizer = optim.Adagrad(self.params, lr=self.learning_rate)\n",
        "            for group in self.optimizer.param_groups:\n",
        "                for p in group['params']:\n",
        "                    self.optimizer.state[p]['sum'] = self.optimizer\\\n",
        "                        .state[p]['sum'].fill_(self.adagrad_accum)\n",
        "        elif self.method == 'adadelta':\n",
        "            self.optimizer = optim.Adadelta(self.params, lr=self.learning_rate)\n",
        "        elif self.method == 'adam':\n",
        "            self.optimizer = optim.Adam(self.params, lr=self.learning_rate,\n",
        "                                        betas=self.betas, eps=1e-9)\n",
        "        elif self.method == 'sparseadam':\n",
        "            self.optimizer = MultipleOptimizer(\n",
        "                [optim.Adam(self.params, lr=self.learning_rate,\n",
        "                            betas=self.betas, eps=1e-8),\n",
        "                 optim.SparseAdam(self.sparse_params, lr=self.learning_rate,\n",
        "                                  betas=self.betas, eps=1e-8)])\n",
        "        else:\n",
        "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
        "\n",
        "    def _set_rate(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        if self.method != 'sparseadam':\n",
        "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
        "        else:\n",
        "            for op in self.optimizer.optimizers:\n",
        "                op.param_groups[0]['lr'] = self.learning_rate\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Update the model parameters based on current gradients.\n",
        "        Optionally, will employ gradient modification or update learning\n",
        "        rate.\n",
        "        \"\"\"\n",
        "        self._step += 1\n",
        "\n",
        "        # Decay method used in tensor2tensor.\n",
        "        if self.decay_method == \"noam\":\n",
        "            self._set_rate(\n",
        "                self.original_lr *\n",
        "\n",
        "                 min(self._step ** (-0.5),\n",
        "                     self._step * self.warmup_steps**(-1.5)))\n",
        "\n",
        "            # self._set_rate(self.original_lr *self.model_size ** (-0.5) *min(1.0, self._step / self.warmup_steps)*max(self._step, self.warmup_steps)**(-0.5))\n",
        "        # Decay based on start_decay_steps every decay_steps\n",
        "        else:\n",
        "            if ((self.start_decay_steps is not None) and (\n",
        "                     self._step >= self.start_decay_steps)):\n",
        "                self.start_decay = True\n",
        "            if self.start_decay:\n",
        "                if ((self._step - self.start_decay_steps)\n",
        "                   % self.decay_steps == 0):\n",
        "                    self.learning_rate = self.learning_rate * self.lr_decay\n",
        "\n",
        "        if self.method != 'sparseadam':\n",
        "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
        "\n",
        "        if self.max_grad_norm:\n",
        "            clip_grad_norm_(self.params, self.max_grad_norm)\n",
        "        self.optimizer.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5LMTHWzr__F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From src/models/neural.py\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n",
        "    Args:\n",
        "        d_model (int): the size of input for the first-layer of the FFN.\n",
        "        d_ff (int): the hidden layer size of the second-layer\n",
        "            of the FNN.\n",
        "        dropout (float): dropout probability in :math:`[0, 1)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.actv = gelu\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inter = self.dropout_1(self.actv(self.w_1(self.layer_norm(x))))\n",
        "        output = self.dropout_2(self.w_2(inter))\n",
        "        return output + x\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention module from\n",
        "    \"Attention is All You Need\"\n",
        "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n",
        "    Similar to standard `dot` attention but uses\n",
        "    multiple attention distributions simulataneously\n",
        "    to select relevant items.\n",
        "    .. mermaid::\n",
        "       graph BT\n",
        "          A[key]\n",
        "          B[value]\n",
        "          C[query]\n",
        "          O[output]\n",
        "          subgraph Attn\n",
        "            D[Attn 1]\n",
        "            E[Attn 2]\n",
        "            F[Attn N]\n",
        "          end\n",
        "          A --> D\n",
        "          C --> D\n",
        "          A --> E\n",
        "          C --> E\n",
        "          A --> F\n",
        "          C --> F\n",
        "          D --> O\n",
        "          E --> O\n",
        "          F --> O\n",
        "          B --> O\n",
        "    Also includes several additional tricks.\n",
        "    Args:\n",
        "       head_count (int): number of parallel heads\n",
        "       model_dim (int): the dimension of keys/values/queries,\n",
        "           must be divisible by head_count\n",
        "       dropout (float): dropout parameter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
        "        assert model_dim % head_count == 0\n",
        "        self.dim_per_head = model_dim // head_count\n",
        "        self.model_dim = model_dim\n",
        "\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.head_count = head_count\n",
        "\n",
        "        self.linear_keys = nn.Linear(model_dim,\n",
        "                                     head_count * self.dim_per_head)\n",
        "        self.linear_values = nn.Linear(model_dim,\n",
        "                                       head_count * self.dim_per_head)\n",
        "        self.linear_query = nn.Linear(model_dim,\n",
        "                                      head_count * self.dim_per_head)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.use_final_linear = use_final_linear\n",
        "        if (self.use_final_linear):\n",
        "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def forward(self, key, value, query, mask=None,\n",
        "                layer_cache=None, type=None, predefined_graph_1=None):\n",
        "        \"\"\"\n",
        "        Compute the context vector and the attention vectors.\n",
        "        Args:\n",
        "           key (`FloatTensor`): set of `key_len`\n",
        "                key vectors `[batch, key_len, dim]`\n",
        "           value (`FloatTensor`): set of `key_len`\n",
        "                value vectors `[batch, key_len, dim]`\n",
        "           query (`FloatTensor`): set of `query_len`\n",
        "                 query vectors  `[batch, query_len, dim]`\n",
        "           mask: binary mask indicating which keys have\n",
        "                 non-zero attention `[batch, query_len, key_len]`\n",
        "        Returns:\n",
        "           (`FloatTensor`, `FloatTensor`) :\n",
        "           * output context vectors `[batch, query_len, dim]`\n",
        "           * one of the attention vectors `[batch, query_len, key_len]`\n",
        "        \"\"\"\n",
        "\n",
        "        # CHECKS\n",
        "        # batch, k_len, d = key.size()\n",
        "        # batch_, k_len_, d_ = value.size()\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(k_len, k_len_)\n",
        "        # aeq(d, d_)\n",
        "        # batch_, q_len, d_ = query.size()\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(d, d_)\n",
        "        # aeq(self.model_dim % 8, 0)\n",
        "        # if mask is not None:\n",
        "        #    batch_, q_len_, k_len_ = mask.size()\n",
        "        #    aeq(batch_, batch)\n",
        "        #    aeq(k_len_, k_len)\n",
        "        #    aeq(q_len_ == q_len)\n",
        "        # END CHECKS\n",
        "\n",
        "        batch_size = key.size(0)\n",
        "        dim_per_head = self.dim_per_head\n",
        "        head_count = self.head_count\n",
        "        key_len = key.size(1)\n",
        "        query_len = query.size(1)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\"  projection \"\"\"\n",
        "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
        "                .transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            \"\"\"  compute context \"\"\"\n",
        "            return x.transpose(1, 2).contiguous() \\\n",
        "                .view(batch_size, -1, head_count * dim_per_head)\n",
        "\n",
        "        # 1) Project key, value, and query.\n",
        "        if layer_cache is not None:\n",
        "            if type == \"self\":\n",
        "                query, key, value = self.linear_query(query), \\\n",
        "                                    self.linear_keys(query), \\\n",
        "                                    self.linear_values(query)\n",
        "\n",
        "                key = shape(key)\n",
        "                value = shape(value)\n",
        "\n",
        "                if layer_cache is not None:\n",
        "                    device = key.device\n",
        "                    if layer_cache[\"self_keys\"] is not None:\n",
        "                        key = torch.cat(\n",
        "                            (layer_cache[\"self_keys\"].to(device), key),\n",
        "                            dim=2)\n",
        "                    if layer_cache[\"self_values\"] is not None:\n",
        "                        value = torch.cat(\n",
        "                            (layer_cache[\"self_values\"].to(device), value),\n",
        "                            dim=2)\n",
        "                    layer_cache[\"self_keys\"] = key\n",
        "                    layer_cache[\"self_values\"] = value\n",
        "            elif type == \"context\":\n",
        "                query = self.linear_query(query)\n",
        "                if layer_cache is not None:\n",
        "                    if layer_cache[\"memory_keys\"] is None:\n",
        "                        key, value = self.linear_keys(key), \\\n",
        "                                     self.linear_values(value)\n",
        "                        key = shape(key)\n",
        "                        value = shape(value)\n",
        "                    else:\n",
        "                        key, value = layer_cache[\"memory_keys\"], \\\n",
        "                                     layer_cache[\"memory_values\"]\n",
        "                    layer_cache[\"memory_keys\"] = key\n",
        "                    layer_cache[\"memory_values\"] = value\n",
        "                else:\n",
        "                    key, value = self.linear_keys(key), \\\n",
        "                                 self.linear_values(value)\n",
        "                    key = shape(key)\n",
        "                    value = shape(value)\n",
        "        else:\n",
        "            key = self.linear_keys(key)\n",
        "            value = self.linear_values(value)\n",
        "            query = self.linear_query(query)\n",
        "            key = shape(key)\n",
        "            value = shape(value)\n",
        "\n",
        "        query = shape(query)\n",
        "\n",
        "        key_len = key.size(2)\n",
        "        query_len = query.size(2)\n",
        "\n",
        "        # 2) Calculate and scale scores.\n",
        "        query = query / math.sqrt(dim_per_head)\n",
        "        scores = torch.matmul(query, key.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand_as(scores)\n",
        "            scores = scores.masked_fill(mask, -1e18)\n",
        "\n",
        "        # 3) Apply attention dropout and compute context vectors.\n",
        "\n",
        "        attn = self.softmax(scores)\n",
        "\n",
        "        if (not predefined_graph_1 is None):\n",
        "            attn_masked = attn[:, -1] * predefined_graph_1\n",
        "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
        "\n",
        "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
        "\n",
        "        drop_attn = self.dropout(attn)\n",
        "        if (self.use_final_linear):\n",
        "            context = unshape(torch.matmul(drop_attn, value))\n",
        "            output = self.final_linear(context)\n",
        "            return output\n",
        "        else:\n",
        "            context = torch.matmul(drop_attn, value)\n",
        "            return context\n",
        "\n",
        "        # CHECK\n",
        "        # batch_, q_len_, d_ = output.size()\n",
        "        # aeq(q_len, q_len_)\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(d, d_)\n",
        "\n",
        "        # Return one attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNV_julAsEwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From /src/models/rnn.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LayerNormLSTMCell(nn.LSTMCell):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super().__init__(input_size, hidden_size, bias)\n",
        "\n",
        "        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n",
        "        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n",
        "        self.ln_ho = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        self.check_forward_input(input)\n",
        "        if hidden is None:\n",
        "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "        else:\n",
        "            hx, cx = hidden\n",
        "        self.check_forward_hidden(input, hx, '[0]')\n",
        "        self.check_forward_hidden(input, cx, '[1]')\n",
        "\n",
        "        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n",
        "                + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n",
        "        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n",
        "        g = gates[:, (3 * self.hidden_size):].tanh()\n",
        "\n",
        "        cy = (f * cx) + (i * g)\n",
        "        hy = o * self.ln_ho(cy).tanh()\n",
        "        return hy, cy\n",
        "\n",
        "\n",
        "class LayerNormLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.hidden0 = nn.ModuleList([\n",
        "            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
        "                              hidden_size=hidden_size, bias=bias)\n",
        "            for layer in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        if self.bidirectional:\n",
        "            self.hidden1 = nn.ModuleList([\n",
        "                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
        "                                  hidden_size=hidden_size, bias=bias)\n",
        "                for layer in range(num_layers)\n",
        "            ])\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "        if hidden is None:\n",
        "            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
        "            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
        "        else:\n",
        "            hx, cx = hidden\n",
        "\n",
        "        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
        "        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
        "\n",
        "        if self.bidirectional:\n",
        "            xs = input\n",
        "            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n",
        "                l0, l1 = 2 * l, 2 * l + 1\n",
        "                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n",
        "                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n",
        "                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\n",
        "                    h0, c0 = ht[t][l0], ct[t][l0]\n",
        "                    t = seq_len - 1 - t\n",
        "                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\n",
        "                    h1, c1 = ht[t][l1], ct[t][l1]\n",
        "                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\n",
        "            y = torch.stack(xs)\n",
        "            hy = torch.stack(ht[-1])\n",
        "            cy = torch.stack(ct[-1])\n",
        "        else:\n",
        "            h, c = hx, cx\n",
        "            for t, x in enumerate(input):\n",
        "                for l, layer in enumerate(self.hidden0):\n",
        "                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\n",
        "                    x = ht[t][l]\n",
        "                h, c = ht[t], ct[t]\n",
        "            y = torch.stack([h[-1] for h in ht])\n",
        "            hy = torch.stack(ht[-1])\n",
        "            cy = torch.stack(ct[-1])\n",
        "\n",
        "        return y, (hy, cy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCF3pdh6rSyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From /src/models/encoder.py\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from models.neural import MultiHeadedAttention, PositionwiseFeedForward\n",
        "#from models.rnn import LayerNormLSTM\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask_cls):\n",
        "        h = self.linear1(x).squeeze(-1)\n",
        "        sent_scores = self.sigmoid(h) * mask_cls.float()\n",
        "        return sent_scores\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout, dim, max_len=5000):\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
        "                              -(math.log(10000.0) / dim)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, emb, step=None):\n",
        "        emb = emb * math.sqrt(self.dim)\n",
        "        if (step):\n",
        "            emb = emb + self.pe[:, step][:, None, :]\n",
        "\n",
        "        else:\n",
        "            emb = emb + self.pe[:, :emb.size(1)]\n",
        "        emb = self.dropout(emb)\n",
        "        return emb\n",
        "\n",
        "    def get_emb(self, emb):\n",
        "        return self.pe[:, :emb.size(1)]\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadedAttention(\n",
        "            heads, d_model, dropout=dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, iter, query, inputs, mask):\n",
        "        if (iter != 0):\n",
        "            input_norm = self.layer_norm(inputs)\n",
        "        else:\n",
        "            input_norm = inputs\n",
        "\n",
        "        mask = mask.unsqueeze(1)\n",
        "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
        "                                 mask=mask)\n",
        "        out = self.dropout(context) + inputs\n",
        "        return self.feed_forward(out)\n",
        "\n",
        "\n",
        "class TransformerInterEncoder(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, heads, dropout, num_inter_layers=0):\n",
        "        super(TransformerInterEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_inter_layers = num_inter_layers\n",
        "        self.pos_emb = PositionalEncoding(dropout, d_model)\n",
        "        self.transformer_inter = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(d_model, heads, d_ff, dropout)\n",
        "             for _ in range(num_inter_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.wo = nn.Linear(d_model, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, top_vecs, mask):\n",
        "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
        "\n",
        "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
        "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
        "        x = top_vecs * mask[:, :, None].float()\n",
        "        x = x + pos_emb\n",
        "\n",
        "        for i in range(self.num_inter_layers):\n",
        "            x = self.transformer_inter[i](i, x, x, 1 - mask)  # all_sents * max_tokens * dim\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        sent_scores = self.sigmoid(self.wo(x))\n",
        "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
        "\n",
        "        return sent_scores\n",
        "\n",
        "\n",
        "class RNNEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, bidirectional, num_layers, input_size,\n",
        "                 hidden_size, dropout=0.0):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        assert hidden_size % num_directions == 0\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.rnn = LayerNormLSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional)\n",
        "\n",
        "        self.wo = nn.Linear(num_directions * hidden_size, 1, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"See :func:`EncoderBase.forward()`\"\"\"\n",
        "        x = torch.transpose(x, 1, 0)\n",
        "        memory_bank, _ = self.rnn(x)\n",
        "        memory_bank = self.dropout(memory_bank) + x\n",
        "        memory_bank = torch.transpose(memory_bank, 1, 0)\n",
        "\n",
        "        sent_scores = self.sigmoid(self.wo(memory_bank))\n",
        "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
        "        return sent_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2_pH2IvrOzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From src/models/model_builder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertModel, BertConfig\n",
        "from torch.nn.init import xavier_uniform_\n",
        "\n",
        "#from models.encoder import TransformerInterEncoder, Classifier, RNNEncoder\n",
        "#from models.optimizers import Optimizer\n",
        "\n",
        "\n",
        "def build_optim(args, model, checkpoint):\n",
        "    \"\"\" Build optimizer \"\"\"\n",
        "    saved_optimizer_state_dict = None\n",
        "\n",
        "    if args.train_from != '':\n",
        "        optim = checkpoint['optim']\n",
        "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
        "    else:\n",
        "        optim = Optimizer(\n",
        "            args.optim, args.lr, args.max_grad_norm,\n",
        "            beta1=args.beta1, beta2=args.beta2,\n",
        "            decay_method=args.decay_method,\n",
        "            warmup_steps=args.warmup_steps)\n",
        "\n",
        "    optim.set_parameters(list(model.named_parameters()))\n",
        "\n",
        "    if args.train_from != '':\n",
        "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
        "        if args.visible_gpus != '-1':\n",
        "            for state in optim.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.cuda()\n",
        "\n",
        "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
        "            raise RuntimeError(\n",
        "                \"Error: loaded Adam optimizer from existing model\" +\n",
        "                \" but optimizer state is empty\")\n",
        "\n",
        "    return optim\n",
        "\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, temp_dir, load_pretrained_bert, bert_config):\n",
        "        super(Bert, self).__init__()\n",
        "        if(load_pretrained_bert):\n",
        "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
        "        else:\n",
        "            self.model = BertModel(bert_config)\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        encoded_layers, _ = self.model(x, segs, attention_mask =mask)\n",
        "        top_vec = encoded_layers[-1]\n",
        "        return top_vec\n",
        "\n",
        "\n",
        "\n",
        "class Summarizer(nn.Module):\n",
        "    def __init__(self, args, device, load_pretrained_bert = False, bert_config = None):\n",
        "        super(Summarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.bert = Bert(args.temp_dir, load_pretrained_bert, bert_config)\n",
        "        if (args.encoder == 'classifier'):\n",
        "            self.encoder = Classifier(self.bert.model.config.hidden_size)\n",
        "        elif(args.encoder=='transformer'):\n",
        "            self.encoder = TransformerInterEncoder(self.bert.model.config.hidden_size, args.ff_size, args.heads,\n",
        "                                                   args.dropout, args.inter_layers)\n",
        "        elif(args.encoder=='rnn'):\n",
        "            self.encoder = RNNEncoder(bidirectional=True, num_layers=1,\n",
        "                                      input_size=self.bert.model.config.hidden_size, hidden_size=args.rnn_size,\n",
        "                                      dropout=args.dropout)\n",
        "        elif (args.encoder == 'baseline'):\n",
        "            bert_config = BertConfig(self.bert.model.config.vocab_size, hidden_size=args.hidden_size,\n",
        "                                     num_hidden_layers=6, num_attention_heads=8, intermediate_size=args.ff_size)\n",
        "            self.bert.model = BertModel(bert_config)\n",
        "            self.encoder = Classifier(self.bert.model.config.hidden_size)\n",
        "\n",
        "        if args.param_init != 0.0:\n",
        "            for p in self.encoder.parameters():\n",
        "                p.data.uniform_(-args.param_init, args.param_init)\n",
        "        if args.param_init_glorot:\n",
        "            for p in self.encoder.parameters():\n",
        "                if p.dim() > 1:\n",
        "                    xavier_uniform_(p)\n",
        "\n",
        "        self.to(device)\n",
        "    def load_cp(self, pt):\n",
        "        self.load_state_dict(pt['model'], strict=True)\n",
        "\n",
        "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
        "\n",
        "        top_vec = self.bert(x, segs, mask)\n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores = self.encoder(sents_vec, mask_cls).squeeze(-1)\n",
        "        return sent_scores, mask_cls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJQ5QroaCQMw",
        "colab_type": "text"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMKQ_-Owfh3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "from argparse import Namespace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfF6gekAq736",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting these as default arguments\n",
        "# These attributes can be edited as needed\n",
        "args = Namespace(\n",
        "    encoder=\"transformer\",\n",
        "    mode=\"test\",\n",
        "    bert_data_path=\"/home/sample_data/cnndm\",\n",
        "    model_path=MODELS_DIR,\n",
        "    result_path=\"/home/bert_results/cnndm\",\n",
        "    temp_dir=\"/home/temp/\",\n",
        "    bert_config_path='/home/BertSum/bert_config_uncased_base.json',\n",
        "    batch_size=1000,\n",
        "    model_fp=MODEL_FP,\n",
        "\n",
        "    use_interval=True,\n",
        "    hidden_size=128,\n",
        "    ff_size=2048, # Size used during training\n",
        "    heads=4,\n",
        "    inter_layers=2,\n",
        "    rnn_size=512,\n",
        "\n",
        "    param_init=0.0,\n",
        "    param_init_glorot=True,\n",
        "    dropout=0.1,\n",
        "    optim='adam',\n",
        "    lr=1,\n",
        "    beta1= 0.9,\n",
        "    beta2=0.999,\n",
        "    decay_method='',\n",
        "    warmup_steps=8000,\n",
        "    max_grad_norm=0,\n",
        "\n",
        "    save_checkpoint_steps=5,\n",
        "    accum_count=1,\n",
        "    world_size=1,\n",
        "    report_every=1,\n",
        "    train_steps=1000,\n",
        "    recall_eval=False,\n",
        "\n",
        "\n",
        "    visible_gpus='-1',\n",
        "    gpu_ranks='0',\n",
        "    log_file='/home/logs/cnndm.log',\n",
        "    dataset='',\n",
        "    seed=666,\n",
        "\n",
        "    test_all=False,\n",
        "    test_from=MODEL,\n",
        "    train_from='',\n",
        "    report_rouge=True,\n",
        "    block_trigram=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TpRlkcoE-00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing arguments\n",
        "# Setting these as default arguments\n",
        "# These attributes can be edited as needed\n",
        "pp_args = Namespace(\n",
        "    mode=\"\",\n",
        "    oracle_mode='greedy',\n",
        "    shard_size=2000,\n",
        "    min_nsents=3,\n",
        "    max_nsents=100,\n",
        "    min_src_ntokens=5,\n",
        "    max_src_ntokens=200,\n",
        "    lower=True,\n",
        "    dataset='',\n",
        "    n_cpus=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP-uy_DuCjXp",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_svQJOZ-sse",
        "colab_type": "code",
        "outputId": "e7f521e8-cbdb-4742-e00c-8871b57ebeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Have to be in this directory for the pickle file loading to import BERTSUM in the same environment that it was trained in\n",
        "cd /home/BertSum/src"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/BertSum/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD2hEkNUClgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7898fcc6-6d13-4966-a36c-14c3bb1a0624"
      },
      "source": [
        "from models import data_loader, model_builder\n",
        "from models.data_loader import load_dataset\n",
        "from models.trainer import build_trainer, Trainer\n",
        "from prepro import data_builder\n",
        "from models import data_loader\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbldDaOFTF9",
        "colab_type": "text"
      },
      "source": [
        "## Forward pass functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59klMZCztvaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_api(self, test_iter, step, top_n_sentences=3, cal_lead=False, cal_oracle=False):\n",
        "    \"\"\" Validate model.\n",
        "        valid_iter: validate data iterator\n",
        "    Returns:\n",
        "        :obj:`nmt.Statistics`: validation loss statistics\n",
        "    \"\"\"\n",
        "    # Set model in validating mode.\n",
        "    def _get_ngrams(n, text):\n",
        "        ngram_set = set()\n",
        "        text_length = len(text)\n",
        "        max_index_ngram_start = text_length - n\n",
        "        for i in range(max_index_ngram_start + 1):\n",
        "            ngram_set.add(tuple(text[i:i + n]))\n",
        "        return ngram_set\n",
        "\n",
        "    def _block_tri(c, p):\n",
        "        tri_c = _get_ngrams(3, c.split())\n",
        "        for s in p:\n",
        "            tri_s = _get_ngrams(3, s.split())\n",
        "            if len(tri_c.intersection(tri_s))>0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    if (not cal_lead and not cal_oracle):\n",
        "        # Evaluate without performing backpropagation and dropout\n",
        "        self.model.eval()\n",
        "    source_article = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "            src_str = batch.src_str\n",
        "\n",
        "            source_article += [' '.join(article) for article in src_str]\n",
        "\n",
        "            if (cal_lead):\n",
        "                selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "            elif (cal_oracle):\n",
        "                selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                range(batch.batch_size)]\n",
        "            else:\n",
        "                sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "                sent_scores = sent_scores + mask.float()\n",
        "                sent_scores = sent_scores.cpu().data.numpy()\n",
        "                # Sort sentence ids in descending order based on sentence scores (representing summary importance)\n",
        "                selected_ids = np.argsort(-sent_scores, 1)\n",
        "            # selected_ids = np.sort(selected_ids,1)\n",
        "            for i, idx in enumerate(selected_ids):\n",
        "                _pred = []\n",
        "                if(len(batch.src_str[i])==0):\n",
        "                    continue\n",
        "                # Loop through each sentence\n",
        "                # len(batch.src_str[i]) refers to the number of sentences in the jth test example\n",
        "                for j in selected_ids[i][:len(batch.src_str[i])]:\n",
        "                    if(j>=len( batch.src_str[i])):\n",
        "                        continue\n",
        "                    candidate = batch.src_str[i][j].strip()\n",
        "                    if(self.args.block_trigram):\n",
        "                        if(not _block_tri(candidate,_pred)):\n",
        "                            _pred.append(candidate)\n",
        "                    else:\n",
        "                        _pred.append(candidate)\n",
        "                    \n",
        "                    # len(_pred) == 3 means that we limit sentences to top top_n_sentences\n",
        "                    if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == top_n_sentences):\n",
        "                        break\n",
        "\n",
        "                _pred = '<q>'.join(_pred)\n",
        "\n",
        "                pred.append(_pred)\n",
        "\n",
        "    results = {'source_article': source_article, 'predicted_summary': pred}\n",
        "    return results\n",
        "\n",
        "def example_api(self, example, step, top_n_sentences=3, device='cpu', cal_lead=False, cal_oracle=False):\n",
        "    \"\"\" \n",
        "    Runs inference on a single test example. Designed for API deployemnt.\n",
        "    \"\"\"\n",
        "    # Set model in validating mode.\n",
        "    def _get_ngrams(n, text):\n",
        "        ngram_set = set()\n",
        "        text_length = len(text)\n",
        "        max_index_ngram_start = text_length - n\n",
        "        for i in range(max_index_ngram_start + 1):\n",
        "            ngram_set.add(tuple(text[i:i + n]))\n",
        "        return ngram_set\n",
        "\n",
        "    def _block_tri(c, p):\n",
        "        tri_c = _get_ngrams(3, c.split())\n",
        "        for s in p:\n",
        "            tri_s = _get_ngrams(3, s.split())\n",
        "            if len(tri_c.intersection(tri_s))>0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    if (not cal_lead and not cal_oracle):\n",
        "        # Evaluate without performing backpropagation and dropout\n",
        "        self.model.eval()\n",
        "    # Set model device (cuda or cpu)\n",
        "    self.model.to(device=device) \n",
        "    source_article = []\n",
        "    pred = []\n",
        "    src = example.src\n",
        "    labels = example.labels\n",
        "    segs = example.segs\n",
        "    clss = example.clss\n",
        "    mask = example.mask\n",
        "    mask_cls = example.mask_cls\n",
        "    src_str = example.src_str\n",
        "\n",
        "    source_article += [' '.join(article) for article in src_str]\n",
        "\n",
        "    if (cal_lead):\n",
        "        selected_ids = [list(range(example.clss.size(1)))] * example.batch_size\n",
        "    elif (cal_oracle):\n",
        "        selected_ids = [[j for j in range(example.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                        range(example.batch_size)]\n",
        "    else:\n",
        "        sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "        sent_scores = sent_scores + mask.float()\n",
        "        sent_scores = sent_scores.cpu().data.numpy()\n",
        "        # Sort sentence ids in descending order based on sentence scores (representing summary importance)\n",
        "        selected_ids = np.argsort(-sent_scores, 1)\n",
        "    # selected_ids = np.sort(selected_ids,1)\n",
        "    for i, idx in enumerate(selected_ids):\n",
        "        _pred = []\n",
        "        if(len(example.src_str[i])==0):\n",
        "            continue\n",
        "        # Loop through each sentence\n",
        "        # len(example.src_str[i]) refers to the number of sentences in the jth test example\n",
        "        for j in selected_ids[i][:len(example.src_str[i])]:\n",
        "            if(j>=len( example.src_str[i])):\n",
        "                continue\n",
        "            candidate = example.src_str[i][j].strip()\n",
        "            if(self.args.block_trigram):\n",
        "                if(not _block_tri(candidate,_pred)):\n",
        "                    _pred.append(candidate)\n",
        "            else:\n",
        "                _pred.append(candidate)\n",
        "\n",
        "            # len(_pred) == 3 means that we limit sentences to top top_n_sentences\n",
        "            if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == top_n_sentences):\n",
        "                break\n",
        "\n",
        "        _pred = '<q>'.join(_pred)\n",
        "\n",
        "        pred.append(_pred)\n",
        "\n",
        "    results = {'source_article': source_article, 'predicted_summary': pred}\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qppmkPcwuJWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Trainer.test_api = test_api\n",
        "Trainer.example_api = example_api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUW1jLoZDHos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize_text(src_str, args, pp_args, top_n_sentences=3, tgt_str=''):\n",
        "    '''\n",
        "    Summarizes input text by returning the most important sentences based on the BERT model fine-tuned on CNN and Daily Mail articles\n",
        "    '''\n",
        "    cp = args.test_from\n",
        "    step = int(cp.split('.')[-2].split('_')[-1])\n",
        "    #Separate documents into list of sentences\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    src = [sent.split() for sent in nltk.tokenize.sent_tokenize(src_str)]\n",
        "    tgt = [sent.split() for sent in nltk.tokenize.sent_tokenize(tgt_str)]\n",
        "    bert = data_builder.BertData(pp_args)\n",
        "    oracle_ids = data_builder.greedy_selection(src, tgt, 3)\n",
        "    b_data = bert.preprocess(src, tgt, oracle_ids)\n",
        "    indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
        "    b_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids,\n",
        "              'clss': cls_ids, 'src_str': src_txt, \"tgt_str\": tgt_txt}\n",
        "    data = [[b_dict['src'], b_dict['labels'], b_dict['segs'], b_dict['clss'], b_dict['src_str'], b_dict['tgt_str']]]\n",
        "    batch = data_loader.Batch(data, is_test=True, device=device)\n",
        "    trained_model = torch.load(args.model_fp, map_location=lambda storage, loc: storage)\n",
        "    # Read BERT config file\n",
        "    # This contains information about the BERT model (e.g. hidden size for the transformer layers, number of transformer layers, number of self attention heads)\n",
        "    config = BertConfig.from_json_file(args.bert_config_path)\n",
        "    # Instantiate Summarizer and load pretrained model\n",
        "    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "    model.load_cp(trained_model)\n",
        "    device_id = 0 if device == \"cuda\" else -1\n",
        "    trainer = build_trainer(args, device_id, model, None)\n",
        "    results = trainer.example_api(batch, step, top_n_sentences=top_n_sentences, device=device)\n",
        "    return results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VunTUx2RFZSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ZER9nTFZrK",
        "colab_type": "text"
      },
      "source": [
        "## Sample run on Tiger Woods Wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9s_Iub8uYA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tiger_woods_wiki = '''Woods grew up in Orange County, California. He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods. Earl was a single-digit handicap amateur golfer who also was one of the earliest African-American college baseball players at Kansas State University. Tiger's father was a member of the military and had playing privileges at the Navy golf course beside the Joint Forces Training Base in Los Alamitos, which allowed Tiger to play there. Tiger also played at the par 3 Heartwell golf course in Long Beach, as well as some of the municipals in Long Beach. In 1978, Tiger putted against comedian Bob Hope in a television appearance on The Mike Douglas Show. At age three, he shot a 48 over nine holes at the Navy course. At age five, he appeared in Golf Digest and on ABC's That's Incredible! Before turning seven, Tiger won the Under Age 10 section of the Drive, Pitch, and Putt competition, held at the Navy Golf Course in Cypress, California. In 1984 at the age of eight, he won the 9–10 boys' event, the youngest age group available, at the Junior World Golf Championships. He first broke 80 at age eight. He went on to win the Junior World Championships six times, including four consecutive wins from 1988 to 1991. Woods' father Earl wrote that Tiger first defeated him at the age of 11 years, with Earl trying his best. Earl lost to Tiger every time from then on. Woods first broke 70 on a regulation golf course at age 12. When Woods was 13 years old, he played in the 1989 Big I, which was his first major national junior tournament. In the final round, he was paired with pro John Daly, who was then relatively unknown. The event's format placed a professional with each group of juniors who had qualified. Daly birdied three of the last four holes to beat Woods by only one stroke. As a young teenager, Woods first met Jack Nicklaus in Los Angeles at the Bel-Air Country Club, when Nicklaus was performing a clinic for the club's members. Woods was part of the show, and he impressed Nicklaus and the crowd with his skills and potential. Earl Woods had researched in detail the career accomplishments of Nicklaus and had set his young son the goals of breaking those records. Woods was 15 years old and a student at Western High School in Anaheim when he became the youngest U.S. Junior Amateur champion; this was a record that stood until it was broken by Jim Liu in 2010. He was named 1991's Southern California Amateur Player of the Year (for the second consecutive year) and Golf Digest Junior Amateur Player of the Year. In 1992, he defended his title at the U.S. Junior Amateur Championship, becoming the tournament's first two-time winner. He also competed in his first PGA Tour event, the Nissan Los Angeles Open (he missed the 36-hole cut), and was named Golf Digest Amateur Player of the Year, Golf World Player of the Year, and Golfweek National Amateur of the Year. The following year, Woods won his third consecutive U.S. Junior Amateur Championship; he remains the event's only three-time winner. In 1994, at the TPC at Sawgrass in Florida, he became the youngest winner of the U.S. Amateur Championship, a record he held until 2008 when it was broken by Danny Lee. He was a member of the American team at the 1994 Eisenhower Trophy World Amateur Golf Team Championships (winning), and the 1995 Walker Cup (losing). Woods graduated from Western High School at age 18 in 1994 and was voted \"Most Likely to Succeed\" among the graduating class. He had starred for the high school's golf team under coach Don Crosby. Woods overcame difficulties with stuttering as a boy. This was not known until he wrote a letter to a boy who contemplated suicide. Woods wrote, \"I know what it's like to be different and to sometimes not fit in. I also stuttered as a child and I would talk to my dog and he would sit there and listen until he fell asleep. I also took a class for two years to help me, and I finally learned to stop. '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srNZyuJizuC9",
        "colab_type": "code",
        "outputId": "61944937-915f-4656-e76b-7e5bbef74fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tiger_woods_wiki"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Woods grew up in Orange County, California. He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods. Earl was a single-digit handicap amateur golfer who also was one of the earliest African-American college baseball players at Kansas State University. Tiger\\'s father was a member of the military and had playing privileges at the Navy golf course beside the Joint Forces Training Base in Los Alamitos, which allowed Tiger to play there. Tiger also played at the par 3 Heartwell golf course in Long Beach, as well as some of the municipals in Long Beach. In 1978, Tiger putted against comedian Bob Hope in a television appearance on The Mike Douglas Show. At age three, he shot a 48 over nine holes at the Navy course. At age five, he appeared in Golf Digest and on ABC\\'s That\\'s Incredible! Before turning seven, Tiger won the Under Age 10 section of the Drive, Pitch, and Putt competition, held at the Navy Golf Course in Cypress, California. In 1984 at the age of eight, he won the 9–10 boys\\' event, the youngest age group available, at the Junior World Golf Championships. He first broke 80 at age eight. He went on to win the Junior World Championships six times, including four consecutive wins from 1988 to 1991. Woods\\' father Earl wrote that Tiger first defeated him at the age of 11 years, with Earl trying his best. Earl lost to Tiger every time from then on. Woods first broke 70 on a regulation golf course at age 12. When Woods was 13 years old, he played in the 1989 Big I, which was his first major national junior tournament. In the final round, he was paired with pro John Daly, who was then relatively unknown. The event\\'s format placed a professional with each group of juniors who had qualified. Daly birdied three of the last four holes to beat Woods by only one stroke. As a young teenager, Woods first met Jack Nicklaus in Los Angeles at the Bel-Air Country Club, when Nicklaus was performing a clinic for the club\\'s members. Woods was part of the show, and he impressed Nicklaus and the crowd with his skills and potential. Earl Woods had researched in detail the career accomplishments of Nicklaus and had set his young son the goals of breaking those records. Woods was 15 years old and a student at Western High School in Anaheim when he became the youngest U.S. Junior Amateur champion; this was a record that stood until it was broken by Jim Liu in 2010. He was named 1991\\'s Southern California Amateur Player of the Year (for the second consecutive year) and Golf Digest Junior Amateur Player of the Year. In 1992, he defended his title at the U.S. Junior Amateur Championship, becoming the tournament\\'s first two-time winner. He also competed in his first PGA Tour event, the Nissan Los Angeles Open (he missed the 36-hole cut), and was named Golf Digest Amateur Player of the Year, Golf World Player of the Year, and Golfweek National Amateur of the Year. The following year, Woods won his third consecutive U.S. Junior Amateur Championship; he remains the event\\'s only three-time winner. In 1994, at the TPC at Sawgrass in Florida, he became the youngest winner of the U.S. Amateur Championship, a record he held until 2008 when it was broken by Danny Lee. He was a member of the American team at the 1994 Eisenhower Trophy World Amateur Golf Team Championships (winning), and the 1995 Walker Cup (losing). Woods graduated from Western High School at age 18 in 1994 and was voted \"Most Likely to Succeed\" among the graduating class. He had starred for the high school\\'s golf team under coach Don Crosby. Woods overcame difficulties with stuttering as a boy. This was not known until he wrote a letter to a boy who contemplated suicide. Woods wrote, \"I know what it\\'s like to be different and to sometimes not fit in. I also stuttered as a child and I would talk to my dog and he would sit there and listen until he fell asleep. I also took a class for two years to help me, and I finally learned to stop. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKLR9x7Rzv83",
        "colab_type": "code",
        "outputId": "d0df613a-7555-419c-b3b9-f1888d8c7be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%%time\n",
        "summarize_text(tiger_woods_wiki, args, pp_args, tgt_str='', top_n_sentences=1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 5631259.27B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gpu_rank 0\n",
            "CPU times: user 4.03 s, sys: 2.38 s, total: 6.41 s\n",
            "Wall time: 7.18 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'predicted_summary': ['He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods.'],\n",
              " 'source_article': ['Woods grew up in Orange County, California. He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods. Earl was a single-digit handicap amateur golfer who also was one of the earliest African-American college baseball players at Kansas State University. Tiger\\'s father was a member of the military and had playing privileges at the Navy golf course beside the Joint Forces Training Base in Los Alamitos, which allowed Tiger to play there. Tiger also played at the par 3 Heartwell golf course in Long Beach, as well as some of the municipals in Long Beach. In 1978, Tiger putted against comedian Bob Hope in a television appearance on The Mike Douglas Show. At age three, he shot a 48 over nine holes at the Navy course. At age five, he appeared in Golf Digest and on ABC\\'s That\\'s Incredible! Before turning seven, Tiger won the Under Age 10 section of the Drive, Pitch, and Putt competition, held at the Navy Golf Course in Cypress, California. In 1984 at the age of eight, he won the 9–10 boys\\' event, the youngest age group available, at the Junior World Golf Championships. He first broke 80 at age eight. He went on to win the Junior World Championships six times, including four consecutive wins from 1988 to 1991. Woods\\' father Earl wrote that Tiger first defeated him at the age of 11 years, with Earl trying his best. Earl lost to Tiger every time from then on. Woods first broke 70 on a regulation golf course at age 12. When Woods was 13 years old, he played in the 1989 Big I, which was his first major national junior tournament. In the final round, he was paired with pro John Daly, who was then relatively unknown. The event\\'s format placed a professional with each group of juniors who had qualified. Daly birdied three of the last four holes to beat Woods by only one stroke. As a young teenager, Woods first met Jack Nicklaus in Los Angeles at the Bel-Air Country Club, when Nicklaus was performing a clinic for the club\\'s members. Woods was part of the show, and he impressed Nicklaus and the crowd with his skills and potential. Earl Woods had researched in detail the career accomplishments of Nicklaus and had set his young son the goals of breaking those records. Woods was 15 years old and a student at Western High School in Anaheim when he became the youngest U.S. Junior Amateur champion; this was a record that stood until it was broken by Jim Liu in 2010. He was named 1991\\'s Southern California Amateur Player of the Year (for the second consecutive year) and Golf Digest Junior Amateur Player of the Year. In 1992, he defended his title at the U.S. Junior Amateur Championship, becoming the tournament\\'s first two-time winner. He also competed in his first PGA Tour event, the Nissan Los Angeles Open (he missed the 36-hole cut), and was named Golf Digest Amateur Player of the Year, Golf World Player of the Year, and Golfweek National Amateur of the Year. The following year, Woods won his third consecutive U.S. Junior Amateur Championship; he remains the event\\'s only three-time winner. In 1994, at the TPC at Sawgrass in Florida, he became the youngest winner of the U.S. Amateur Championship, a record he held until 2008 when it was broken by Danny Lee. He was a member of the American team at the 1994 Eisenhower Trophy World Amateur Golf Team Championships (winning), and the 1995 Walker Cup (losing). Woods graduated from Western High School at age 18 in 1994 and was voted \"Most Likely to Succeed\" among the graduating class. He had starred for the high school\\'s golf team under coach Don Crosby. Woods overcame difficulties with stuttering as a boy. This was not known until he wrote a letter to a boy who contemplated suicide. Woods wrote, \"I know what it\\'s like to be different and to sometimes not fit in. I also stuttered as a child and I would talk to my dog and he would sit there and listen until he fell asleep. I also took a class for two years to help me, and I finally learned to stop.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WM9W5oAFHbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}