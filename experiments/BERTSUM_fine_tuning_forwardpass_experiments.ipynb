{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTSUM_fine-tuning_forwardpass_experiments.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "5uoSgF4e_PCb",
        "EXRJ7SVkdb05"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re1fJcNGNeLA",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Ci0MfBqiL8",
        "colab_type": "code",
        "outputId": "c937a5a5-0971-4987-8a77-73350a27dbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install googledrivedownloader\n",
        "!pip install pyrouge\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.157)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.157 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.157)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.157->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.157->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.157->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.4)\n",
            "Collecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXP9_LQCZ5J9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_from_gdrive(file_id, destination_path, unzip=False):\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "    gdd.download_file_from_google_drive(file_id=file_id,\n",
        "                                        dest_path=destination_path,\n",
        "                                        unzip=unzip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJLajp7X1fPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl3Xv0k2dT3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROCESSED_DATA = '1-NJKRoNzk2ugjjB2mfnmpR15KYB_Fr0s'\n",
        "CNN_STORIES = '0BwmD_VLjROrfTHk4NFg2SndKcjQ'\n",
        "DAILY_MAIN_STORIES = '0BwmD_VLjROrfM1BxdkxVaTY2bWs'\n",
        "CORENLP_PATH = '/home/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar'\n",
        "MAP_PATH = '/home/BertSum/urls/'\n",
        "SIMPLER_JSON_PATH = '/home/json_data/cnndm'\n",
        "TOKENIZED_PATH = '/home/merged_stories_tokenized/'\n",
        "RAW_PATH = '/home/raw_stories/'\n",
        "FINAL_JSON_PATH = '/home/json_data/'\n",
        "BERT_DATA_PATH = '/home/bert_data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6TrpXrOvcGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf2-21P-dGuY",
        "colab_type": "text"
      },
      "source": [
        "# BERSTUM - High level implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eS7eMxlNhhm",
        "colab_type": "text"
      },
      "source": [
        "## Prepare sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD-l5J9keVku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip -P /home &>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoE2B2HhfMW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /home/stanford-corenlp-full-2018-10-05.zip -d /home &>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwcqmLe8t00g",
        "colab_type": "code",
        "outputId": "5bfa30da-39ad-4672-f177-096d7e17c6c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls /home/stanford-corenlp-full-2018-10-05/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "build.xml                                 LICENSE.txt\n",
            "\u001b[0m\u001b[01;32mcorenlp.sh\u001b[0m*                               Makefile\n",
            "\u001b[01;32mCoreNLP-to-HTML.xsl\u001b[0m*                      \u001b[01;34mpatterns\u001b[0m/\n",
            "ejml-0.23.jar                             pom-java-11.xml\n",
            "ejml-0.23-src.zip                         pom.xml\n",
            "input.txt                                 protobuf.jar\n",
            "input.txt.xml                             README.txt\n",
            "javax.activation-api-1.2.0.jar            RESOURCE-LICENSES\n",
            "javax.activation-api-1.2.0-sources.jar    SemgrexDemo.java\n",
            "javax.json-api-1.0-sources.jar            ShiftReduceDemo.java\n",
            "javax.json.jar                            slf4j-api.jar\n",
            "jaxb-api-2.4.0-b180830.0359.jar           slf4j-simple.jar\n",
            "jaxb-api-2.4.0-b180830.0359-sources.jar   stanford-corenlp-3.9.2.jar\n",
            "jaxb-core-2.3.0.1.jar                     stanford-corenlp-3.9.2-javadoc.jar\n",
            "jaxb-core-2.3.0.1-sources.jar             stanford-corenlp-3.9.2-models.jar\n",
            "jaxb-impl-2.4.0-b180830.0438.jar          stanford-corenlp-3.9.2-sources.jar\n",
            "jaxb-impl-2.4.0-b180830.0438-sources.jar  StanfordCoreNlpDemo.java\n",
            "joda-time-2.9-sources.jar                 StanfordDependenciesManual.pdf\n",
            "joda-time.jar                             \u001b[01;34msutime\u001b[0m/\n",
            "jollyday-0.4.9-sources.jar                \u001b[01;34mtokensregex\u001b[0m/\n",
            "jollyday.jar                              xom-1.2.10-src.jar\n",
            "LIBRARY-LICENSES                          xom.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-sJYy3hfQkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!export CLASSPATH=/root/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar#:/root/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjVoe0RlbUsn",
        "colab_type": "code",
        "outputId": "6a7ca56f-0a52-4cc8-f292-91d8a4ea49e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#download_from_gdrive(file_id=PROCESSED_DATA, destination_path='/home/bert_data/cnn_dailymail.zip', unzip=True)\n",
        "#!rm /home/bert_data/cnn_dailymail.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1-NJKRoNzk2ugjjB2mfnmpR15KYB_Fr0s into /home/bert_data/cnn_dailymail.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtvCmXQPfwG3",
        "colab_type": "code",
        "outputId": "9dee8665-7902-46a2-cc30-b0b51b29eedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir {TOKENIZED_PATH}\n",
        "!mkdir /home/raw_stories # This is where we store the individual story files\n",
        "!mkdir /home/story_dump # This is where we store the story files from the gdrive download"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/home/story_dump’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRmOiQUpIiol",
        "colab_type": "code",
        "outputId": "df1a577f-96a9-4259-f4fa-1ed50ea2000f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir /home/story_dump/all_stories"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/home/story_dump/all_stories’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iC8w9iOe-bl",
        "colab_type": "code",
        "outputId": "075a618b-ebc8-4e22-f2ca-d223406b1d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "download_from_gdrive(file_id=CNN_STORIES, destination_path='/home/story_dump/cnn_stories.tgz', unzip=False)\n",
        "download_from_gdrive(file_id=DAILY_MAIN_STORIES, destination_path='/home/story_dump/dailymail_stories.tgz', unzip=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 0BwmD_VLjROrfTHk4NFg2SndKcjQ into /home/story_dump/cnn_stories.tgz... Done.\n",
            "Downloading 0BwmD_VLjROrfM1BxdkxVaTY2bWs into /home/story_dump/dailymail_stories.tgz... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K81hRyUbdEQ",
        "colab_type": "code",
        "outputId": "de643a85-a485-4dee-ca35-245601d6c47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/bert_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/home/bert_data': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53vB6KcggYAQ",
        "colab_type": "code",
        "outputId": "0f188e43-5323-4b61-d6a5-00d169a32c60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/story_dump"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mall_stories\u001b[0m/  cnn_stories.tgz  dailymail_stories.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4W3587ictUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xvzf /home/story_dump/cnn_stories.tgz  -C /home/story_dump/all_stories/ &>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OPdFWO-g6x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xvzf /home/story_dump/dailymail_stories.tgz  -C /home/story_dump/all_stories/ &>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2d_8yiSIXSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_stories = !ls /home/story_dump/all_stories/cnn/stories\n",
        "dailymail_stories = !ls /home/story_dump/all_stories/dailymail/stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6wLrz-YIb6X",
        "colab_type": "code",
        "outputId": "b317a531-7358-471c-8d02-e5ab1b2d06f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(cnn_stories), len(dailymail_stories)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92579, 219506)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThoBZfqZK-Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_stories = cnn_stories + dailymail_stories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0l1Xl8NLBhs",
        "colab_type": "code",
        "outputId": "779560cc-ec2e-4a78-e123-b390f44ebc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "all_stories[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0001d1afc246a7964130f43ae940af6bc6c57f01.story',\n",
              " '0002095e55fcbd3a2f366d9bf92a95433dc305ef.story',\n",
              " '00027e965c8264c35cc1bc55556db388da82b07f.story',\n",
              " '0002c17436637c4fe1837c935c04de47adb18e9a.story',\n",
              " '0003ad6ef0c37534f80b55b4235108024b407f0b.story']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu03EXwkL6o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6VSdR2dL-BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random sample of 5 stories for both cnn and daily mail (total sample size of 10)\n",
        "sample_cnn_stories = np.random.permutation(cnn_stories)[:5]\n",
        "sample_dailymail_stories = np.random.permutation(dailymail_stories)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnqhfmtcMfHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for story_ in sample_cnn_stories:\n",
        "    !cp /home/story_dump/all_stories/cnn/stories/{story_} /home/story_dump/\n",
        "\n",
        "for story_ in sample_dailymail_stories:\n",
        "    !cp /home/story_dump/all_stories/dailymail/stories/{story_} /home/story_dump/\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAElDfdfNa-I",
        "colab_type": "code",
        "outputId": "c6a4d0ac-863c-4b2e-b78a-bd047d8c3196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "ls /home/story_dump/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story\n",
            "5b76984e7d6c3ffb9e10b0fe9b356e90beac4c0b.story\n",
            "63f019e87c5c3e7cb247a6942832c9395340e733.story\n",
            "69a61a5463de5eea642fe884d019c6b4b4c2ee9f.story\n",
            "6e4e310308475f1c7a47dc98074bb187bfd05e72.story\n",
            "965ec9ba5b923cdf09b97fa7eadc79d7a48438ed.story\n",
            "\u001b[0m\u001b[01;34mall_stories\u001b[0m/\n",
            "cnn_stories.tgz\n",
            "d50ea3563e1cd9c0d744d0e8b1a4ba6f8bb2c7dc.story\n",
            "dailymail_stories.tgz\n",
            "e25064cc1a7a29921c79bc8e1d476a4a8fbabf60.story\n",
            "f5b6b1729d6088f07e7c43afbea9bc685ae054f1.story\n",
            "ff0345888fafc20f762f3a74fac915a137b6a942.story\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFuIEvZ4NqcX",
        "colab_type": "text"
      },
      "source": [
        "## Text pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78GtKGHuotRU",
        "colab_type": "code",
        "outputId": "5e99aced-3e3c-48d5-ef58-80cc714d74ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /home"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AG506Xso0yG",
        "colab_type": "code",
        "outputId": "3adfb4d5-c8f1-4a8a-e8dd-316aeb08e3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/thinkingmachines/BertSum.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BertSum'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 303 (delta 19), reused 25 (delta 13), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (303/303), 15.04 MiB | 12.70 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5I4I7uZq9Pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /logs/ # Used to store log files for preprocess.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxU_RGhp8fcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /home/json_data/\n",
        "!mkdir {SIMPLER_JSON_PATH}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGafTPVz6Mp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!for i in /home/story_dump/*.story; do mv \"$i\" /home/raw_stories/; done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdKlIf3upXil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /home/BertSum/src/preprocess.py -mode tokenize -raw_path {RAW_PATH} -save_path {TOKENIZED_PATH} -corenlp_path {CORENLP_PATH} &>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP95LVWLFZ8r",
        "colab_type": "code",
        "outputId": "1c130cae-4d98-41c8-c42a-105921aef48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls -ltr {RAW_PATH}| tail -2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 5135 May 15 01:57 0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story\n",
            "-rw-r--r-- 1 root root 4152 May 15 01:57 f5b6b1729d6088f07e7c43afbea9bc685ae054f1.story\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ABUkCuFg_3",
        "colab_type": "code",
        "outputId": "ffb24a09-bc97-4da4-d558-a267ce072ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!tail -20 {RAW_PATH}0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "@highlight\n",
            "\n",
            "One died in the flames while the other succumbed to severe burns on the way to hospital\n",
            "\n",
            "@highlight\n",
            "\n",
            "Initial investigation suggests fire was not suspicious but twins had tried to kill one another before\n",
            "\n",
            "@highlight\n",
            "\n",
            "Pair were addicted to long distance running and fractured their feet endlessly attempting marathons\n",
            "\n",
            "@highlight\n",
            "\n",
            "Authorities considered locking sisters up to stop them starving themselves as each weighed little more than four stone \n",
            "\n",
            "@highlight\n",
            "\n",
            "Twins predicted they would 'die together' as they transformed into living skeletons"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68byLVcFFyWv",
        "colab_type": "code",
        "outputId": "26f78a8e-a6e7-4ac5-97b5-94331e10c629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!head -20 {RAW_PATH}0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "By\n",
            "Richard  Shears\n",
            "\n",
            "PUBLISHED:\n",
            "  \n",
            "  \n",
            "      22:02 EST, 27 August 2012\n",
            "    \n",
            "  \n",
            "\n",
            " | \n",
            "  UPDATED:\n",
            "  \n",
            "  \n",
            "      06:45 EST, 28 August 2012\n",
            "\n",
            "A pair of identical twins, who became famous through their desperate battle with anorexia, have died in a house fire.\n",
            "\n",
            "Clare and Rachel Wallmeyer, 42, were killed after a fire broke out in their home in Geelong, near Melbourne, one perishing in the flames, the other succumbing to her severe burns on the way to hospital.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyq0DXvEF4xa",
        "colab_type": "code",
        "outputId": "74194333-9c35-42c4-9f55-ad11cb2341a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2349
        }
      },
      "source": [
        "# Notice the story starts with the article and ends with the highlighted sentences\n",
        "cat {RAW_PATH}0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "By\n",
            "Richard  Shears\n",
            "\n",
            "PUBLISHED:\n",
            "  \n",
            "  \n",
            "      22:02 EST, 27 August 2012\n",
            "    \n",
            "  \n",
            "\n",
            " | \n",
            "  UPDATED:\n",
            "  \n",
            "  \n",
            "      06:45 EST, 28 August 2012\n",
            "\n",
            "A pair of identical twins, who became famous through their desperate battle with anorexia, have died in a house fire.\n",
            "\n",
            "Clare and Rachel Wallmeyer, 42, were killed after a fire broke out in their home in Geelong, near Melbourne, one perishing in the flames, the other succumbing to her severe burns on the way to hospital.\n",
            "\n",
            "It was a tragic end to two turbulent lives, for the sisters had appeared on Australian TV several times to talk about the anorexia which had turned both into virtual living skeletons and a problem pair for their parents, social workers and the police.\n",
            "\n",
            "Scroll down for video\n",
            "\n",
            "Tragic: Identical twins Clare and Rachel Wallmeyer, 42, died when fire swept through their home in Geelong, near Melbourne\n",
            "\n",
            "'At least we'll die together': Twins predicted their sad demise\n",
            "\n",
            "In a poignant review of their lives they said in recent years that they had never been in love, never had a job and they believed that it was only a matter of time before they died – and they would die together.\n",
            "\n",
            "Their deaths in the fire are believed to have been accidental, according to detectives from the Geelong Crime Investigation Unit who said that initial checks did not reveal any suspicious activity.\n",
            "\n",
            "Yet there had been reports over the years of the women each trying to kill one another.\n",
            "\n",
            "Rachel was charged with the attempted\n",
            " murder of Clare after police, who were called to their home, claimed \n",
            "they witnessed Rachel with her hands locked around her sister’s throat.\n",
            "\n",
            "The charge was later withdrawn.\n",
            "\n",
            "Their existence, balanced between \n",
            "life and death, had resulted in TV companies searching them out for \n",
            "interviews after authorities considered jailing them in an attempt to \n",
            "stop the women starving themselves to death and ‘turn their lives \n",
            "around.’\n",
            "\n",
            "In fact Clare \n",
            "was later jailed by a Geelong court for a series of thefts – but only \n",
            "after magistrate Ian von Einem said he saw no option but to send her to \n",
            "prison to stop her from self-destructing.\n",
            "\n",
            "Her\n",
            " sister also presented a headache for the authorities when she was \n",
            "arrested for driving under the influence of drugs and was also accused \n",
            "of pushing a victim on to train tracks.\n",
            "\n",
            "She received a 21-month suspended\n",
            " jail sentence.\n",
            "\n",
            "The \n",
            "women, who were compulsive long-distance runners, described themselves \n",
            "as perfectionists in biomedical science and physical education, topics \n",
            "they studied avidly, side by side.\n",
            "\n",
            "But as they started to waste away \n",
            "over two decades, the weight of each of them dropped to little more than\n",
            " four stone. Doctors said they had the bone structure of women aged \n",
            "between 70 and 100.\n",
            "\n",
            "The \n",
            "twins developed severe eating disorders in their early teens losing more\n",
            " weight when they became addicted to long-distance running.\n",
            "\n",
            "They were so\n",
            " obsessed with marathons that they each suffered stress fractures in \n",
            "their feet.\n",
            "\n",
            "Problem pair: Clare and Rachel Wallmeyer pictured leaving a police station in Geelong, Australia in 2007\n",
            "\n",
            "Blaze: The home in Geelong, Australia, where the anorexic twin sisters died\n",
            "\n",
            "Inseparable to their tragic end, \n",
            "there was the time in 1996 when Rachel was so ill she was admitted to a \n",
            "psychiatric unit at the Royal Melbourne Hospital – followed by Clare who\n",
            " voluntarily admitted herself too.\n",
            "\n",
            "Rachel told Melbourne’s Herald Sun \n",
            "newspaper on one occasion that no-one understood anorexia until they \n",
            "have lived it.\n",
            "\n",
            "‘It’s like the Grim Reaper – a black hole in your soul,’ \n",
            "she said.\n",
            "\n",
            "Their \n",
            "parents, Bob and Moya admitted that when the twins were teenagers they \n",
            "feared they would find them dead in bed because of their disorder.\n",
            "\n",
            "Happier times: But behind the smiles the two sisters led a turbulent, troubled existence. Pictured with friend Rachael Walker\n",
            "\n",
            "Desperate: Authorities considered jailing them in an attempt to stop Rachel, left, and Clare, right, starving themselves to death\n",
            "\n",
            "In an interview with Australia’s 60 Minutes programme the twins gave a startling insight into their eating habits.\n",
            "\n",
            "Said Clare: ‘Essentially, we don’t eat anything. We might have a piece of watermelon.’\n",
            "\n",
            "Rachel added: ‘And Diet Coke we have, and coffee.’\n",
            "\n",
            "They also revealed they took at least 20 laxatives.\n",
            "\n",
            "Rachel said that Clare was the only person who remained by her side. ‘And at least we’ll die together.’\n",
            "\n",
            "Clare said: ‘Being with Rachel…makes it somewhat easier to die.’\n",
            "\n",
            "@highlight\n",
            "\n",
            "Clare and Rachel Wallmeyer, 42, died when fire swept through their home in Geelong, near Melbourne\n",
            "\n",
            "@highlight\n",
            "\n",
            "One died in the flames while the other succumbed to severe burns on the way to hospital\n",
            "\n",
            "@highlight\n",
            "\n",
            "Initial investigation suggests fire was not suspicious but twins had tried to kill one another before\n",
            "\n",
            "@highlight\n",
            "\n",
            "Pair were addicted to long distance running and fractured their feet endlessly attempting marathons\n",
            "\n",
            "@highlight\n",
            "\n",
            "Authorities considered locking sisters up to stop them starving themselves as each weighed little more than four stone \n",
            "\n",
            "@highlight\n",
            "\n",
            "Twins predicted they would 'die together' as they transformed into living skeletons"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kM2mabWGW5Z",
        "colab_type": "code",
        "outputId": "05a62632-cb47-40fe-bb54-0a6f26bbcd5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls -ltr {TOKENIZED_PATH}| tail -2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 171144 May 15 02:01 5b76984e7d6c3ffb9e10b0fe9b356e90beac4c0b.story.json\n",
            "-rw-r--r-- 1 root root 219086 May 15 02:01 0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RlLyfaGbTB",
        "colab_type": "code",
        "outputId": "bb1fea8c-6245-47e8-aca3-c2febd35863c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149940
        }
      },
      "source": [
        "!cat {TOKENIZED_PATH}0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"docId\": \"0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story\",\n",
            "  \"sentences\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"By\",\n",
            "          \"originalText\": \"By\",\n",
            "          \"characterOffsetBegin\": 0,\n",
            "          \"characterOffsetEnd\": 2,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 1,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Richard\",\n",
            "          \"originalText\": \"Richard\",\n",
            "          \"characterOffsetBegin\": 3,\n",
            "          \"characterOffsetEnd\": 10,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"  \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"Shears\",\n",
            "          \"originalText\": \"Shears\",\n",
            "          \"characterOffsetBegin\": 12,\n",
            "          \"characterOffsetEnd\": 18,\n",
            "          \"before\": \"  \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 2,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"PUBLISHED\",\n",
            "          \"originalText\": \"PUBLISHED\",\n",
            "          \"characterOffsetBegin\": 20,\n",
            "          \"characterOffsetEnd\": 29,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 29,\n",
            "          \"characterOffsetEnd\": 30,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 3,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"22:02\",\n",
            "          \"originalText\": \"22:02\",\n",
            "          \"characterOffsetBegin\": 43,\n",
            "          \"characterOffsetEnd\": 48,\n",
            "          \"before\": \"\\n      \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"EST\",\n",
            "          \"originalText\": \"EST\",\n",
            "          \"characterOffsetBegin\": 49,\n",
            "          \"characterOffsetEnd\": 52,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 52,\n",
            "          \"characterOffsetEnd\": 53,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"27\",\n",
            "          \"originalText\": \"27\",\n",
            "          \"characterOffsetBegin\": 54,\n",
            "          \"characterOffsetEnd\": 56,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"August\",\n",
            "          \"originalText\": \"August\",\n",
            "          \"characterOffsetBegin\": 57,\n",
            "          \"characterOffsetEnd\": 63,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"2012\",\n",
            "          \"originalText\": \"2012\",\n",
            "          \"characterOffsetBegin\": 64,\n",
            "          \"characterOffsetEnd\": 68,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 4,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"|\",\n",
            "          \"originalText\": \"|\",\n",
            "          \"characterOffsetBegin\": 79,\n",
            "          \"characterOffsetEnd\": 80,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 5,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"UPDATED\",\n",
            "          \"originalText\": \"UPDATED\",\n",
            "          \"characterOffsetBegin\": 84,\n",
            "          \"characterOffsetEnd\": 91,\n",
            "          \"before\": \"\\n  \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 91,\n",
            "          \"characterOffsetEnd\": 92,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 6,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"06:45\",\n",
            "          \"originalText\": \"06:45\",\n",
            "          \"characterOffsetBegin\": 105,\n",
            "          \"characterOffsetEnd\": 110,\n",
            "          \"before\": \"\\n      \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"EST\",\n",
            "          \"originalText\": \"EST\",\n",
            "          \"characterOffsetBegin\": 111,\n",
            "          \"characterOffsetEnd\": 114,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 114,\n",
            "          \"characterOffsetEnd\": 115,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"28\",\n",
            "          \"originalText\": \"28\",\n",
            "          \"characterOffsetBegin\": 116,\n",
            "          \"characterOffsetEnd\": 118,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"August\",\n",
            "          \"originalText\": \"August\",\n",
            "          \"characterOffsetBegin\": 119,\n",
            "          \"characterOffsetEnd\": 125,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"2012\",\n",
            "          \"originalText\": \"2012\",\n",
            "          \"characterOffsetBegin\": 126,\n",
            "          \"characterOffsetEnd\": 130,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 7,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"A\",\n",
            "          \"originalText\": \"A\",\n",
            "          \"characterOffsetBegin\": 132,\n",
            "          \"characterOffsetEnd\": 133,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"pair\",\n",
            "          \"originalText\": \"pair\",\n",
            "          \"characterOffsetBegin\": 134,\n",
            "          \"characterOffsetEnd\": 138,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 139,\n",
            "          \"characterOffsetEnd\": 141,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"identical\",\n",
            "          \"originalText\": \"identical\",\n",
            "          \"characterOffsetBegin\": 142,\n",
            "          \"characterOffsetEnd\": 151,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"twins\",\n",
            "          \"originalText\": \"twins\",\n",
            "          \"characterOffsetBegin\": 152,\n",
            "          \"characterOffsetEnd\": 157,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 157,\n",
            "          \"characterOffsetEnd\": 158,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"who\",\n",
            "          \"originalText\": \"who\",\n",
            "          \"characterOffsetBegin\": 159,\n",
            "          \"characterOffsetEnd\": 162,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"became\",\n",
            "          \"originalText\": \"became\",\n",
            "          \"characterOffsetBegin\": 163,\n",
            "          \"characterOffsetEnd\": 169,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"famous\",\n",
            "          \"originalText\": \"famous\",\n",
            "          \"characterOffsetBegin\": 170,\n",
            "          \"characterOffsetEnd\": 176,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"through\",\n",
            "          \"originalText\": \"through\",\n",
            "          \"characterOffsetBegin\": 177,\n",
            "          \"characterOffsetEnd\": 184,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 185,\n",
            "          \"characterOffsetEnd\": 190,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"desperate\",\n",
            "          \"originalText\": \"desperate\",\n",
            "          \"characterOffsetBegin\": 191,\n",
            "          \"characterOffsetEnd\": 200,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"battle\",\n",
            "          \"originalText\": \"battle\",\n",
            "          \"characterOffsetBegin\": 201,\n",
            "          \"characterOffsetEnd\": 207,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"characterOffsetBegin\": 208,\n",
            "          \"characterOffsetEnd\": 212,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"anorexia\",\n",
            "          \"originalText\": \"anorexia\",\n",
            "          \"characterOffsetBegin\": 213,\n",
            "          \"characterOffsetEnd\": 221,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 221,\n",
            "          \"characterOffsetEnd\": 222,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"have\",\n",
            "          \"originalText\": \"have\",\n",
            "          \"characterOffsetBegin\": 223,\n",
            "          \"characterOffsetEnd\": 227,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"died\",\n",
            "          \"originalText\": \"died\",\n",
            "          \"characterOffsetBegin\": 228,\n",
            "          \"characterOffsetEnd\": 232,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 233,\n",
            "          \"characterOffsetEnd\": 235,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 236,\n",
            "          \"characterOffsetEnd\": 237,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 21,\n",
            "          \"word\": \"house\",\n",
            "          \"originalText\": \"house\",\n",
            "          \"characterOffsetBegin\": 238,\n",
            "          \"characterOffsetEnd\": 243,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 22,\n",
            "          \"word\": \"fire\",\n",
            "          \"originalText\": \"fire\",\n",
            "          \"characterOffsetBegin\": 244,\n",
            "          \"characterOffsetEnd\": 248,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 23,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 248,\n",
            "          \"characterOffsetEnd\": 249,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 8,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 251,\n",
            "          \"characterOffsetEnd\": 256,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 257,\n",
            "          \"characterOffsetEnd\": 260,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 261,\n",
            "          \"characterOffsetEnd\": 267,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"Wallmeyer\",\n",
            "          \"originalText\": \"Wallmeyer\",\n",
            "          \"characterOffsetBegin\": 268,\n",
            "          \"characterOffsetEnd\": 277,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 277,\n",
            "          \"characterOffsetEnd\": 278,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"42\",\n",
            "          \"originalText\": \"42\",\n",
            "          \"characterOffsetBegin\": 279,\n",
            "          \"characterOffsetEnd\": 281,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 281,\n",
            "          \"characterOffsetEnd\": 282,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"were\",\n",
            "          \"originalText\": \"were\",\n",
            "          \"characterOffsetBegin\": 283,\n",
            "          \"characterOffsetEnd\": 287,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"killed\",\n",
            "          \"originalText\": \"killed\",\n",
            "          \"characterOffsetBegin\": 288,\n",
            "          \"characterOffsetEnd\": 294,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"after\",\n",
            "          \"originalText\": \"after\",\n",
            "          \"characterOffsetBegin\": 295,\n",
            "          \"characterOffsetEnd\": 300,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 301,\n",
            "          \"characterOffsetEnd\": 302,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"fire\",\n",
            "          \"originalText\": \"fire\",\n",
            "          \"characterOffsetBegin\": 303,\n",
            "          \"characterOffsetEnd\": 307,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"broke\",\n",
            "          \"originalText\": \"broke\",\n",
            "          \"characterOffsetBegin\": 308,\n",
            "          \"characterOffsetEnd\": 313,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"out\",\n",
            "          \"originalText\": \"out\",\n",
            "          \"characterOffsetBegin\": 314,\n",
            "          \"characterOffsetEnd\": 317,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 318,\n",
            "          \"characterOffsetEnd\": 320,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 321,\n",
            "          \"characterOffsetEnd\": 326,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"home\",\n",
            "          \"originalText\": \"home\",\n",
            "          \"characterOffsetBegin\": 327,\n",
            "          \"characterOffsetEnd\": 331,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 332,\n",
            "          \"characterOffsetEnd\": 334,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"Geelong\",\n",
            "          \"originalText\": \"Geelong\",\n",
            "          \"characterOffsetBegin\": 335,\n",
            "          \"characterOffsetEnd\": 342,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 342,\n",
            "          \"characterOffsetEnd\": 343,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 21,\n",
            "          \"word\": \"near\",\n",
            "          \"originalText\": \"near\",\n",
            "          \"characterOffsetBegin\": 344,\n",
            "          \"characterOffsetEnd\": 348,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 22,\n",
            "          \"word\": \"Melbourne\",\n",
            "          \"originalText\": \"Melbourne\",\n",
            "          \"characterOffsetBegin\": 349,\n",
            "          \"characterOffsetEnd\": 358,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 23,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 358,\n",
            "          \"characterOffsetEnd\": 359,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 24,\n",
            "          \"word\": \"one\",\n",
            "          \"originalText\": \"one\",\n",
            "          \"characterOffsetBegin\": 360,\n",
            "          \"characterOffsetEnd\": 363,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 25,\n",
            "          \"word\": \"perishing\",\n",
            "          \"originalText\": \"perishing\",\n",
            "          \"characterOffsetBegin\": 364,\n",
            "          \"characterOffsetEnd\": 373,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 26,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 374,\n",
            "          \"characterOffsetEnd\": 376,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 27,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 377,\n",
            "          \"characterOffsetEnd\": 380,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 28,\n",
            "          \"word\": \"flames\",\n",
            "          \"originalText\": \"flames\",\n",
            "          \"characterOffsetBegin\": 381,\n",
            "          \"characterOffsetEnd\": 387,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 29,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 387,\n",
            "          \"characterOffsetEnd\": 388,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 30,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 389,\n",
            "          \"characterOffsetEnd\": 392,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 31,\n",
            "          \"word\": \"other\",\n",
            "          \"originalText\": \"other\",\n",
            "          \"characterOffsetBegin\": 393,\n",
            "          \"characterOffsetEnd\": 398,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 32,\n",
            "          \"word\": \"succumbing\",\n",
            "          \"originalText\": \"succumbing\",\n",
            "          \"characterOffsetBegin\": 399,\n",
            "          \"characterOffsetEnd\": 409,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 33,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 410,\n",
            "          \"characterOffsetEnd\": 412,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 34,\n",
            "          \"word\": \"her\",\n",
            "          \"originalText\": \"her\",\n",
            "          \"characterOffsetBegin\": 413,\n",
            "          \"characterOffsetEnd\": 416,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 35,\n",
            "          \"word\": \"severe\",\n",
            "          \"originalText\": \"severe\",\n",
            "          \"characterOffsetBegin\": 417,\n",
            "          \"characterOffsetEnd\": 423,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 36,\n",
            "          \"word\": \"burns\",\n",
            "          \"originalText\": \"burns\",\n",
            "          \"characterOffsetBegin\": 424,\n",
            "          \"characterOffsetEnd\": 429,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 37,\n",
            "          \"word\": \"on\",\n",
            "          \"originalText\": \"on\",\n",
            "          \"characterOffsetBegin\": 430,\n",
            "          \"characterOffsetEnd\": 432,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 38,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 433,\n",
            "          \"characterOffsetEnd\": 436,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 39,\n",
            "          \"word\": \"way\",\n",
            "          \"originalText\": \"way\",\n",
            "          \"characterOffsetBegin\": 437,\n",
            "          \"characterOffsetEnd\": 440,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 40,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 441,\n",
            "          \"characterOffsetEnd\": 443,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 41,\n",
            "          \"word\": \"hospital\",\n",
            "          \"originalText\": \"hospital\",\n",
            "          \"characterOffsetBegin\": 444,\n",
            "          \"characterOffsetEnd\": 452,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 42,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 452,\n",
            "          \"characterOffsetEnd\": 453,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 9,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"It\",\n",
            "          \"originalText\": \"It\",\n",
            "          \"characterOffsetBegin\": 455,\n",
            "          \"characterOffsetEnd\": 457,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 458,\n",
            "          \"characterOffsetEnd\": 461,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 462,\n",
            "          \"characterOffsetEnd\": 463,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"tragic\",\n",
            "          \"originalText\": \"tragic\",\n",
            "          \"characterOffsetBegin\": 464,\n",
            "          \"characterOffsetEnd\": 470,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"end\",\n",
            "          \"originalText\": \"end\",\n",
            "          \"characterOffsetBegin\": 471,\n",
            "          \"characterOffsetEnd\": 474,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 475,\n",
            "          \"characterOffsetEnd\": 477,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"two\",\n",
            "          \"originalText\": \"two\",\n",
            "          \"characterOffsetBegin\": 478,\n",
            "          \"characterOffsetEnd\": 481,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"turbulent\",\n",
            "          \"originalText\": \"turbulent\",\n",
            "          \"characterOffsetBegin\": 482,\n",
            "          \"characterOffsetEnd\": 491,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"lives\",\n",
            "          \"originalText\": \"lives\",\n",
            "          \"characterOffsetBegin\": 492,\n",
            "          \"characterOffsetEnd\": 497,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 497,\n",
            "          \"characterOffsetEnd\": 498,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"for\",\n",
            "          \"originalText\": \"for\",\n",
            "          \"characterOffsetBegin\": 499,\n",
            "          \"characterOffsetEnd\": 502,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 503,\n",
            "          \"characterOffsetEnd\": 506,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"sisters\",\n",
            "          \"originalText\": \"sisters\",\n",
            "          \"characterOffsetBegin\": 507,\n",
            "          \"characterOffsetEnd\": 514,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 515,\n",
            "          \"characterOffsetEnd\": 518,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"appeared\",\n",
            "          \"originalText\": \"appeared\",\n",
            "          \"characterOffsetBegin\": 519,\n",
            "          \"characterOffsetEnd\": 527,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"on\",\n",
            "          \"originalText\": \"on\",\n",
            "          \"characterOffsetBegin\": 528,\n",
            "          \"characterOffsetEnd\": 530,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"Australian\",\n",
            "          \"originalText\": \"Australian\",\n",
            "          \"characterOffsetBegin\": 531,\n",
            "          \"characterOffsetEnd\": 541,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"TV\",\n",
            "          \"originalText\": \"TV\",\n",
            "          \"characterOffsetBegin\": 542,\n",
            "          \"characterOffsetEnd\": 544,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"several\",\n",
            "          \"originalText\": \"several\",\n",
            "          \"characterOffsetBegin\": 545,\n",
            "          \"characterOffsetEnd\": 552,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \"times\",\n",
            "          \"originalText\": \"times\",\n",
            "          \"characterOffsetBegin\": 553,\n",
            "          \"characterOffsetEnd\": 558,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 21,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 559,\n",
            "          \"characterOffsetEnd\": 561,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 22,\n",
            "          \"word\": \"talk\",\n",
            "          \"originalText\": \"talk\",\n",
            "          \"characterOffsetBegin\": 562,\n",
            "          \"characterOffsetEnd\": 566,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 23,\n",
            "          \"word\": \"about\",\n",
            "          \"originalText\": \"about\",\n",
            "          \"characterOffsetBegin\": 567,\n",
            "          \"characterOffsetEnd\": 572,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 24,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 573,\n",
            "          \"characterOffsetEnd\": 576,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 25,\n",
            "          \"word\": \"anorexia\",\n",
            "          \"originalText\": \"anorexia\",\n",
            "          \"characterOffsetBegin\": 577,\n",
            "          \"characterOffsetEnd\": 585,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 26,\n",
            "          \"word\": \"which\",\n",
            "          \"originalText\": \"which\",\n",
            "          \"characterOffsetBegin\": 586,\n",
            "          \"characterOffsetEnd\": 591,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 27,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 592,\n",
            "          \"characterOffsetEnd\": 595,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 28,\n",
            "          \"word\": \"turned\",\n",
            "          \"originalText\": \"turned\",\n",
            "          \"characterOffsetBegin\": 596,\n",
            "          \"characterOffsetEnd\": 602,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 29,\n",
            "          \"word\": \"both\",\n",
            "          \"originalText\": \"both\",\n",
            "          \"characterOffsetBegin\": 603,\n",
            "          \"characterOffsetEnd\": 607,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 30,\n",
            "          \"word\": \"into\",\n",
            "          \"originalText\": \"into\",\n",
            "          \"characterOffsetBegin\": 608,\n",
            "          \"characterOffsetEnd\": 612,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 31,\n",
            "          \"word\": \"virtual\",\n",
            "          \"originalText\": \"virtual\",\n",
            "          \"characterOffsetBegin\": 613,\n",
            "          \"characterOffsetEnd\": 620,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 32,\n",
            "          \"word\": \"living\",\n",
            "          \"originalText\": \"living\",\n",
            "          \"characterOffsetBegin\": 621,\n",
            "          \"characterOffsetEnd\": 627,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 33,\n",
            "          \"word\": \"skeletons\",\n",
            "          \"originalText\": \"skeletons\",\n",
            "          \"characterOffsetBegin\": 628,\n",
            "          \"characterOffsetEnd\": 637,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 34,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 638,\n",
            "          \"characterOffsetEnd\": 641,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 35,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 642,\n",
            "          \"characterOffsetEnd\": 643,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 36,\n",
            "          \"word\": \"problem\",\n",
            "          \"originalText\": \"problem\",\n",
            "          \"characterOffsetBegin\": 644,\n",
            "          \"characterOffsetEnd\": 651,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 37,\n",
            "          \"word\": \"pair\",\n",
            "          \"originalText\": \"pair\",\n",
            "          \"characterOffsetBegin\": 652,\n",
            "          \"characterOffsetEnd\": 656,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 38,\n",
            "          \"word\": \"for\",\n",
            "          \"originalText\": \"for\",\n",
            "          \"characterOffsetBegin\": 657,\n",
            "          \"characterOffsetEnd\": 660,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 39,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 661,\n",
            "          \"characterOffsetEnd\": 666,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 40,\n",
            "          \"word\": \"parents\",\n",
            "          \"originalText\": \"parents\",\n",
            "          \"characterOffsetBegin\": 667,\n",
            "          \"characterOffsetEnd\": 674,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 41,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 674,\n",
            "          \"characterOffsetEnd\": 675,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 42,\n",
            "          \"word\": \"social\",\n",
            "          \"originalText\": \"social\",\n",
            "          \"characterOffsetBegin\": 676,\n",
            "          \"characterOffsetEnd\": 682,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 43,\n",
            "          \"word\": \"workers\",\n",
            "          \"originalText\": \"workers\",\n",
            "          \"characterOffsetBegin\": 683,\n",
            "          \"characterOffsetEnd\": 690,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 44,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 691,\n",
            "          \"characterOffsetEnd\": 694,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 45,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 695,\n",
            "          \"characterOffsetEnd\": 698,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 46,\n",
            "          \"word\": \"police\",\n",
            "          \"originalText\": \"police\",\n",
            "          \"characterOffsetBegin\": 699,\n",
            "          \"characterOffsetEnd\": 705,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 47,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 705,\n",
            "          \"characterOffsetEnd\": 706,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 10,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Scroll\",\n",
            "          \"originalText\": \"Scroll\",\n",
            "          \"characterOffsetBegin\": 708,\n",
            "          \"characterOffsetEnd\": 714,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"down\",\n",
            "          \"originalText\": \"down\",\n",
            "          \"characterOffsetBegin\": 715,\n",
            "          \"characterOffsetEnd\": 719,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"for\",\n",
            "          \"originalText\": \"for\",\n",
            "          \"characterOffsetBegin\": 720,\n",
            "          \"characterOffsetEnd\": 723,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"video\",\n",
            "          \"originalText\": \"video\",\n",
            "          \"characterOffsetBegin\": 724,\n",
            "          \"characterOffsetEnd\": 729,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 11,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Tragic\",\n",
            "          \"originalText\": \"Tragic\",\n",
            "          \"characterOffsetBegin\": 731,\n",
            "          \"characterOffsetEnd\": 737,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 737,\n",
            "          \"characterOffsetEnd\": 738,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Identical\",\n",
            "          \"originalText\": \"Identical\",\n",
            "          \"characterOffsetBegin\": 739,\n",
            "          \"characterOffsetEnd\": 748,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"twins\",\n",
            "          \"originalText\": \"twins\",\n",
            "          \"characterOffsetBegin\": 749,\n",
            "          \"characterOffsetEnd\": 754,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 755,\n",
            "          \"characterOffsetEnd\": 760,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 761,\n",
            "          \"characterOffsetEnd\": 764,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 765,\n",
            "          \"characterOffsetEnd\": 771,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"Wallmeyer\",\n",
            "          \"originalText\": \"Wallmeyer\",\n",
            "          \"characterOffsetBegin\": 772,\n",
            "          \"characterOffsetEnd\": 781,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 781,\n",
            "          \"characterOffsetEnd\": 782,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"42\",\n",
            "          \"originalText\": \"42\",\n",
            "          \"characterOffsetBegin\": 783,\n",
            "          \"characterOffsetEnd\": 785,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 785,\n",
            "          \"characterOffsetEnd\": 786,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"died\",\n",
            "          \"originalText\": \"died\",\n",
            "          \"characterOffsetBegin\": 787,\n",
            "          \"characterOffsetEnd\": 791,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"when\",\n",
            "          \"originalText\": \"when\",\n",
            "          \"characterOffsetBegin\": 792,\n",
            "          \"characterOffsetEnd\": 796,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"fire\",\n",
            "          \"originalText\": \"fire\",\n",
            "          \"characterOffsetBegin\": 797,\n",
            "          \"characterOffsetEnd\": 801,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"swept\",\n",
            "          \"originalText\": \"swept\",\n",
            "          \"characterOffsetBegin\": 802,\n",
            "          \"characterOffsetEnd\": 807,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"through\",\n",
            "          \"originalText\": \"through\",\n",
            "          \"characterOffsetBegin\": 808,\n",
            "          \"characterOffsetEnd\": 815,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 816,\n",
            "          \"characterOffsetEnd\": 821,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"home\",\n",
            "          \"originalText\": \"home\",\n",
            "          \"characterOffsetBegin\": 822,\n",
            "          \"characterOffsetEnd\": 826,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 827,\n",
            "          \"characterOffsetEnd\": 829,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \"Geelong\",\n",
            "          \"originalText\": \"Geelong\",\n",
            "          \"characterOffsetBegin\": 830,\n",
            "          \"characterOffsetEnd\": 837,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 21,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 837,\n",
            "          \"characterOffsetEnd\": 838,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 22,\n",
            "          \"word\": \"near\",\n",
            "          \"originalText\": \"near\",\n",
            "          \"characterOffsetBegin\": 839,\n",
            "          \"characterOffsetEnd\": 843,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 23,\n",
            "          \"word\": \"Melbourne\",\n",
            "          \"originalText\": \"Melbourne\",\n",
            "          \"characterOffsetBegin\": 844,\n",
            "          \"characterOffsetEnd\": 853,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 12,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"'\",\n",
            "          \"characterOffsetBegin\": 855,\n",
            "          \"characterOffsetEnd\": 856,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"At\",\n",
            "          \"originalText\": \"At\",\n",
            "          \"characterOffsetBegin\": 856,\n",
            "          \"characterOffsetEnd\": 858,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"least\",\n",
            "          \"originalText\": \"least\",\n",
            "          \"characterOffsetBegin\": 859,\n",
            "          \"characterOffsetEnd\": 864,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"we\",\n",
            "          \"originalText\": \"we\",\n",
            "          \"characterOffsetBegin\": 865,\n",
            "          \"characterOffsetEnd\": 867,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"'ll\",\n",
            "          \"originalText\": \"'ll\",\n",
            "          \"characterOffsetBegin\": 867,\n",
            "          \"characterOffsetEnd\": 870,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"die\",\n",
            "          \"originalText\": \"die\",\n",
            "          \"characterOffsetBegin\": 871,\n",
            "          \"characterOffsetEnd\": 874,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"together\",\n",
            "          \"originalText\": \"together\",\n",
            "          \"characterOffsetBegin\": 875,\n",
            "          \"characterOffsetEnd\": 883,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"'\",\n",
            "          \"characterOffsetBegin\": 883,\n",
            "          \"characterOffsetEnd\": 884,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 884,\n",
            "          \"characterOffsetEnd\": 885,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"Twins\",\n",
            "          \"originalText\": \"Twins\",\n",
            "          \"characterOffsetBegin\": 886,\n",
            "          \"characterOffsetEnd\": 891,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"predicted\",\n",
            "          \"originalText\": \"predicted\",\n",
            "          \"characterOffsetBegin\": 892,\n",
            "          \"characterOffsetEnd\": 901,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 902,\n",
            "          \"characterOffsetEnd\": 907,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"sad\",\n",
            "          \"originalText\": \"sad\",\n",
            "          \"characterOffsetBegin\": 908,\n",
            "          \"characterOffsetEnd\": 911,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"demise\",\n",
            "          \"originalText\": \"demise\",\n",
            "          \"characterOffsetBegin\": 912,\n",
            "          \"characterOffsetEnd\": 918,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 13,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"In\",\n",
            "          \"originalText\": \"In\",\n",
            "          \"characterOffsetBegin\": 920,\n",
            "          \"characterOffsetEnd\": 922,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 923,\n",
            "          \"characterOffsetEnd\": 924,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"poignant\",\n",
            "          \"originalText\": \"poignant\",\n",
            "          \"characterOffsetBegin\": 925,\n",
            "          \"characterOffsetEnd\": 933,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"review\",\n",
            "          \"originalText\": \"review\",\n",
            "          \"characterOffsetBegin\": 934,\n",
            "          \"characterOffsetEnd\": 940,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 941,\n",
            "          \"characterOffsetEnd\": 943,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 944,\n",
            "          \"characterOffsetEnd\": 949,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"lives\",\n",
            "          \"originalText\": \"lives\",\n",
            "          \"characterOffsetBegin\": 950,\n",
            "          \"characterOffsetEnd\": 955,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 956,\n",
            "          \"characterOffsetEnd\": 960,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"said\",\n",
            "          \"originalText\": \"said\",\n",
            "          \"characterOffsetBegin\": 961,\n",
            "          \"characterOffsetEnd\": 965,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 966,\n",
            "          \"characterOffsetEnd\": 968,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"recent\",\n",
            "          \"originalText\": \"recent\",\n",
            "          \"characterOffsetBegin\": 969,\n",
            "          \"characterOffsetEnd\": 975,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"years\",\n",
            "          \"originalText\": \"years\",\n",
            "          \"characterOffsetBegin\": 976,\n",
            "          \"characterOffsetEnd\": 981,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"that\",\n",
            "          \"originalText\": \"that\",\n",
            "          \"characterOffsetBegin\": 982,\n",
            "          \"characterOffsetEnd\": 986,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 987,\n",
            "          \"characterOffsetEnd\": 991,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 992,\n",
            "          \"characterOffsetEnd\": 995,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"never\",\n",
            "          \"originalText\": \"never\",\n",
            "          \"characterOffsetBegin\": 996,\n",
            "          \"characterOffsetEnd\": 1001,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"been\",\n",
            "          \"originalText\": \"been\",\n",
            "          \"characterOffsetBegin\": 1002,\n",
            "          \"characterOffsetEnd\": 1006,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 1007,\n",
            "          \"characterOffsetEnd\": 1009,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"love\",\n",
            "          \"originalText\": \"love\",\n",
            "          \"characterOffsetBegin\": 1010,\n",
            "          \"characterOffsetEnd\": 1014,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 1014,\n",
            "          \"characterOffsetEnd\": 1015,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 21,\n",
            "          \"word\": \"never\",\n",
            "          \"originalText\": \"never\",\n",
            "          \"characterOffsetBegin\": 1016,\n",
            "          \"characterOffsetEnd\": 1021,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 22,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 1022,\n",
            "          \"characterOffsetEnd\": 1025,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 23,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 1026,\n",
            "          \"characterOffsetEnd\": 1027,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 24,\n",
            "          \"word\": \"job\",\n",
            "          \"originalText\": \"job\",\n",
            "          \"characterOffsetBegin\": 1028,\n",
            "          \"characterOffsetEnd\": 1031,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 25,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 1032,\n",
            "          \"characterOffsetEnd\": 1035,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 26,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 1036,\n",
            "          \"characterOffsetEnd\": 1040,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 27,\n",
            "          \"word\": \"believed\",\n",
            "          \"originalText\": \"believed\",\n",
            "          \"characterOffsetBegin\": 1041,\n",
            "          \"characterOffsetEnd\": 1049,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 28,\n",
            "          \"word\": \"that\",\n",
            "          \"originalText\": \"that\",\n",
            "          \"characterOffsetBegin\": 1050,\n",
            "          \"characterOffsetEnd\": 1054,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 29,\n",
            "          \"word\": \"it\",\n",
            "          \"originalText\": \"it\",\n",
            "          \"characterOffsetBegin\": 1055,\n",
            "          \"characterOffsetEnd\": 1057,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 30,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 1058,\n",
            "          \"characterOffsetEnd\": 1061,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 31,\n",
            "          \"word\": \"only\",\n",
            "          \"originalText\": \"only\",\n",
            "          \"characterOffsetBegin\": 1062,\n",
            "          \"characterOffsetEnd\": 1066,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 32,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 1067,\n",
            "          \"characterOffsetEnd\": 1068,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 33,\n",
            "          \"word\": \"matter\",\n",
            "          \"originalText\": \"matter\",\n",
            "          \"characterOffsetBegin\": 1069,\n",
            "          \"characterOffsetEnd\": 1075,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 34,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 1076,\n",
            "          \"characterOffsetEnd\": 1078,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 35,\n",
            "          \"word\": \"time\",\n",
            "          \"originalText\": \"time\",\n",
            "          \"characterOffsetBegin\": 1079,\n",
            "          \"characterOffsetEnd\": 1083,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 36,\n",
            "          \"word\": \"before\",\n",
            "          \"originalText\": \"before\",\n",
            "          \"characterOffsetBegin\": 1084,\n",
            "          \"characterOffsetEnd\": 1090,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 37,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 1091,\n",
            "          \"characterOffsetEnd\": 1095,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 38,\n",
            "          \"word\": \"died\",\n",
            "          \"originalText\": \"died\",\n",
            "          \"characterOffsetBegin\": 1096,\n",
            "          \"characterOffsetEnd\": 1100,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 39,\n",
            "          \"word\": \"--\",\n",
            "          \"originalText\": \"–\",\n",
            "          \"characterOffsetBegin\": 1101,\n",
            "          \"characterOffsetEnd\": 1102,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 40,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 1103,\n",
            "          \"characterOffsetEnd\": 1106,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 41,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 1107,\n",
            "          \"characterOffsetEnd\": 1111,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 42,\n",
            "          \"word\": \"would\",\n",
            "          \"originalText\": \"would\",\n",
            "          \"characterOffsetBegin\": 1112,\n",
            "          \"characterOffsetEnd\": 1117,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 43,\n",
            "          \"word\": \"die\",\n",
            "          \"originalText\": \"die\",\n",
            "          \"characterOffsetBegin\": 1118,\n",
            "          \"characterOffsetEnd\": 1121,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 44,\n",
            "          \"word\": \"together\",\n",
            "          \"originalText\": \"together\",\n",
            "          \"characterOffsetBegin\": 1122,\n",
            "          \"characterOffsetEnd\": 1130,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 45,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 1130,\n",
            "          \"characterOffsetEnd\": 1131,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 14,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Their\",\n",
            "          \"originalText\": \"Their\",\n",
            "          \"characterOffsetBegin\": 1133,\n",
            "          \"characterOffsetEnd\": 1138,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"deaths\",\n",
            "          \"originalText\": \"deaths\",\n",
            "          \"characterOffsetBegin\": 1139,\n",
            "          \"characterOffsetEnd\": 1145,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 1146,\n",
            "          \"characterOffsetEnd\": 1148,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 1149,\n",
            "          \"characterOffsetEnd\": 1152,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"fire\",\n",
            "          \"originalText\": \"fire\",\n",
            "          \"characterOffsetBegin\": 1153,\n",
            "          \"characterOffsetEnd\": 1157,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"are\",\n",
            "          \"originalText\": \"are\",\n",
            "          \"characterOffsetBegin\": 1158,\n",
            "          \"characterOffsetEnd\": 1161,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"believed\",\n",
            "          \"originalText\": \"believed\",\n",
            "          \"characterOffsetBegin\": 1162,\n",
            "          \"characterOffsetEnd\": 1170,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 1171,\n",
            "          \"characterOffsetEnd\": 1173,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"have\",\n",
            "          \"originalText\": \"have\",\n",
            "          \"characterOffsetBegin\": 1174,\n",
            "          \"characterOffsetEnd\": 1178,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"been\",\n",
            "          \"originalText\": \"been\",\n",
            "          \"characterOffsetBegin\": 1179,\n",
            "          \"characterOffsetEnd\": 1183,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"accidental\",\n",
            "          \"originalText\": \"accidental\",\n",
            "          \"characterOffsetBegin\": 1184,\n",
            "          \"characterOffsetEnd\": 1194,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 1194,\n",
            "          \"characterOffsetEnd\": 1195,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"according\",\n",
            "          \"originalText\": \"according\",\n",
            "          \"characterOffsetBegin\": 1196,\n",
            "          \"characterOffsetEnd\": 1205,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 1206,\n",
            "          \"characterOffsetEnd\": 1208,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"detectives\",\n",
            "          \"originalText\": \"detectives\",\n",
            "          \"characterOffsetBegin\": 1209,\n",
            "          \"characterOffsetEnd\": 1219,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"from\",\n",
            "          \"originalText\": \"from\",\n",
            "          \"characterOffsetBegin\": 1220,\n",
            "          \"characterOffsetEnd\": 1224,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 1225,\n",
            "          \"characterOffsetEnd\": 1228,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"Geelong\",\n",
            "          \"originalText\": \"Geelong\",\n",
            "          \"characterOffsetBegin\": 1229,\n",
            "          \"characterOffsetEnd\": 1236,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"Crime\",\n",
            "          \"originalText\": \"Crime\",\n",
            "          \"characterOffsetBegin\": 1237,\n",
            "          \"characterOffsetEnd\": 1242,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \"Investigation\",\n",
            "          \"originalText\": \"Investigation\",\n",
            "          \"characterOffsetBegin\": 1243,\n",
            "          \"characterOffsetEnd\": 1256,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 21,\n",
            "          \"word\": \"Unit\",\n",
            "          \"originalText\": \"Unit\",\n",
            "          \"characterOffsetBegin\": 1257,\n",
            "          \"characterOffsetEnd\": 1261,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 22,\n",
            "          \"word\": \"who\",\n",
            "          \"originalText\": \"who\",\n",
            "          \"characterOffsetBegin\": 1262,\n",
            "          \"characterOffsetEnd\": 1265,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 23,\n",
            "          \"word\": \"said\",\n",
            "          \"originalText\": \"said\",\n",
            "          \"characterOffsetBegin\": 1266,\n",
            "          \"characterOffsetEnd\": 1270,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 24,\n",
            "          \"word\": \"that\",\n",
            "          \"originalText\": \"that\",\n",
            "          \"characterOffsetBegin\": 1271,\n",
            "          \"characterOffsetEnd\": 1275,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 25,\n",
            "          \"word\": \"initial\",\n",
            "          \"originalText\": \"initial\",\n",
            "          \"characterOffsetBegin\": 1276,\n",
            "          \"characterOffsetEnd\": 1283,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 26,\n",
            "          \"word\": \"checks\",\n",
            "          \"originalText\": \"checks\",\n",
            "          \"characterOffsetBegin\": 1284,\n",
            "          \"characterOffsetEnd\": 1290,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 27,\n",
            "          \"word\": \"did\",\n",
            "          \"originalText\": \"did\",\n",
            "          \"characterOffsetBegin\": 1291,\n",
            "          \"characterOffsetEnd\": 1294,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 28,\n",
            "          \"word\": \"not\",\n",
            "          \"originalText\": \"not\",\n",
            "          \"characterOffsetBegin\": 1295,\n",
            "          \"characterOffsetEnd\": 1298,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 29,\n",
            "          \"word\": \"reveal\",\n",
            "          \"originalText\": \"reveal\",\n",
            "          \"characterOffsetBegin\": 1299,\n",
            "          \"characterOffsetEnd\": 1305,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 30,\n",
            "          \"word\": \"any\",\n",
            "          \"originalText\": \"any\",\n",
            "          \"characterOffsetBegin\": 1306,\n",
            "          \"characterOffsetEnd\": 1309,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 31,\n",
            "          \"word\": \"suspicious\",\n",
            "          \"originalText\": \"suspicious\",\n",
            "          \"characterOffsetBegin\": 1310,\n",
            "          \"characterOffsetEnd\": 1320,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 32,\n",
            "          \"word\": \"activity\",\n",
            "          \"originalText\": \"activity\",\n",
            "          \"characterOffsetBegin\": 1321,\n",
            "          \"characterOffsetEnd\": 1329,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 33,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 1329,\n",
            "          \"characterOffsetEnd\": 1330,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 15,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Yet\",\n",
            "          \"originalText\": \"Yet\",\n",
            "          \"characterOffsetBegin\": 1332,\n",
            "          \"characterOffsetEnd\": 1335,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"there\",\n",
            "          \"originalText\": \"there\",\n",
            "          \"characterOffsetBegin\": 1336,\n",
            "          \"characterOffsetEnd\": 1341,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 1342,\n",
            "          \"characterOffsetEnd\": 1345,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"been\",\n",
            "          \"originalText\": \"been\",\n",
            "          \"characterOffsetBegin\": 1346,\n",
            "          \"characterOffsetEnd\": 1350,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"reports\",\n",
            "          \"originalText\": \"reports\",\n",
            "          \"characterOffsetBegin\": 1351,\n",
            "          \"characterOffsetEnd\": 1358,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"over\",\n",
            "          \"originalText\": \"over\",\n",
            "          \"characterOffsetBegin\": 1359,\n",
            "          \"characterOffsetEnd\": 1363,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 1364,\n",
            "          \"characterOffsetEnd\": 1367,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"years\",\n",
            "          \"originalText\": \"years\",\n",
            "          \"characterOffsetBegin\": 1368,\n",
            "          \"characterOffsetEnd\": 1373,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 1374,\n",
            "          \"characterOffsetEnd\": 1376,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 1377,\n",
            "          \"characterOffsetEnd\": 1380,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"women\",\n",
            "          \"originalText\": \"women\",\n",
            "          \"characterOffsetBegin\": 1381,\n",
            "          \"characterOffsetEnd\": 1386,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"each\",\n",
            "          \"originalText\": \"each\",\n",
            "          \"characterOffsetBegin\": 1387,\n",
            "          \"characterOffsetEnd\": 1391,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"trying\",\n",
            "          \"originalText\": \"trying\",\n",
            "          \"characterOffsetBegin\": 1392,\n",
            "          \"characterOffsetEnd\": 1398,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 1399,\n",
            "          \"characterOffsetEnd\": 1401,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"kill\",\n",
            "          \"originalText\": \"kill\",\n",
            "          \"characterOffsetBegin\": 1402,\n",
            "          \"characterOffsetEnd\": 1406,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"one\",\n",
            "          \"originalText\": \"one\",\n",
            "          \"characterOffsetBegin\": 1407,\n",
            "          \"characterOffsetEnd\": 1410,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"another\",\n",
            "          \"originalText\": \"another\",\n",
            "          \"characterOffsetBegin\": 1411,\n",
            "          \"characterOffsetEnd\": 1418,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 1418,\n",
            "          \"characterOffsetEnd\": 1419,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 16,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 1421,\n",
            "          \"characterOffsetEnd\": 1427,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 1428,\n",
            "          \"characterOffsetEnd\": 1431,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"charged\",\n",
            "          \"originalText\": \"charged\",\n",
            "          \"characterOffsetBegin\": 1432,\n",
            "          \"characterOffsetEnd\": 1439,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"characterOffsetBegin\": 1440,\n",
            "          \"characterOffsetEnd\": 1444,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 1445,\n",
            "          \"characterOffsetEnd\": 1448,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"attempted\",\n",
            "          \"originalText\": \"attempted\",\n",
            "          \"characterOffsetBegin\": 1449,\n",
            "          \"characterOffsetEnd\": 1458,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 17,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"murder\",\n",
            "          \"originalText\": \"murder\",\n",
            "          \"characterOffsetBegin\": 1460,\n",
            "          \"characterOffsetEnd\": 1466,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 1467,\n",
            "          \"characterOffsetEnd\": 1469,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 1470,\n",
            "          \"characterOffsetEnd\": 1475,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"after\",\n",
            "          \"originalText\": \"after\",\n",
            "          \"characterOffsetBegin\": 1476,\n",
            "          \"characterOffsetEnd\": 1481,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"police\",\n",
            "          \"originalText\": \"police\",\n",
            "          \"characterOffsetBegin\": 1482,\n",
            "          \"characterOffsetEnd\": 1488,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 1488,\n",
            "          \"characterOffsetEnd\": 1489,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"who\",\n",
            "          \"originalText\": \"who\",\n",
            "          \"characterOffsetBegin\": 1490,\n",
            "          \"characterOffsetEnd\": 1493,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"were\",\n",
            "          \"originalText\": \"were\",\n",
            "          \"characterOffsetBegin\": 1494,\n",
            "          \"characterOffsetEnd\": 1498,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"called\",\n",
            "          \"originalText\": \"called\",\n",
            "          \"characterOffsetBegin\": 1499,\n",
            "          \"characterOffsetEnd\": 1505,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 1506,\n",
            "          \"characterOffsetEnd\": 1508,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 1509,\n",
            "          \"characterOffsetEnd\": 1514,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"home\",\n",
            "          \"originalText\": \"home\",\n",
            "          \"characterOffsetBegin\": 1515,\n",
            "          \"characterOffsetEnd\": 1519,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 1519,\n",
            "          \"characterOffsetEnd\": 1520,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"claimed\",\n",
            "          \"originalText\": \"claimed\",\n",
            "          \"characterOffsetBegin\": 1521,\n",
            "          \"characterOffsetEnd\": 1528,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 18,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 1530,\n",
            "          \"characterOffsetEnd\": 1534,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"witnessed\",\n",
            "          \"originalText\": \"witnessed\",\n",
            "          \"characterOffsetBegin\": 1535,\n",
            "          \"characterOffsetEnd\": 1544,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 1545,\n",
            "          \"characterOffsetEnd\": 1551,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"characterOffsetBegin\": 1552,\n",
            "          \"characterOffsetEnd\": 1556,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"her\",\n",
            "          \"originalText\": \"her\",\n",
            "          \"characterOffsetBegin\": 1557,\n",
            "          \"characterOffsetEnd\": 1560,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"hands\",\n",
            "          \"originalText\": \"hands\",\n",
            "          \"characterOffsetBegin\": 1561,\n",
            "          \"characterOffsetEnd\": 1566,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"locked\",\n",
            "          \"originalText\": \"locked\",\n",
            "          \"characterOffsetBegin\": 1567,\n",
            "          \"characterOffsetEnd\": 1573,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"around\",\n",
            "          \"originalText\": \"around\",\n",
            "          \"characterOffsetBegin\": 1574,\n",
            "          \"characterOffsetEnd\": 1580,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"her\",\n",
            "          \"originalText\": \"her\",\n",
            "          \"characterOffsetBegin\": 1581,\n",
            "          \"characterOffsetEnd\": 1584,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"sister\",\n",
            "          \"originalText\": \"sister\",\n",
            "          \"characterOffsetBegin\": 1585,\n",
            "          \"characterOffsetEnd\": 1591,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"'s\",\n",
            "          \"originalText\": \"’s\",\n",
            "          \"characterOffsetBegin\": 1591,\n",
            "          \"characterOffsetEnd\": 1593,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"throat\",\n",
            "          \"originalText\": \"throat\",\n",
            "          \"characterOffsetBegin\": 1594,\n",
            "          \"characterOffsetEnd\": 1600,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 1600,\n",
            "          \"characterOffsetEnd\": 1601,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 19,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"The\",\n",
            "          \"originalText\": \"The\",\n",
            "          \"characterOffsetBegin\": 1603,\n",
            "          \"characterOffsetEnd\": 1606,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"charge\",\n",
            "          \"originalText\": \"charge\",\n",
            "          \"characterOffsetBegin\": 1607,\n",
            "          \"characterOffsetEnd\": 1613,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 1614,\n",
            "          \"characterOffsetEnd\": 1617,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"later\",\n",
            "          \"originalText\": \"later\",\n",
            "          \"characterOffsetBegin\": 1618,\n",
            "          \"characterOffsetEnd\": 1623,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"withdrawn\",\n",
            "          \"originalText\": \"withdrawn\",\n",
            "          \"characterOffsetBegin\": 1624,\n",
            "          \"characterOffsetEnd\": 1633,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 1633,\n",
            "          \"characterOffsetEnd\": 1634,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 20,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Their\",\n",
            "          \"originalText\": \"Their\",\n",
            "          \"characterOffsetBegin\": 1636,\n",
            "          \"characterOffsetEnd\": 1641,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"existence\",\n",
            "          \"originalText\": \"existence\",\n",
            "          \"characterOffsetBegin\": 1642,\n",
            "          \"characterOffsetEnd\": 1651,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 1651,\n",
            "          \"characterOffsetEnd\": 1652,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"balanced\",\n",
            "          \"originalText\": \"balanced\",\n",
            "          \"characterOffsetBegin\": 1653,\n",
            "          \"characterOffsetEnd\": 1661,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"between\",\n",
            "          \"originalText\": \"between\",\n",
            "          \"characterOffsetBegin\": 1662,\n",
            "          \"characterOffsetEnd\": 1669,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 21,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"life\",\n",
            "          \"originalText\": \"life\",\n",
            "          \"characterOffsetBegin\": 1671,\n",
            "          \"characterOffsetEnd\": 1675,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 1676,\n",
            "          \"characterOffsetEnd\": 1679,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"death\",\n",
            "          \"originalText\": \"death\",\n",
            "          \"characterOffsetBegin\": 1680,\n",
            "          \"characterOffsetEnd\": 1685,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 1685,\n",
            "          \"characterOffsetEnd\": 1686,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 1687,\n",
            "          \"characterOffsetEnd\": 1690,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"resulted\",\n",
            "          \"originalText\": \"resulted\",\n",
            "          \"characterOffsetBegin\": 1691,\n",
            "          \"characterOffsetEnd\": 1699,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 1700,\n",
            "          \"characterOffsetEnd\": 1702,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"TV\",\n",
            "          \"originalText\": \"TV\",\n",
            "          \"characterOffsetBegin\": 1703,\n",
            "          \"characterOffsetEnd\": 1705,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"companies\",\n",
            "          \"originalText\": \"companies\",\n",
            "          \"characterOffsetBegin\": 1706,\n",
            "          \"characterOffsetEnd\": 1715,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"searching\",\n",
            "          \"originalText\": \"searching\",\n",
            "          \"characterOffsetBegin\": 1716,\n",
            "          \"characterOffsetEnd\": 1725,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"them\",\n",
            "          \"originalText\": \"them\",\n",
            "          \"characterOffsetBegin\": 1726,\n",
            "          \"characterOffsetEnd\": 1730,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"out\",\n",
            "          \"originalText\": \"out\",\n",
            "          \"characterOffsetBegin\": 1731,\n",
            "          \"characterOffsetEnd\": 1734,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"for\",\n",
            "          \"originalText\": \"for\",\n",
            "          \"characterOffsetBegin\": 1735,\n",
            "          \"characterOffsetEnd\": 1738,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 22,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"interviews\",\n",
            "          \"originalText\": \"interviews\",\n",
            "          \"characterOffsetBegin\": 1740,\n",
            "          \"characterOffsetEnd\": 1750,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"after\",\n",
            "          \"originalText\": \"after\",\n",
            "          \"characterOffsetBegin\": 1751,\n",
            "          \"characterOffsetEnd\": 1756,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"authorities\",\n",
            "          \"originalText\": \"authorities\",\n",
            "          \"characterOffsetBegin\": 1757,\n",
            "          \"characterOffsetEnd\": 1768,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"considered\",\n",
            "          \"originalText\": \"considered\",\n",
            "          \"characterOffsetBegin\": 1769,\n",
            "          \"characterOffsetEnd\": 1779,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"jailing\",\n",
            "          \"originalText\": \"jailing\",\n",
            "          \"characterOffsetBegin\": 1780,\n",
            "          \"characterOffsetEnd\": 1787,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"them\",\n",
            "          \"originalText\": \"them\",\n",
            "          \"characterOffsetBegin\": 1788,\n",
            "          \"characterOffsetEnd\": 1792,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 1793,\n",
            "          \"characterOffsetEnd\": 1795,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"an\",\n",
            "          \"originalText\": \"an\",\n",
            "          \"characterOffsetBegin\": 1796,\n",
            "          \"characterOffsetEnd\": 1798,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"attempt\",\n",
            "          \"originalText\": \"attempt\",\n",
            "          \"characterOffsetBegin\": 1799,\n",
            "          \"characterOffsetEnd\": 1806,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 1807,\n",
            "          \"characterOffsetEnd\": 1809,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 23,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"stop\",\n",
            "          \"originalText\": \"stop\",\n",
            "          \"characterOffsetBegin\": 1811,\n",
            "          \"characterOffsetEnd\": 1815,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 1816,\n",
            "          \"characterOffsetEnd\": 1819,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"women\",\n",
            "          \"originalText\": \"women\",\n",
            "          \"characterOffsetBegin\": 1820,\n",
            "          \"characterOffsetEnd\": 1825,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"starving\",\n",
            "          \"originalText\": \"starving\",\n",
            "          \"characterOffsetBegin\": 1826,\n",
            "          \"characterOffsetEnd\": 1834,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"themselves\",\n",
            "          \"originalText\": \"themselves\",\n",
            "          \"characterOffsetBegin\": 1835,\n",
            "          \"characterOffsetEnd\": 1845,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 1846,\n",
            "          \"characterOffsetEnd\": 1848,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"death\",\n",
            "          \"originalText\": \"death\",\n",
            "          \"characterOffsetBegin\": 1849,\n",
            "          \"characterOffsetEnd\": 1854,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 1855,\n",
            "          \"characterOffsetEnd\": 1858,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"‘\",\n",
            "          \"characterOffsetBegin\": 1859,\n",
            "          \"characterOffsetEnd\": 1860,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"turn\",\n",
            "          \"originalText\": \"turn\",\n",
            "          \"characterOffsetBegin\": 1860,\n",
            "          \"characterOffsetEnd\": 1864,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 1865,\n",
            "          \"characterOffsetEnd\": 1870,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"lives\",\n",
            "          \"originalText\": \"lives\",\n",
            "          \"characterOffsetBegin\": 1871,\n",
            "          \"characterOffsetEnd\": 1876,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 24,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"around\",\n",
            "          \"originalText\": \"around\",\n",
            "          \"characterOffsetBegin\": 1878,\n",
            "          \"characterOffsetEnd\": 1884,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 1884,\n",
            "          \"characterOffsetEnd\": 1885,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"’\",\n",
            "          \"characterOffsetBegin\": 1885,\n",
            "          \"characterOffsetEnd\": 1886,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 25,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"In\",\n",
            "          \"originalText\": \"In\",\n",
            "          \"characterOffsetBegin\": 1888,\n",
            "          \"characterOffsetEnd\": 1890,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"fact\",\n",
            "          \"originalText\": \"fact\",\n",
            "          \"characterOffsetBegin\": 1891,\n",
            "          \"characterOffsetEnd\": 1895,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 1896,\n",
            "          \"characterOffsetEnd\": 1901,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 26,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 1903,\n",
            "          \"characterOffsetEnd\": 1906,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"later\",\n",
            "          \"originalText\": \"later\",\n",
            "          \"characterOffsetBegin\": 1907,\n",
            "          \"characterOffsetEnd\": 1912,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"jailed\",\n",
            "          \"originalText\": \"jailed\",\n",
            "          \"characterOffsetBegin\": 1913,\n",
            "          \"characterOffsetEnd\": 1919,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"by\",\n",
            "          \"originalText\": \"by\",\n",
            "          \"characterOffsetBegin\": 1920,\n",
            "          \"characterOffsetEnd\": 1922,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 1923,\n",
            "          \"characterOffsetEnd\": 1924,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"Geelong\",\n",
            "          \"originalText\": \"Geelong\",\n",
            "          \"characterOffsetBegin\": 1925,\n",
            "          \"characterOffsetEnd\": 1932,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"court\",\n",
            "          \"originalText\": \"court\",\n",
            "          \"characterOffsetBegin\": 1933,\n",
            "          \"characterOffsetEnd\": 1938,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"for\",\n",
            "          \"originalText\": \"for\",\n",
            "          \"characterOffsetBegin\": 1939,\n",
            "          \"characterOffsetEnd\": 1942,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 1943,\n",
            "          \"characterOffsetEnd\": 1944,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"series\",\n",
            "          \"originalText\": \"series\",\n",
            "          \"characterOffsetBegin\": 1945,\n",
            "          \"characterOffsetEnd\": 1951,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 1952,\n",
            "          \"characterOffsetEnd\": 1954,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"thefts\",\n",
            "          \"originalText\": \"thefts\",\n",
            "          \"characterOffsetBegin\": 1955,\n",
            "          \"characterOffsetEnd\": 1961,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"--\",\n",
            "          \"originalText\": \"–\",\n",
            "          \"characterOffsetBegin\": 1962,\n",
            "          \"characterOffsetEnd\": 1963,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"but\",\n",
            "          \"originalText\": \"but\",\n",
            "          \"characterOffsetBegin\": 1964,\n",
            "          \"characterOffsetEnd\": 1967,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"only\",\n",
            "          \"originalText\": \"only\",\n",
            "          \"characterOffsetBegin\": 1968,\n",
            "          \"characterOffsetEnd\": 1972,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 27,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"after\",\n",
            "          \"originalText\": \"after\",\n",
            "          \"characterOffsetBegin\": 1974,\n",
            "          \"characterOffsetEnd\": 1979,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"magistrate\",\n",
            "          \"originalText\": \"magistrate\",\n",
            "          \"characterOffsetBegin\": 1980,\n",
            "          \"characterOffsetEnd\": 1990,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Ian\",\n",
            "          \"originalText\": \"Ian\",\n",
            "          \"characterOffsetBegin\": 1991,\n",
            "          \"characterOffsetEnd\": 1994,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"von\",\n",
            "          \"originalText\": \"von\",\n",
            "          \"characterOffsetBegin\": 1995,\n",
            "          \"characterOffsetEnd\": 1998,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Einem\",\n",
            "          \"originalText\": \"Einem\",\n",
            "          \"characterOffsetBegin\": 1999,\n",
            "          \"characterOffsetEnd\": 2004,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"said\",\n",
            "          \"originalText\": \"said\",\n",
            "          \"characterOffsetBegin\": 2005,\n",
            "          \"characterOffsetEnd\": 2009,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"he\",\n",
            "          \"originalText\": \"he\",\n",
            "          \"characterOffsetBegin\": 2010,\n",
            "          \"characterOffsetEnd\": 2012,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"saw\",\n",
            "          \"originalText\": \"saw\",\n",
            "          \"characterOffsetBegin\": 2013,\n",
            "          \"characterOffsetEnd\": 2016,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"no\",\n",
            "          \"originalText\": \"no\",\n",
            "          \"characterOffsetBegin\": 2017,\n",
            "          \"characterOffsetEnd\": 2019,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"option\",\n",
            "          \"originalText\": \"option\",\n",
            "          \"characterOffsetBegin\": 2020,\n",
            "          \"characterOffsetEnd\": 2026,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"but\",\n",
            "          \"originalText\": \"but\",\n",
            "          \"characterOffsetBegin\": 2027,\n",
            "          \"characterOffsetEnd\": 2030,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 2031,\n",
            "          \"characterOffsetEnd\": 2033,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"send\",\n",
            "          \"originalText\": \"send\",\n",
            "          \"characterOffsetBegin\": 2034,\n",
            "          \"characterOffsetEnd\": 2038,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"her\",\n",
            "          \"originalText\": \"her\",\n",
            "          \"characterOffsetBegin\": 2039,\n",
            "          \"characterOffsetEnd\": 2042,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 2043,\n",
            "          \"characterOffsetEnd\": 2045,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 28,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"prison\",\n",
            "          \"originalText\": \"prison\",\n",
            "          \"characterOffsetBegin\": 2047,\n",
            "          \"characterOffsetEnd\": 2053,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 2054,\n",
            "          \"characterOffsetEnd\": 2056,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"stop\",\n",
            "          \"originalText\": \"stop\",\n",
            "          \"characterOffsetBegin\": 2057,\n",
            "          \"characterOffsetEnd\": 2061,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"her\",\n",
            "          \"originalText\": \"her\",\n",
            "          \"characterOffsetBegin\": 2062,\n",
            "          \"characterOffsetEnd\": 2065,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"from\",\n",
            "          \"originalText\": \"from\",\n",
            "          \"characterOffsetBegin\": 2066,\n",
            "          \"characterOffsetEnd\": 2070,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"self-destructing\",\n",
            "          \"originalText\": \"self-destructing\",\n",
            "          \"characterOffsetBegin\": 2071,\n",
            "          \"characterOffsetEnd\": 2087,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2087,\n",
            "          \"characterOffsetEnd\": 2088,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 29,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Her\",\n",
            "          \"originalText\": \"Her\",\n",
            "          \"characterOffsetBegin\": 2090,\n",
            "          \"characterOffsetEnd\": 2093,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 30,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"sister\",\n",
            "          \"originalText\": \"sister\",\n",
            "          \"characterOffsetBegin\": 2095,\n",
            "          \"characterOffsetEnd\": 2101,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"also\",\n",
            "          \"originalText\": \"also\",\n",
            "          \"characterOffsetBegin\": 2102,\n",
            "          \"characterOffsetEnd\": 2106,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"presented\",\n",
            "          \"originalText\": \"presented\",\n",
            "          \"characterOffsetBegin\": 2107,\n",
            "          \"characterOffsetEnd\": 2116,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 2117,\n",
            "          \"characterOffsetEnd\": 2118,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"headache\",\n",
            "          \"originalText\": \"headache\",\n",
            "          \"characterOffsetBegin\": 2119,\n",
            "          \"characterOffsetEnd\": 2127,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"for\",\n",
            "          \"originalText\": \"for\",\n",
            "          \"characterOffsetBegin\": 2128,\n",
            "          \"characterOffsetEnd\": 2131,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 2132,\n",
            "          \"characterOffsetEnd\": 2135,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"authorities\",\n",
            "          \"originalText\": \"authorities\",\n",
            "          \"characterOffsetBegin\": 2136,\n",
            "          \"characterOffsetEnd\": 2147,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"when\",\n",
            "          \"originalText\": \"when\",\n",
            "          \"characterOffsetBegin\": 2148,\n",
            "          \"characterOffsetEnd\": 2152,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"she\",\n",
            "          \"originalText\": \"she\",\n",
            "          \"characterOffsetBegin\": 2153,\n",
            "          \"characterOffsetEnd\": 2156,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 2157,\n",
            "          \"characterOffsetEnd\": 2160,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 31,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"arrested\",\n",
            "          \"originalText\": \"arrested\",\n",
            "          \"characterOffsetBegin\": 2162,\n",
            "          \"characterOffsetEnd\": 2170,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"for\",\n",
            "          \"originalText\": \"for\",\n",
            "          \"characterOffsetBegin\": 2171,\n",
            "          \"characterOffsetEnd\": 2174,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"driving\",\n",
            "          \"originalText\": \"driving\",\n",
            "          \"characterOffsetBegin\": 2175,\n",
            "          \"characterOffsetEnd\": 2182,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"under\",\n",
            "          \"originalText\": \"under\",\n",
            "          \"characterOffsetBegin\": 2183,\n",
            "          \"characterOffsetEnd\": 2188,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 2189,\n",
            "          \"characterOffsetEnd\": 2192,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"influence\",\n",
            "          \"originalText\": \"influence\",\n",
            "          \"characterOffsetBegin\": 2193,\n",
            "          \"characterOffsetEnd\": 2202,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 2203,\n",
            "          \"characterOffsetEnd\": 2205,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"drugs\",\n",
            "          \"originalText\": \"drugs\",\n",
            "          \"characterOffsetBegin\": 2206,\n",
            "          \"characterOffsetEnd\": 2211,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 2212,\n",
            "          \"characterOffsetEnd\": 2215,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 2216,\n",
            "          \"characterOffsetEnd\": 2219,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"also\",\n",
            "          \"originalText\": \"also\",\n",
            "          \"characterOffsetBegin\": 2220,\n",
            "          \"characterOffsetEnd\": 2224,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"accused\",\n",
            "          \"originalText\": \"accused\",\n",
            "          \"characterOffsetBegin\": 2225,\n",
            "          \"characterOffsetEnd\": 2232,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 32,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 2234,\n",
            "          \"characterOffsetEnd\": 2236,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"pushing\",\n",
            "          \"originalText\": \"pushing\",\n",
            "          \"characterOffsetBegin\": 2237,\n",
            "          \"characterOffsetEnd\": 2244,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 2245,\n",
            "          \"characterOffsetEnd\": 2246,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"victim\",\n",
            "          \"originalText\": \"victim\",\n",
            "          \"characterOffsetBegin\": 2247,\n",
            "          \"characterOffsetEnd\": 2253,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"on\",\n",
            "          \"originalText\": \"on\",\n",
            "          \"characterOffsetBegin\": 2254,\n",
            "          \"characterOffsetEnd\": 2256,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 2257,\n",
            "          \"characterOffsetEnd\": 2259,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"train\",\n",
            "          \"originalText\": \"train\",\n",
            "          \"characterOffsetBegin\": 2260,\n",
            "          \"characterOffsetEnd\": 2265,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"tracks\",\n",
            "          \"originalText\": \"tracks\",\n",
            "          \"characterOffsetBegin\": 2266,\n",
            "          \"characterOffsetEnd\": 2272,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2272,\n",
            "          \"characterOffsetEnd\": 2273,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 33,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"She\",\n",
            "          \"originalText\": \"She\",\n",
            "          \"characterOffsetBegin\": 2275,\n",
            "          \"characterOffsetEnd\": 2278,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"received\",\n",
            "          \"originalText\": \"received\",\n",
            "          \"characterOffsetBegin\": 2279,\n",
            "          \"characterOffsetEnd\": 2287,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 2288,\n",
            "          \"characterOffsetEnd\": 2289,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"21-month\",\n",
            "          \"originalText\": \"21-month\",\n",
            "          \"characterOffsetBegin\": 2290,\n",
            "          \"characterOffsetEnd\": 2298,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"suspended\",\n",
            "          \"originalText\": \"suspended\",\n",
            "          \"characterOffsetBegin\": 2299,\n",
            "          \"characterOffsetEnd\": 2308,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 34,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"jail\",\n",
            "          \"originalText\": \"jail\",\n",
            "          \"characterOffsetBegin\": 2310,\n",
            "          \"characterOffsetEnd\": 2314,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"sentence\",\n",
            "          \"originalText\": \"sentence\",\n",
            "          \"characterOffsetBegin\": 2315,\n",
            "          \"characterOffsetEnd\": 2323,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2323,\n",
            "          \"characterOffsetEnd\": 2324,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 35,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"The\",\n",
            "          \"originalText\": \"The\",\n",
            "          \"characterOffsetBegin\": 2326,\n",
            "          \"characterOffsetEnd\": 2329,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 36,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"women\",\n",
            "          \"originalText\": \"women\",\n",
            "          \"characterOffsetBegin\": 2331,\n",
            "          \"characterOffsetEnd\": 2336,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 2336,\n",
            "          \"characterOffsetEnd\": 2337,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"who\",\n",
            "          \"originalText\": \"who\",\n",
            "          \"characterOffsetBegin\": 2338,\n",
            "          \"characterOffsetEnd\": 2341,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"were\",\n",
            "          \"originalText\": \"were\",\n",
            "          \"characterOffsetBegin\": 2342,\n",
            "          \"characterOffsetEnd\": 2346,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"compulsive\",\n",
            "          \"originalText\": \"compulsive\",\n",
            "          \"characterOffsetBegin\": 2347,\n",
            "          \"characterOffsetEnd\": 2357,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"long-distance\",\n",
            "          \"originalText\": \"long-distance\",\n",
            "          \"characterOffsetBegin\": 2358,\n",
            "          \"characterOffsetEnd\": 2371,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"runners\",\n",
            "          \"originalText\": \"runners\",\n",
            "          \"characterOffsetBegin\": 2372,\n",
            "          \"characterOffsetEnd\": 2379,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 2379,\n",
            "          \"characterOffsetEnd\": 2380,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"described\",\n",
            "          \"originalText\": \"described\",\n",
            "          \"characterOffsetBegin\": 2381,\n",
            "          \"characterOffsetEnd\": 2390,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"themselves\",\n",
            "          \"originalText\": \"themselves\",\n",
            "          \"characterOffsetBegin\": 2391,\n",
            "          \"characterOffsetEnd\": 2401,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 37,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"as\",\n",
            "          \"originalText\": \"as\",\n",
            "          \"characterOffsetBegin\": 2403,\n",
            "          \"characterOffsetEnd\": 2405,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"perfectionists\",\n",
            "          \"originalText\": \"perfectionists\",\n",
            "          \"characterOffsetBegin\": 2406,\n",
            "          \"characterOffsetEnd\": 2420,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 2421,\n",
            "          \"characterOffsetEnd\": 2423,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"biomedical\",\n",
            "          \"originalText\": \"biomedical\",\n",
            "          \"characterOffsetBegin\": 2424,\n",
            "          \"characterOffsetEnd\": 2434,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"science\",\n",
            "          \"originalText\": \"science\",\n",
            "          \"characterOffsetBegin\": 2435,\n",
            "          \"characterOffsetEnd\": 2442,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 2443,\n",
            "          \"characterOffsetEnd\": 2446,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"physical\",\n",
            "          \"originalText\": \"physical\",\n",
            "          \"characterOffsetBegin\": 2447,\n",
            "          \"characterOffsetEnd\": 2455,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"education\",\n",
            "          \"originalText\": \"education\",\n",
            "          \"characterOffsetBegin\": 2456,\n",
            "          \"characterOffsetEnd\": 2465,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 2465,\n",
            "          \"characterOffsetEnd\": 2466,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"topics\",\n",
            "          \"originalText\": \"topics\",\n",
            "          \"characterOffsetBegin\": 2467,\n",
            "          \"characterOffsetEnd\": 2473,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 38,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 2475,\n",
            "          \"characterOffsetEnd\": 2479,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"studied\",\n",
            "          \"originalText\": \"studied\",\n",
            "          \"characterOffsetBegin\": 2480,\n",
            "          \"characterOffsetEnd\": 2487,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"avidly\",\n",
            "          \"originalText\": \"avidly\",\n",
            "          \"characterOffsetBegin\": 2488,\n",
            "          \"characterOffsetEnd\": 2494,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 2494,\n",
            "          \"characterOffsetEnd\": 2495,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"side\",\n",
            "          \"originalText\": \"side\",\n",
            "          \"characterOffsetBegin\": 2496,\n",
            "          \"characterOffsetEnd\": 2500,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"by\",\n",
            "          \"originalText\": \"by\",\n",
            "          \"characterOffsetBegin\": 2501,\n",
            "          \"characterOffsetEnd\": 2503,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"side\",\n",
            "          \"originalText\": \"side\",\n",
            "          \"characterOffsetBegin\": 2504,\n",
            "          \"characterOffsetEnd\": 2508,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2508,\n",
            "          \"characterOffsetEnd\": 2509,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 39,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"But\",\n",
            "          \"originalText\": \"But\",\n",
            "          \"characterOffsetBegin\": 2511,\n",
            "          \"characterOffsetEnd\": 2514,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"as\",\n",
            "          \"originalText\": \"as\",\n",
            "          \"characterOffsetBegin\": 2515,\n",
            "          \"characterOffsetEnd\": 2517,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 2518,\n",
            "          \"characterOffsetEnd\": 2522,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"started\",\n",
            "          \"originalText\": \"started\",\n",
            "          \"characterOffsetBegin\": 2523,\n",
            "          \"characterOffsetEnd\": 2530,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 2531,\n",
            "          \"characterOffsetEnd\": 2533,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"waste\",\n",
            "          \"originalText\": \"waste\",\n",
            "          \"characterOffsetBegin\": 2534,\n",
            "          \"characterOffsetEnd\": 2539,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"away\",\n",
            "          \"originalText\": \"away\",\n",
            "          \"characterOffsetBegin\": 2540,\n",
            "          \"characterOffsetEnd\": 2544,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 40,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"over\",\n",
            "          \"originalText\": \"over\",\n",
            "          \"characterOffsetBegin\": 2546,\n",
            "          \"characterOffsetEnd\": 2550,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"two\",\n",
            "          \"originalText\": \"two\",\n",
            "          \"characterOffsetBegin\": 2551,\n",
            "          \"characterOffsetEnd\": 2554,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"decades\",\n",
            "          \"originalText\": \"decades\",\n",
            "          \"characterOffsetBegin\": 2555,\n",
            "          \"characterOffsetEnd\": 2562,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 2562,\n",
            "          \"characterOffsetEnd\": 2563,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 2564,\n",
            "          \"characterOffsetEnd\": 2567,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"weight\",\n",
            "          \"originalText\": \"weight\",\n",
            "          \"characterOffsetBegin\": 2568,\n",
            "          \"characterOffsetEnd\": 2574,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 2575,\n",
            "          \"characterOffsetEnd\": 2577,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"each\",\n",
            "          \"originalText\": \"each\",\n",
            "          \"characterOffsetBegin\": 2578,\n",
            "          \"characterOffsetEnd\": 2582,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 2583,\n",
            "          \"characterOffsetEnd\": 2585,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"them\",\n",
            "          \"originalText\": \"them\",\n",
            "          \"characterOffsetBegin\": 2586,\n",
            "          \"characterOffsetEnd\": 2590,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"dropped\",\n",
            "          \"originalText\": \"dropped\",\n",
            "          \"characterOffsetBegin\": 2591,\n",
            "          \"characterOffsetEnd\": 2598,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 2599,\n",
            "          \"characterOffsetEnd\": 2601,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"little\",\n",
            "          \"originalText\": \"little\",\n",
            "          \"characterOffsetBegin\": 2602,\n",
            "          \"characterOffsetEnd\": 2608,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"more\",\n",
            "          \"originalText\": \"more\",\n",
            "          \"characterOffsetBegin\": 2609,\n",
            "          \"characterOffsetEnd\": 2613,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"than\",\n",
            "          \"originalText\": \"than\",\n",
            "          \"characterOffsetBegin\": 2614,\n",
            "          \"characterOffsetEnd\": 2618,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 41,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"four\",\n",
            "          \"originalText\": \"four\",\n",
            "          \"characterOffsetBegin\": 2620,\n",
            "          \"characterOffsetEnd\": 2624,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"stone\",\n",
            "          \"originalText\": \"stone\",\n",
            "          \"characterOffsetBegin\": 2625,\n",
            "          \"characterOffsetEnd\": 2630,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2630,\n",
            "          \"characterOffsetEnd\": 2631,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 42,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Doctors\",\n",
            "          \"originalText\": \"Doctors\",\n",
            "          \"characterOffsetBegin\": 2632,\n",
            "          \"characterOffsetEnd\": 2639,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"said\",\n",
            "          \"originalText\": \"said\",\n",
            "          \"characterOffsetBegin\": 2640,\n",
            "          \"characterOffsetEnd\": 2644,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 2645,\n",
            "          \"characterOffsetEnd\": 2649,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 2650,\n",
            "          \"characterOffsetEnd\": 2653,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 2654,\n",
            "          \"characterOffsetEnd\": 2657,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"bone\",\n",
            "          \"originalText\": \"bone\",\n",
            "          \"characterOffsetBegin\": 2658,\n",
            "          \"characterOffsetEnd\": 2662,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"structure\",\n",
            "          \"originalText\": \"structure\",\n",
            "          \"characterOffsetBegin\": 2663,\n",
            "          \"characterOffsetEnd\": 2672,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 2673,\n",
            "          \"characterOffsetEnd\": 2675,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"women\",\n",
            "          \"originalText\": \"women\",\n",
            "          \"characterOffsetBegin\": 2676,\n",
            "          \"characterOffsetEnd\": 2681,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"aged\",\n",
            "          \"originalText\": \"aged\",\n",
            "          \"characterOffsetBegin\": 2682,\n",
            "          \"characterOffsetEnd\": 2686,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 43,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"between\",\n",
            "          \"originalText\": \"between\",\n",
            "          \"characterOffsetBegin\": 2688,\n",
            "          \"characterOffsetEnd\": 2695,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"70\",\n",
            "          \"originalText\": \"70\",\n",
            "          \"characterOffsetBegin\": 2696,\n",
            "          \"characterOffsetEnd\": 2698,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 2699,\n",
            "          \"characterOffsetEnd\": 2702,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"100\",\n",
            "          \"originalText\": \"100\",\n",
            "          \"characterOffsetBegin\": 2703,\n",
            "          \"characterOffsetEnd\": 2706,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2706,\n",
            "          \"characterOffsetEnd\": 2707,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 44,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"The\",\n",
            "          \"originalText\": \"The\",\n",
            "          \"characterOffsetBegin\": 2709,\n",
            "          \"characterOffsetEnd\": 2712,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 45,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"twins\",\n",
            "          \"originalText\": \"twins\",\n",
            "          \"characterOffsetBegin\": 2714,\n",
            "          \"characterOffsetEnd\": 2719,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"developed\",\n",
            "          \"originalText\": \"developed\",\n",
            "          \"characterOffsetBegin\": 2720,\n",
            "          \"characterOffsetEnd\": 2729,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"severe\",\n",
            "          \"originalText\": \"severe\",\n",
            "          \"characterOffsetBegin\": 2730,\n",
            "          \"characterOffsetEnd\": 2736,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"eating\",\n",
            "          \"originalText\": \"eating\",\n",
            "          \"characterOffsetBegin\": 2737,\n",
            "          \"characterOffsetEnd\": 2743,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"disorders\",\n",
            "          \"originalText\": \"disorders\",\n",
            "          \"characterOffsetBegin\": 2744,\n",
            "          \"characterOffsetEnd\": 2753,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 2754,\n",
            "          \"characterOffsetEnd\": 2756,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 2757,\n",
            "          \"characterOffsetEnd\": 2762,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"early\",\n",
            "          \"originalText\": \"early\",\n",
            "          \"characterOffsetBegin\": 2763,\n",
            "          \"characterOffsetEnd\": 2768,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"teens\",\n",
            "          \"originalText\": \"teens\",\n",
            "          \"characterOffsetBegin\": 2769,\n",
            "          \"characterOffsetEnd\": 2774,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"losing\",\n",
            "          \"originalText\": \"losing\",\n",
            "          \"characterOffsetBegin\": 2775,\n",
            "          \"characterOffsetEnd\": 2781,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"more\",\n",
            "          \"originalText\": \"more\",\n",
            "          \"characterOffsetBegin\": 2782,\n",
            "          \"characterOffsetEnd\": 2786,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 46,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"weight\",\n",
            "          \"originalText\": \"weight\",\n",
            "          \"characterOffsetBegin\": 2788,\n",
            "          \"characterOffsetEnd\": 2794,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"when\",\n",
            "          \"originalText\": \"when\",\n",
            "          \"characterOffsetBegin\": 2795,\n",
            "          \"characterOffsetEnd\": 2799,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 2800,\n",
            "          \"characterOffsetEnd\": 2804,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"became\",\n",
            "          \"originalText\": \"became\",\n",
            "          \"characterOffsetBegin\": 2805,\n",
            "          \"characterOffsetEnd\": 2811,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"addicted\",\n",
            "          \"originalText\": \"addicted\",\n",
            "          \"characterOffsetBegin\": 2812,\n",
            "          \"characterOffsetEnd\": 2820,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 2821,\n",
            "          \"characterOffsetEnd\": 2823,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"long-distance\",\n",
            "          \"originalText\": \"long-distance\",\n",
            "          \"characterOffsetBegin\": 2824,\n",
            "          \"characterOffsetEnd\": 2837,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"running\",\n",
            "          \"originalText\": \"running\",\n",
            "          \"characterOffsetBegin\": 2838,\n",
            "          \"characterOffsetEnd\": 2845,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2845,\n",
            "          \"characterOffsetEnd\": 2846,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 47,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"They\",\n",
            "          \"originalText\": \"They\",\n",
            "          \"characterOffsetBegin\": 2848,\n",
            "          \"characterOffsetEnd\": 2852,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"were\",\n",
            "          \"originalText\": \"were\",\n",
            "          \"characterOffsetBegin\": 2853,\n",
            "          \"characterOffsetEnd\": 2857,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"so\",\n",
            "          \"originalText\": \"so\",\n",
            "          \"characterOffsetBegin\": 2858,\n",
            "          \"characterOffsetEnd\": 2860,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 48,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"obsessed\",\n",
            "          \"originalText\": \"obsessed\",\n",
            "          \"characterOffsetBegin\": 2862,\n",
            "          \"characterOffsetEnd\": 2870,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"characterOffsetBegin\": 2871,\n",
            "          \"characterOffsetEnd\": 2875,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"marathons\",\n",
            "          \"originalText\": \"marathons\",\n",
            "          \"characterOffsetBegin\": 2876,\n",
            "          \"characterOffsetEnd\": 2885,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"that\",\n",
            "          \"originalText\": \"that\",\n",
            "          \"characterOffsetBegin\": 2886,\n",
            "          \"characterOffsetEnd\": 2890,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 2891,\n",
            "          \"characterOffsetEnd\": 2895,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"each\",\n",
            "          \"originalText\": \"each\",\n",
            "          \"characterOffsetBegin\": 2896,\n",
            "          \"characterOffsetEnd\": 2900,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"suffered\",\n",
            "          \"originalText\": \"suffered\",\n",
            "          \"characterOffsetBegin\": 2901,\n",
            "          \"characterOffsetEnd\": 2909,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"stress\",\n",
            "          \"originalText\": \"stress\",\n",
            "          \"characterOffsetBegin\": 2910,\n",
            "          \"characterOffsetEnd\": 2916,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"fractures\",\n",
            "          \"originalText\": \"fractures\",\n",
            "          \"characterOffsetBegin\": 2917,\n",
            "          \"characterOffsetEnd\": 2926,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 2927,\n",
            "          \"characterOffsetEnd\": 2929,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 49,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 2931,\n",
            "          \"characterOffsetEnd\": 2936,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"feet\",\n",
            "          \"originalText\": \"feet\",\n",
            "          \"characterOffsetBegin\": 2937,\n",
            "          \"characterOffsetEnd\": 2941,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 2941,\n",
            "          \"characterOffsetEnd\": 2942,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 50,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Problem\",\n",
            "          \"originalText\": \"Problem\",\n",
            "          \"characterOffsetBegin\": 2944,\n",
            "          \"characterOffsetEnd\": 2951,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"pair\",\n",
            "          \"originalText\": \"pair\",\n",
            "          \"characterOffsetBegin\": 2952,\n",
            "          \"characterOffsetEnd\": 2956,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 2956,\n",
            "          \"characterOffsetEnd\": 2957,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 2958,\n",
            "          \"characterOffsetEnd\": 2963,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 2964,\n",
            "          \"characterOffsetEnd\": 2967,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 2968,\n",
            "          \"characterOffsetEnd\": 2974,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"Wallmeyer\",\n",
            "          \"originalText\": \"Wallmeyer\",\n",
            "          \"characterOffsetBegin\": 2975,\n",
            "          \"characterOffsetEnd\": 2984,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"pictured\",\n",
            "          \"originalText\": \"pictured\",\n",
            "          \"characterOffsetBegin\": 2985,\n",
            "          \"characterOffsetEnd\": 2993,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"leaving\",\n",
            "          \"originalText\": \"leaving\",\n",
            "          \"characterOffsetBegin\": 2994,\n",
            "          \"characterOffsetEnd\": 3001,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 3002,\n",
            "          \"characterOffsetEnd\": 3003,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"police\",\n",
            "          \"originalText\": \"police\",\n",
            "          \"characterOffsetBegin\": 3004,\n",
            "          \"characterOffsetEnd\": 3010,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"station\",\n",
            "          \"originalText\": \"station\",\n",
            "          \"characterOffsetBegin\": 3011,\n",
            "          \"characterOffsetEnd\": 3018,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 3019,\n",
            "          \"characterOffsetEnd\": 3021,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"Geelong\",\n",
            "          \"originalText\": \"Geelong\",\n",
            "          \"characterOffsetBegin\": 3022,\n",
            "          \"characterOffsetEnd\": 3029,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3029,\n",
            "          \"characterOffsetEnd\": 3030,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"Australia\",\n",
            "          \"originalText\": \"Australia\",\n",
            "          \"characterOffsetBegin\": 3031,\n",
            "          \"characterOffsetEnd\": 3040,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 3041,\n",
            "          \"characterOffsetEnd\": 3043,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"2007\",\n",
            "          \"originalText\": \"2007\",\n",
            "          \"characterOffsetBegin\": 3044,\n",
            "          \"characterOffsetEnd\": 3048,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 51,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Blaze\",\n",
            "          \"originalText\": \"Blaze\",\n",
            "          \"characterOffsetBegin\": 3050,\n",
            "          \"characterOffsetEnd\": 3055,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 3055,\n",
            "          \"characterOffsetEnd\": 3056,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"The\",\n",
            "          \"originalText\": \"The\",\n",
            "          \"characterOffsetBegin\": 3057,\n",
            "          \"characterOffsetEnd\": 3060,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"home\",\n",
            "          \"originalText\": \"home\",\n",
            "          \"characterOffsetBegin\": 3061,\n",
            "          \"characterOffsetEnd\": 3065,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 3066,\n",
            "          \"characterOffsetEnd\": 3068,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"Geelong\",\n",
            "          \"originalText\": \"Geelong\",\n",
            "          \"characterOffsetBegin\": 3069,\n",
            "          \"characterOffsetEnd\": 3076,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3076,\n",
            "          \"characterOffsetEnd\": 3077,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"Australia\",\n",
            "          \"originalText\": \"Australia\",\n",
            "          \"characterOffsetBegin\": 3078,\n",
            "          \"characterOffsetEnd\": 3087,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3087,\n",
            "          \"characterOffsetEnd\": 3088,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"where\",\n",
            "          \"originalText\": \"where\",\n",
            "          \"characterOffsetBegin\": 3089,\n",
            "          \"characterOffsetEnd\": 3094,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3095,\n",
            "          \"characterOffsetEnd\": 3098,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"anorexic\",\n",
            "          \"originalText\": \"anorexic\",\n",
            "          \"characterOffsetBegin\": 3099,\n",
            "          \"characterOffsetEnd\": 3107,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"twin\",\n",
            "          \"originalText\": \"twin\",\n",
            "          \"characterOffsetBegin\": 3108,\n",
            "          \"characterOffsetEnd\": 3112,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"sisters\",\n",
            "          \"originalText\": \"sisters\",\n",
            "          \"characterOffsetBegin\": 3113,\n",
            "          \"characterOffsetEnd\": 3120,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"died\",\n",
            "          \"originalText\": \"died\",\n",
            "          \"characterOffsetBegin\": 3121,\n",
            "          \"characterOffsetEnd\": 3125,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 52,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Inseparable\",\n",
            "          \"originalText\": \"Inseparable\",\n",
            "          \"characterOffsetBegin\": 3127,\n",
            "          \"characterOffsetEnd\": 3138,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 3139,\n",
            "          \"characterOffsetEnd\": 3141,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 3142,\n",
            "          \"characterOffsetEnd\": 3147,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"tragic\",\n",
            "          \"originalText\": \"tragic\",\n",
            "          \"characterOffsetBegin\": 3148,\n",
            "          \"characterOffsetEnd\": 3154,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"end\",\n",
            "          \"originalText\": \"end\",\n",
            "          \"characterOffsetBegin\": 3155,\n",
            "          \"characterOffsetEnd\": 3158,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3158,\n",
            "          \"characterOffsetEnd\": 3159,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 53,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"there\",\n",
            "          \"originalText\": \"there\",\n",
            "          \"characterOffsetBegin\": 3161,\n",
            "          \"characterOffsetEnd\": 3166,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 3167,\n",
            "          \"characterOffsetEnd\": 3170,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3171,\n",
            "          \"characterOffsetEnd\": 3174,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"time\",\n",
            "          \"originalText\": \"time\",\n",
            "          \"characterOffsetBegin\": 3175,\n",
            "          \"characterOffsetEnd\": 3179,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 3180,\n",
            "          \"characterOffsetEnd\": 3182,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"1996\",\n",
            "          \"originalText\": \"1996\",\n",
            "          \"characterOffsetBegin\": 3183,\n",
            "          \"characterOffsetEnd\": 3187,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"when\",\n",
            "          \"originalText\": \"when\",\n",
            "          \"characterOffsetBegin\": 3188,\n",
            "          \"characterOffsetEnd\": 3192,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 3193,\n",
            "          \"characterOffsetEnd\": 3199,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 3200,\n",
            "          \"characterOffsetEnd\": 3203,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"so\",\n",
            "          \"originalText\": \"so\",\n",
            "          \"characterOffsetBegin\": 3204,\n",
            "          \"characterOffsetEnd\": 3206,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"ill\",\n",
            "          \"originalText\": \"ill\",\n",
            "          \"characterOffsetBegin\": 3207,\n",
            "          \"characterOffsetEnd\": 3210,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"she\",\n",
            "          \"originalText\": \"she\",\n",
            "          \"characterOffsetBegin\": 3211,\n",
            "          \"characterOffsetEnd\": 3214,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 3215,\n",
            "          \"characterOffsetEnd\": 3218,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"admitted\",\n",
            "          \"originalText\": \"admitted\",\n",
            "          \"characterOffsetBegin\": 3219,\n",
            "          \"characterOffsetEnd\": 3227,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 3228,\n",
            "          \"characterOffsetEnd\": 3230,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 3231,\n",
            "          \"characterOffsetEnd\": 3232,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 54,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"psychiatric\",\n",
            "          \"originalText\": \"psychiatric\",\n",
            "          \"characterOffsetBegin\": 3234,\n",
            "          \"characterOffsetEnd\": 3245,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"unit\",\n",
            "          \"originalText\": \"unit\",\n",
            "          \"characterOffsetBegin\": 3246,\n",
            "          \"characterOffsetEnd\": 3250,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"at\",\n",
            "          \"originalText\": \"at\",\n",
            "          \"characterOffsetBegin\": 3251,\n",
            "          \"characterOffsetEnd\": 3253,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3254,\n",
            "          \"characterOffsetEnd\": 3257,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Royal\",\n",
            "          \"originalText\": \"Royal\",\n",
            "          \"characterOffsetBegin\": 3258,\n",
            "          \"characterOffsetEnd\": 3263,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"Melbourne\",\n",
            "          \"originalText\": \"Melbourne\",\n",
            "          \"characterOffsetBegin\": 3264,\n",
            "          \"characterOffsetEnd\": 3273,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"Hospital\",\n",
            "          \"originalText\": \"Hospital\",\n",
            "          \"characterOffsetBegin\": 3274,\n",
            "          \"characterOffsetEnd\": 3282,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"--\",\n",
            "          \"originalText\": \"–\",\n",
            "          \"characterOffsetBegin\": 3283,\n",
            "          \"characterOffsetEnd\": 3284,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"followed\",\n",
            "          \"originalText\": \"followed\",\n",
            "          \"characterOffsetBegin\": 3285,\n",
            "          \"characterOffsetEnd\": 3293,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"by\",\n",
            "          \"originalText\": \"by\",\n",
            "          \"characterOffsetBegin\": 3294,\n",
            "          \"characterOffsetEnd\": 3296,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 3297,\n",
            "          \"characterOffsetEnd\": 3302,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"who\",\n",
            "          \"originalText\": \"who\",\n",
            "          \"characterOffsetBegin\": 3303,\n",
            "          \"characterOffsetEnd\": 3306,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 55,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"voluntarily\",\n",
            "          \"originalText\": \"voluntarily\",\n",
            "          \"characterOffsetBegin\": 3308,\n",
            "          \"characterOffsetEnd\": 3319,\n",
            "          \"before\": \"\\n \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"admitted\",\n",
            "          \"originalText\": \"admitted\",\n",
            "          \"characterOffsetBegin\": 3320,\n",
            "          \"characterOffsetEnd\": 3328,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"herself\",\n",
            "          \"originalText\": \"herself\",\n",
            "          \"characterOffsetBegin\": 3329,\n",
            "          \"characterOffsetEnd\": 3336,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"too\",\n",
            "          \"originalText\": \"too\",\n",
            "          \"characterOffsetBegin\": 3337,\n",
            "          \"characterOffsetEnd\": 3340,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 3340,\n",
            "          \"characterOffsetEnd\": 3341,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 56,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 3343,\n",
            "          \"characterOffsetEnd\": 3349,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"told\",\n",
            "          \"originalText\": \"told\",\n",
            "          \"characterOffsetBegin\": 3350,\n",
            "          \"characterOffsetEnd\": 3354,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Melbourne\",\n",
            "          \"originalText\": \"Melbourne\",\n",
            "          \"characterOffsetBegin\": 3355,\n",
            "          \"characterOffsetEnd\": 3364,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"'s\",\n",
            "          \"originalText\": \"’s\",\n",
            "          \"characterOffsetBegin\": 3364,\n",
            "          \"characterOffsetEnd\": 3366,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Herald\",\n",
            "          \"originalText\": \"Herald\",\n",
            "          \"characterOffsetBegin\": 3367,\n",
            "          \"characterOffsetEnd\": 3373,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"Sun\",\n",
            "          \"originalText\": \"Sun\",\n",
            "          \"characterOffsetBegin\": 3374,\n",
            "          \"characterOffsetEnd\": 3377,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 57,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"newspaper\",\n",
            "          \"originalText\": \"newspaper\",\n",
            "          \"characterOffsetBegin\": 3379,\n",
            "          \"characterOffsetEnd\": 3388,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"on\",\n",
            "          \"originalText\": \"on\",\n",
            "          \"characterOffsetBegin\": 3389,\n",
            "          \"characterOffsetEnd\": 3391,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"one\",\n",
            "          \"originalText\": \"one\",\n",
            "          \"characterOffsetBegin\": 3392,\n",
            "          \"characterOffsetEnd\": 3395,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"occasion\",\n",
            "          \"originalText\": \"occasion\",\n",
            "          \"characterOffsetBegin\": 3396,\n",
            "          \"characterOffsetEnd\": 3404,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"that\",\n",
            "          \"originalText\": \"that\",\n",
            "          \"characterOffsetBegin\": 3405,\n",
            "          \"characterOffsetEnd\": 3409,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"no-one\",\n",
            "          \"originalText\": \"no-one\",\n",
            "          \"characterOffsetBegin\": 3410,\n",
            "          \"characterOffsetEnd\": 3416,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"understood\",\n",
            "          \"originalText\": \"understood\",\n",
            "          \"characterOffsetBegin\": 3417,\n",
            "          \"characterOffsetEnd\": 3427,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"anorexia\",\n",
            "          \"originalText\": \"anorexia\",\n",
            "          \"characterOffsetBegin\": 3428,\n",
            "          \"characterOffsetEnd\": 3436,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"until\",\n",
            "          \"originalText\": \"until\",\n",
            "          \"characterOffsetBegin\": 3437,\n",
            "          \"characterOffsetEnd\": 3442,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 3443,\n",
            "          \"characterOffsetEnd\": 3447,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 58,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"have\",\n",
            "          \"originalText\": \"have\",\n",
            "          \"characterOffsetBegin\": 3449,\n",
            "          \"characterOffsetEnd\": 3453,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"lived\",\n",
            "          \"originalText\": \"lived\",\n",
            "          \"characterOffsetBegin\": 3454,\n",
            "          \"characterOffsetEnd\": 3459,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"it\",\n",
            "          \"originalText\": \"it\",\n",
            "          \"characterOffsetBegin\": 3460,\n",
            "          \"characterOffsetEnd\": 3462,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 3462,\n",
            "          \"characterOffsetEnd\": 3463,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 59,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"‘\",\n",
            "          \"characterOffsetBegin\": 3465,\n",
            "          \"characterOffsetEnd\": 3466,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"It\",\n",
            "          \"originalText\": \"It\",\n",
            "          \"characterOffsetBegin\": 3466,\n",
            "          \"characterOffsetEnd\": 3468,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"'s\",\n",
            "          \"originalText\": \"’s\",\n",
            "          \"characterOffsetBegin\": 3468,\n",
            "          \"characterOffsetEnd\": 3470,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"like\",\n",
            "          \"originalText\": \"like\",\n",
            "          \"characterOffsetBegin\": 3471,\n",
            "          \"characterOffsetEnd\": 3475,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3476,\n",
            "          \"characterOffsetEnd\": 3479,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"Grim\",\n",
            "          \"originalText\": \"Grim\",\n",
            "          \"characterOffsetBegin\": 3480,\n",
            "          \"characterOffsetEnd\": 3484,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"Reaper\",\n",
            "          \"originalText\": \"Reaper\",\n",
            "          \"characterOffsetBegin\": 3485,\n",
            "          \"characterOffsetEnd\": 3491,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"--\",\n",
            "          \"originalText\": \"–\",\n",
            "          \"characterOffsetBegin\": 3492,\n",
            "          \"characterOffsetEnd\": 3493,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 3494,\n",
            "          \"characterOffsetEnd\": 3495,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"black\",\n",
            "          \"originalText\": \"black\",\n",
            "          \"characterOffsetBegin\": 3496,\n",
            "          \"characterOffsetEnd\": 3501,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"hole\",\n",
            "          \"originalText\": \"hole\",\n",
            "          \"characterOffsetBegin\": 3502,\n",
            "          \"characterOffsetEnd\": 3506,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 3507,\n",
            "          \"characterOffsetEnd\": 3509,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"your\",\n",
            "          \"originalText\": \"your\",\n",
            "          \"characterOffsetBegin\": 3510,\n",
            "          \"characterOffsetEnd\": 3514,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"soul\",\n",
            "          \"originalText\": \"soul\",\n",
            "          \"characterOffsetBegin\": 3515,\n",
            "          \"characterOffsetEnd\": 3519,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3519,\n",
            "          \"characterOffsetEnd\": 3520,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"’\",\n",
            "          \"characterOffsetBegin\": 3520,\n",
            "          \"characterOffsetEnd\": 3521,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 60,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"she\",\n",
            "          \"originalText\": \"she\",\n",
            "          \"characterOffsetBegin\": 3523,\n",
            "          \"characterOffsetEnd\": 3526,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"said\",\n",
            "          \"originalText\": \"said\",\n",
            "          \"characterOffsetBegin\": 3527,\n",
            "          \"characterOffsetEnd\": 3531,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 3531,\n",
            "          \"characterOffsetEnd\": 3532,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 61,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Their\",\n",
            "          \"originalText\": \"Their\",\n",
            "          \"characterOffsetBegin\": 3534,\n",
            "          \"characterOffsetEnd\": 3539,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 62,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"parents\",\n",
            "          \"originalText\": \"parents\",\n",
            "          \"characterOffsetBegin\": 3541,\n",
            "          \"characterOffsetEnd\": 3548,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3548,\n",
            "          \"characterOffsetEnd\": 3549,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Bob\",\n",
            "          \"originalText\": \"Bob\",\n",
            "          \"characterOffsetBegin\": 3550,\n",
            "          \"characterOffsetEnd\": 3553,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 3554,\n",
            "          \"characterOffsetEnd\": 3557,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Moya\",\n",
            "          \"originalText\": \"Moya\",\n",
            "          \"characterOffsetBegin\": 3558,\n",
            "          \"characterOffsetEnd\": 3562,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"admitted\",\n",
            "          \"originalText\": \"admitted\",\n",
            "          \"characterOffsetBegin\": 3563,\n",
            "          \"characterOffsetEnd\": 3571,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"that\",\n",
            "          \"originalText\": \"that\",\n",
            "          \"characterOffsetBegin\": 3572,\n",
            "          \"characterOffsetEnd\": 3576,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"when\",\n",
            "          \"originalText\": \"when\",\n",
            "          \"characterOffsetBegin\": 3577,\n",
            "          \"characterOffsetEnd\": 3581,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3582,\n",
            "          \"characterOffsetEnd\": 3585,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"twins\",\n",
            "          \"originalText\": \"twins\",\n",
            "          \"characterOffsetBegin\": 3586,\n",
            "          \"characterOffsetEnd\": 3591,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"were\",\n",
            "          \"originalText\": \"were\",\n",
            "          \"characterOffsetBegin\": 3592,\n",
            "          \"characterOffsetEnd\": 3596,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"teenagers\",\n",
            "          \"originalText\": \"teenagers\",\n",
            "          \"characterOffsetBegin\": 3597,\n",
            "          \"characterOffsetEnd\": 3606,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 3607,\n",
            "          \"characterOffsetEnd\": 3611,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 63,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"feared\",\n",
            "          \"originalText\": \"feared\",\n",
            "          \"characterOffsetBegin\": 3613,\n",
            "          \"characterOffsetEnd\": 3619,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 3620,\n",
            "          \"characterOffsetEnd\": 3624,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"would\",\n",
            "          \"originalText\": \"would\",\n",
            "          \"characterOffsetBegin\": 3625,\n",
            "          \"characterOffsetEnd\": 3630,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"find\",\n",
            "          \"originalText\": \"find\",\n",
            "          \"characterOffsetBegin\": 3631,\n",
            "          \"characterOffsetEnd\": 3635,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"them\",\n",
            "          \"originalText\": \"them\",\n",
            "          \"characterOffsetBegin\": 3636,\n",
            "          \"characterOffsetEnd\": 3640,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"dead\",\n",
            "          \"originalText\": \"dead\",\n",
            "          \"characterOffsetBegin\": 3641,\n",
            "          \"characterOffsetEnd\": 3645,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 3646,\n",
            "          \"characterOffsetEnd\": 3648,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"bed\",\n",
            "          \"originalText\": \"bed\",\n",
            "          \"characterOffsetBegin\": 3649,\n",
            "          \"characterOffsetEnd\": 3652,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"because\",\n",
            "          \"originalText\": \"because\",\n",
            "          \"characterOffsetBegin\": 3653,\n",
            "          \"characterOffsetEnd\": 3660,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 3661,\n",
            "          \"characterOffsetEnd\": 3663,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 3664,\n",
            "          \"characterOffsetEnd\": 3669,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"disorder\",\n",
            "          \"originalText\": \"disorder\",\n",
            "          \"characterOffsetBegin\": 3670,\n",
            "          \"characterOffsetEnd\": 3678,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 3678,\n",
            "          \"characterOffsetEnd\": 3679,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 64,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Happier\",\n",
            "          \"originalText\": \"Happier\",\n",
            "          \"characterOffsetBegin\": 3681,\n",
            "          \"characterOffsetEnd\": 3688,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"times\",\n",
            "          \"originalText\": \"times\",\n",
            "          \"characterOffsetBegin\": 3689,\n",
            "          \"characterOffsetEnd\": 3694,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 3694,\n",
            "          \"characterOffsetEnd\": 3695,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"But\",\n",
            "          \"originalText\": \"But\",\n",
            "          \"characterOffsetBegin\": 3696,\n",
            "          \"characterOffsetEnd\": 3699,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"behind\",\n",
            "          \"originalText\": \"behind\",\n",
            "          \"characterOffsetBegin\": 3700,\n",
            "          \"characterOffsetEnd\": 3706,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3707,\n",
            "          \"characterOffsetEnd\": 3710,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"smiles\",\n",
            "          \"originalText\": \"smiles\",\n",
            "          \"characterOffsetBegin\": 3711,\n",
            "          \"characterOffsetEnd\": 3717,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3718,\n",
            "          \"characterOffsetEnd\": 3721,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"two\",\n",
            "          \"originalText\": \"two\",\n",
            "          \"characterOffsetBegin\": 3722,\n",
            "          \"characterOffsetEnd\": 3725,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"sisters\",\n",
            "          \"originalText\": \"sisters\",\n",
            "          \"characterOffsetBegin\": 3726,\n",
            "          \"characterOffsetEnd\": 3733,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"led\",\n",
            "          \"originalText\": \"led\",\n",
            "          \"characterOffsetBegin\": 3734,\n",
            "          \"characterOffsetEnd\": 3737,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 3738,\n",
            "          \"characterOffsetEnd\": 3739,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"turbulent\",\n",
            "          \"originalText\": \"turbulent\",\n",
            "          \"characterOffsetBegin\": 3740,\n",
            "          \"characterOffsetEnd\": 3749,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3749,\n",
            "          \"characterOffsetEnd\": 3750,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"troubled\",\n",
            "          \"originalText\": \"troubled\",\n",
            "          \"characterOffsetBegin\": 3751,\n",
            "          \"characterOffsetEnd\": 3759,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"existence\",\n",
            "          \"originalText\": \"existence\",\n",
            "          \"characterOffsetBegin\": 3760,\n",
            "          \"characterOffsetEnd\": 3769,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 3769,\n",
            "          \"characterOffsetEnd\": 3770,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 65,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Pictured\",\n",
            "          \"originalText\": \"Pictured\",\n",
            "          \"characterOffsetBegin\": 3771,\n",
            "          \"characterOffsetEnd\": 3779,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"characterOffsetBegin\": 3780,\n",
            "          \"characterOffsetEnd\": 3784,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"friend\",\n",
            "          \"originalText\": \"friend\",\n",
            "          \"characterOffsetBegin\": 3785,\n",
            "          \"characterOffsetEnd\": 3791,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"Rachael\",\n",
            "          \"originalText\": \"Rachael\",\n",
            "          \"characterOffsetBegin\": 3792,\n",
            "          \"characterOffsetEnd\": 3799,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Walker\",\n",
            "          \"originalText\": \"Walker\",\n",
            "          \"characterOffsetBegin\": 3800,\n",
            "          \"characterOffsetEnd\": 3806,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 66,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Desperate\",\n",
            "          \"originalText\": \"Desperate\",\n",
            "          \"characterOffsetBegin\": 3808,\n",
            "          \"characterOffsetEnd\": 3817,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 3817,\n",
            "          \"characterOffsetEnd\": 3818,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Authorities\",\n",
            "          \"originalText\": \"Authorities\",\n",
            "          \"characterOffsetBegin\": 3819,\n",
            "          \"characterOffsetEnd\": 3830,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"considered\",\n",
            "          \"originalText\": \"considered\",\n",
            "          \"characterOffsetBegin\": 3831,\n",
            "          \"characterOffsetEnd\": 3841,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"jailing\",\n",
            "          \"originalText\": \"jailing\",\n",
            "          \"characterOffsetBegin\": 3842,\n",
            "          \"characterOffsetEnd\": 3849,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"them\",\n",
            "          \"originalText\": \"them\",\n",
            "          \"characterOffsetBegin\": 3850,\n",
            "          \"characterOffsetEnd\": 3854,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 3855,\n",
            "          \"characterOffsetEnd\": 3857,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"an\",\n",
            "          \"originalText\": \"an\",\n",
            "          \"characterOffsetBegin\": 3858,\n",
            "          \"characterOffsetEnd\": 3860,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"attempt\",\n",
            "          \"originalText\": \"attempt\",\n",
            "          \"characterOffsetBegin\": 3861,\n",
            "          \"characterOffsetEnd\": 3868,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 3869,\n",
            "          \"characterOffsetEnd\": 3871,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"stop\",\n",
            "          \"originalText\": \"stop\",\n",
            "          \"characterOffsetBegin\": 3872,\n",
            "          \"characterOffsetEnd\": 3876,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 3877,\n",
            "          \"characterOffsetEnd\": 3883,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3883,\n",
            "          \"characterOffsetEnd\": 3884,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"left\",\n",
            "          \"originalText\": \"left\",\n",
            "          \"characterOffsetBegin\": 3885,\n",
            "          \"characterOffsetEnd\": 3889,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3889,\n",
            "          \"characterOffsetEnd\": 3890,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 3891,\n",
            "          \"characterOffsetEnd\": 3894,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 3895,\n",
            "          \"characterOffsetEnd\": 3900,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3900,\n",
            "          \"characterOffsetEnd\": 3901,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"right\",\n",
            "          \"originalText\": \"right\",\n",
            "          \"characterOffsetBegin\": 3902,\n",
            "          \"characterOffsetEnd\": 3907,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 3907,\n",
            "          \"characterOffsetEnd\": 3908,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 21,\n",
            "          \"word\": \"starving\",\n",
            "          \"originalText\": \"starving\",\n",
            "          \"characterOffsetBegin\": 3909,\n",
            "          \"characterOffsetEnd\": 3917,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 22,\n",
            "          \"word\": \"themselves\",\n",
            "          \"originalText\": \"themselves\",\n",
            "          \"characterOffsetBegin\": 3918,\n",
            "          \"characterOffsetEnd\": 3928,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 23,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 3929,\n",
            "          \"characterOffsetEnd\": 3931,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 24,\n",
            "          \"word\": \"death\",\n",
            "          \"originalText\": \"death\",\n",
            "          \"characterOffsetBegin\": 3932,\n",
            "          \"characterOffsetEnd\": 3937,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 67,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"In\",\n",
            "          \"originalText\": \"In\",\n",
            "          \"characterOffsetBegin\": 3939,\n",
            "          \"characterOffsetEnd\": 3941,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"an\",\n",
            "          \"originalText\": \"an\",\n",
            "          \"characterOffsetBegin\": 3942,\n",
            "          \"characterOffsetEnd\": 3944,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"interview\",\n",
            "          \"originalText\": \"interview\",\n",
            "          \"characterOffsetBegin\": 3945,\n",
            "          \"characterOffsetEnd\": 3954,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"characterOffsetBegin\": 3955,\n",
            "          \"characterOffsetEnd\": 3959,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Australia\",\n",
            "          \"originalText\": \"Australia\",\n",
            "          \"characterOffsetBegin\": 3960,\n",
            "          \"characterOffsetEnd\": 3969,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"'s\",\n",
            "          \"originalText\": \"’s\",\n",
            "          \"characterOffsetBegin\": 3969,\n",
            "          \"characterOffsetEnd\": 3971,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"60\",\n",
            "          \"originalText\": \"60\",\n",
            "          \"characterOffsetBegin\": 3972,\n",
            "          \"characterOffsetEnd\": 3974,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"Minutes\",\n",
            "          \"originalText\": \"Minutes\",\n",
            "          \"characterOffsetBegin\": 3975,\n",
            "          \"characterOffsetEnd\": 3982,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"programme\",\n",
            "          \"originalText\": \"programme\",\n",
            "          \"characterOffsetBegin\": 3983,\n",
            "          \"characterOffsetEnd\": 3992,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 3993,\n",
            "          \"characterOffsetEnd\": 3996,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"twins\",\n",
            "          \"originalText\": \"twins\",\n",
            "          \"characterOffsetBegin\": 3997,\n",
            "          \"characterOffsetEnd\": 4002,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"gave\",\n",
            "          \"originalText\": \"gave\",\n",
            "          \"characterOffsetBegin\": 4003,\n",
            "          \"characterOffsetEnd\": 4007,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 4008,\n",
            "          \"characterOffsetEnd\": 4009,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"startling\",\n",
            "          \"originalText\": \"startling\",\n",
            "          \"characterOffsetBegin\": 4010,\n",
            "          \"characterOffsetEnd\": 4019,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"insight\",\n",
            "          \"originalText\": \"insight\",\n",
            "          \"characterOffsetBegin\": 4020,\n",
            "          \"characterOffsetEnd\": 4027,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"into\",\n",
            "          \"originalText\": \"into\",\n",
            "          \"characterOffsetBegin\": 4028,\n",
            "          \"characterOffsetEnd\": 4032,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 4033,\n",
            "          \"characterOffsetEnd\": 4038,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"eating\",\n",
            "          \"originalText\": \"eating\",\n",
            "          \"characterOffsetBegin\": 4039,\n",
            "          \"characterOffsetEnd\": 4045,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"habits\",\n",
            "          \"originalText\": \"habits\",\n",
            "          \"characterOffsetBegin\": 4046,\n",
            "          \"characterOffsetEnd\": 4052,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 20,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4052,\n",
            "          \"characterOffsetEnd\": 4053,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 68,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Said\",\n",
            "          \"originalText\": \"Said\",\n",
            "          \"characterOffsetBegin\": 4055,\n",
            "          \"characterOffsetEnd\": 4059,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 4060,\n",
            "          \"characterOffsetEnd\": 4065,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 4065,\n",
            "          \"characterOffsetEnd\": 4066,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"‘\",\n",
            "          \"characterOffsetBegin\": 4067,\n",
            "          \"characterOffsetEnd\": 4068,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Essentially\",\n",
            "          \"originalText\": \"Essentially\",\n",
            "          \"characterOffsetBegin\": 4068,\n",
            "          \"characterOffsetEnd\": 4079,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 4079,\n",
            "          \"characterOffsetEnd\": 4080,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"we\",\n",
            "          \"originalText\": \"we\",\n",
            "          \"characterOffsetBegin\": 4081,\n",
            "          \"characterOffsetEnd\": 4083,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"do\",\n",
            "          \"originalText\": \"do\",\n",
            "          \"characterOffsetBegin\": 4084,\n",
            "          \"characterOffsetEnd\": 4086,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"n't\",\n",
            "          \"originalText\": \"n’t\",\n",
            "          \"characterOffsetBegin\": 4086,\n",
            "          \"characterOffsetEnd\": 4089,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"eat\",\n",
            "          \"originalText\": \"eat\",\n",
            "          \"characterOffsetBegin\": 4090,\n",
            "          \"characterOffsetEnd\": 4093,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"anything\",\n",
            "          \"originalText\": \"anything\",\n",
            "          \"characterOffsetBegin\": 4094,\n",
            "          \"characterOffsetEnd\": 4102,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4102,\n",
            "          \"characterOffsetEnd\": 4103,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 69,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"We\",\n",
            "          \"originalText\": \"We\",\n",
            "          \"characterOffsetBegin\": 4104,\n",
            "          \"characterOffsetEnd\": 4106,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"might\",\n",
            "          \"originalText\": \"might\",\n",
            "          \"characterOffsetBegin\": 4107,\n",
            "          \"characterOffsetEnd\": 4112,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"have\",\n",
            "          \"originalText\": \"have\",\n",
            "          \"characterOffsetBegin\": 4113,\n",
            "          \"characterOffsetEnd\": 4117,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"characterOffsetBegin\": 4118,\n",
            "          \"characterOffsetEnd\": 4119,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"piece\",\n",
            "          \"originalText\": \"piece\",\n",
            "          \"characterOffsetBegin\": 4120,\n",
            "          \"characterOffsetEnd\": 4125,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"of\",\n",
            "          \"originalText\": \"of\",\n",
            "          \"characterOffsetBegin\": 4126,\n",
            "          \"characterOffsetEnd\": 4128,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"watermelon\",\n",
            "          \"originalText\": \"watermelon\",\n",
            "          \"characterOffsetBegin\": 4129,\n",
            "          \"characterOffsetEnd\": 4139,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4139,\n",
            "          \"characterOffsetEnd\": 4140,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"’\",\n",
            "          \"characterOffsetBegin\": 4140,\n",
            "          \"characterOffsetEnd\": 4141,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 70,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 4143,\n",
            "          \"characterOffsetEnd\": 4149,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"added\",\n",
            "          \"originalText\": \"added\",\n",
            "          \"characterOffsetBegin\": 4150,\n",
            "          \"characterOffsetEnd\": 4155,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 4155,\n",
            "          \"characterOffsetEnd\": 4156,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"‘\",\n",
            "          \"characterOffsetBegin\": 4157,\n",
            "          \"characterOffsetEnd\": 4158,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"And\",\n",
            "          \"originalText\": \"And\",\n",
            "          \"characterOffsetBegin\": 4158,\n",
            "          \"characterOffsetEnd\": 4161,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"Diet\",\n",
            "          \"originalText\": \"Diet\",\n",
            "          \"characterOffsetBegin\": 4162,\n",
            "          \"characterOffsetEnd\": 4166,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"Coke\",\n",
            "          \"originalText\": \"Coke\",\n",
            "          \"characterOffsetBegin\": 4167,\n",
            "          \"characterOffsetEnd\": 4171,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"we\",\n",
            "          \"originalText\": \"we\",\n",
            "          \"characterOffsetBegin\": 4172,\n",
            "          \"characterOffsetEnd\": 4174,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"have\",\n",
            "          \"originalText\": \"have\",\n",
            "          \"characterOffsetBegin\": 4175,\n",
            "          \"characterOffsetEnd\": 4179,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 4179,\n",
            "          \"characterOffsetEnd\": 4180,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 4181,\n",
            "          \"characterOffsetEnd\": 4184,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"coffee\",\n",
            "          \"originalText\": \"coffee\",\n",
            "          \"characterOffsetBegin\": 4185,\n",
            "          \"characterOffsetEnd\": 4191,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4191,\n",
            "          \"characterOffsetEnd\": 4192,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"’\",\n",
            "          \"characterOffsetBegin\": 4192,\n",
            "          \"characterOffsetEnd\": 4193,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 71,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"They\",\n",
            "          \"originalText\": \"They\",\n",
            "          \"characterOffsetBegin\": 4195,\n",
            "          \"characterOffsetEnd\": 4199,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"also\",\n",
            "          \"originalText\": \"also\",\n",
            "          \"characterOffsetBegin\": 4200,\n",
            "          \"characterOffsetEnd\": 4204,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"revealed\",\n",
            "          \"originalText\": \"revealed\",\n",
            "          \"characterOffsetBegin\": 4205,\n",
            "          \"characterOffsetEnd\": 4213,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 4214,\n",
            "          \"characterOffsetEnd\": 4218,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"took\",\n",
            "          \"originalText\": \"took\",\n",
            "          \"characterOffsetBegin\": 4219,\n",
            "          \"characterOffsetEnd\": 4223,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"at\",\n",
            "          \"originalText\": \"at\",\n",
            "          \"characterOffsetBegin\": 4224,\n",
            "          \"characterOffsetEnd\": 4226,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"least\",\n",
            "          \"originalText\": \"least\",\n",
            "          \"characterOffsetBegin\": 4227,\n",
            "          \"characterOffsetEnd\": 4232,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"20\",\n",
            "          \"originalText\": \"20\",\n",
            "          \"characterOffsetBegin\": 4233,\n",
            "          \"characterOffsetEnd\": 4235,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"laxatives\",\n",
            "          \"originalText\": \"laxatives\",\n",
            "          \"characterOffsetBegin\": 4236,\n",
            "          \"characterOffsetEnd\": 4245,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4245,\n",
            "          \"characterOffsetEnd\": 4246,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 72,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 4248,\n",
            "          \"characterOffsetEnd\": 4254,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"said\",\n",
            "          \"originalText\": \"said\",\n",
            "          \"characterOffsetBegin\": 4255,\n",
            "          \"characterOffsetEnd\": 4259,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"that\",\n",
            "          \"originalText\": \"that\",\n",
            "          \"characterOffsetBegin\": 4260,\n",
            "          \"characterOffsetEnd\": 4264,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 4265,\n",
            "          \"characterOffsetEnd\": 4270,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 4271,\n",
            "          \"characterOffsetEnd\": 4274,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 4275,\n",
            "          \"characterOffsetEnd\": 4278,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"only\",\n",
            "          \"originalText\": \"only\",\n",
            "          \"characterOffsetBegin\": 4279,\n",
            "          \"characterOffsetEnd\": 4283,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"person\",\n",
            "          \"originalText\": \"person\",\n",
            "          \"characterOffsetBegin\": 4284,\n",
            "          \"characterOffsetEnd\": 4290,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"who\",\n",
            "          \"originalText\": \"who\",\n",
            "          \"characterOffsetBegin\": 4291,\n",
            "          \"characterOffsetEnd\": 4294,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"remained\",\n",
            "          \"originalText\": \"remained\",\n",
            "          \"characterOffsetBegin\": 4295,\n",
            "          \"characterOffsetEnd\": 4303,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"by\",\n",
            "          \"originalText\": \"by\",\n",
            "          \"characterOffsetBegin\": 4304,\n",
            "          \"characterOffsetEnd\": 4306,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"her\",\n",
            "          \"originalText\": \"her\",\n",
            "          \"characterOffsetBegin\": 4307,\n",
            "          \"characterOffsetEnd\": 4310,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"side\",\n",
            "          \"originalText\": \"side\",\n",
            "          \"characterOffsetBegin\": 4311,\n",
            "          \"characterOffsetEnd\": 4315,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4315,\n",
            "          \"characterOffsetEnd\": 4316,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 73,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"‘\",\n",
            "          \"characterOffsetBegin\": 4317,\n",
            "          \"characterOffsetEnd\": 4318,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"And\",\n",
            "          \"originalText\": \"And\",\n",
            "          \"characterOffsetBegin\": 4318,\n",
            "          \"characterOffsetEnd\": 4321,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"at\",\n",
            "          \"originalText\": \"at\",\n",
            "          \"characterOffsetBegin\": 4322,\n",
            "          \"characterOffsetEnd\": 4324,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"least\",\n",
            "          \"originalText\": \"least\",\n",
            "          \"characterOffsetBegin\": 4325,\n",
            "          \"characterOffsetEnd\": 4330,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"we\",\n",
            "          \"originalText\": \"we\",\n",
            "          \"characterOffsetBegin\": 4331,\n",
            "          \"characterOffsetEnd\": 4333,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"'ll\",\n",
            "          \"originalText\": \"’ll\",\n",
            "          \"characterOffsetBegin\": 4333,\n",
            "          \"characterOffsetEnd\": 4336,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"die\",\n",
            "          \"originalText\": \"die\",\n",
            "          \"characterOffsetBegin\": 4337,\n",
            "          \"characterOffsetEnd\": 4340,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"together\",\n",
            "          \"originalText\": \"together\",\n",
            "          \"characterOffsetBegin\": 4341,\n",
            "          \"characterOffsetEnd\": 4349,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4349,\n",
            "          \"characterOffsetEnd\": 4350,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"’\",\n",
            "          \"characterOffsetBegin\": 4350,\n",
            "          \"characterOffsetEnd\": 4351,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 74,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 4353,\n",
            "          \"characterOffsetEnd\": 4358,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"said\",\n",
            "          \"originalText\": \"said\",\n",
            "          \"characterOffsetBegin\": 4359,\n",
            "          \"characterOffsetEnd\": 4363,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \":\",\n",
            "          \"originalText\": \":\",\n",
            "          \"characterOffsetBegin\": 4363,\n",
            "          \"characterOffsetEnd\": 4364,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"‘\",\n",
            "          \"characterOffsetBegin\": 4365,\n",
            "          \"characterOffsetEnd\": 4366,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"Being\",\n",
            "          \"originalText\": \"Being\",\n",
            "          \"characterOffsetBegin\": 4366,\n",
            "          \"characterOffsetEnd\": 4371,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"characterOffsetBegin\": 4372,\n",
            "          \"characterOffsetEnd\": 4376,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 4377,\n",
            "          \"characterOffsetEnd\": 4383,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"...\",\n",
            "          \"originalText\": \"…\",\n",
            "          \"characterOffsetBegin\": 4383,\n",
            "          \"characterOffsetEnd\": 4384,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"makes\",\n",
            "          \"originalText\": \"makes\",\n",
            "          \"characterOffsetBegin\": 4384,\n",
            "          \"characterOffsetEnd\": 4389,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"it\",\n",
            "          \"originalText\": \"it\",\n",
            "          \"characterOffsetBegin\": 4390,\n",
            "          \"characterOffsetEnd\": 4392,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"somewhat\",\n",
            "          \"originalText\": \"somewhat\",\n",
            "          \"characterOffsetBegin\": 4393,\n",
            "          \"characterOffsetEnd\": 4401,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"easier\",\n",
            "          \"originalText\": \"easier\",\n",
            "          \"characterOffsetBegin\": 4402,\n",
            "          \"characterOffsetEnd\": 4408,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 4409,\n",
            "          \"characterOffsetEnd\": 4411,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"die\",\n",
            "          \"originalText\": \"die\",\n",
            "          \"characterOffsetBegin\": 4412,\n",
            "          \"characterOffsetEnd\": 4415,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"characterOffsetBegin\": 4415,\n",
            "          \"characterOffsetEnd\": 4416,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"’\",\n",
            "          \"characterOffsetBegin\": 4416,\n",
            "          \"characterOffsetEnd\": 4417,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 75,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"@highlight\",\n",
            "          \"originalText\": \"@highlight\",\n",
            "          \"characterOffsetBegin\": 4419,\n",
            "          \"characterOffsetEnd\": 4429,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 76,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Clare\",\n",
            "          \"originalText\": \"Clare\",\n",
            "          \"characterOffsetBegin\": 4431,\n",
            "          \"characterOffsetEnd\": 4436,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 4437,\n",
            "          \"characterOffsetEnd\": 4440,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"Rachel\",\n",
            "          \"originalText\": \"Rachel\",\n",
            "          \"characterOffsetBegin\": 4441,\n",
            "          \"characterOffsetEnd\": 4447,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"Wallmeyer\",\n",
            "          \"originalText\": \"Wallmeyer\",\n",
            "          \"characterOffsetBegin\": 4448,\n",
            "          \"characterOffsetEnd\": 4457,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 4457,\n",
            "          \"characterOffsetEnd\": 4458,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"42\",\n",
            "          \"originalText\": \"42\",\n",
            "          \"characterOffsetBegin\": 4459,\n",
            "          \"characterOffsetEnd\": 4461,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 4461,\n",
            "          \"characterOffsetEnd\": 4462,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"died\",\n",
            "          \"originalText\": \"died\",\n",
            "          \"characterOffsetBegin\": 4463,\n",
            "          \"characterOffsetEnd\": 4467,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"when\",\n",
            "          \"originalText\": \"when\",\n",
            "          \"characterOffsetBegin\": 4468,\n",
            "          \"characterOffsetEnd\": 4472,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"fire\",\n",
            "          \"originalText\": \"fire\",\n",
            "          \"characterOffsetBegin\": 4473,\n",
            "          \"characterOffsetEnd\": 4477,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"swept\",\n",
            "          \"originalText\": \"swept\",\n",
            "          \"characterOffsetBegin\": 4478,\n",
            "          \"characterOffsetEnd\": 4483,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"through\",\n",
            "          \"originalText\": \"through\",\n",
            "          \"characterOffsetBegin\": 4484,\n",
            "          \"characterOffsetEnd\": 4491,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 4492,\n",
            "          \"characterOffsetEnd\": 4497,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"home\",\n",
            "          \"originalText\": \"home\",\n",
            "          \"characterOffsetBegin\": 4498,\n",
            "          \"characterOffsetEnd\": 4502,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 4503,\n",
            "          \"characterOffsetEnd\": 4505,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"Geelong\",\n",
            "          \"originalText\": \"Geelong\",\n",
            "          \"characterOffsetBegin\": 4506,\n",
            "          \"characterOffsetEnd\": 4513,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \",\",\n",
            "          \"originalText\": \",\",\n",
            "          \"characterOffsetBegin\": 4513,\n",
            "          \"characterOffsetEnd\": 4514,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"near\",\n",
            "          \"originalText\": \"near\",\n",
            "          \"characterOffsetBegin\": 4515,\n",
            "          \"characterOffsetEnd\": 4519,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 19,\n",
            "          \"word\": \"Melbourne\",\n",
            "          \"originalText\": \"Melbourne\",\n",
            "          \"characterOffsetBegin\": 4520,\n",
            "          \"characterOffsetEnd\": 4529,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 77,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"@highlight\",\n",
            "          \"originalText\": \"@highlight\",\n",
            "          \"characterOffsetBegin\": 4531,\n",
            "          \"characterOffsetEnd\": 4541,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 78,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"One\",\n",
            "          \"originalText\": \"One\",\n",
            "          \"characterOffsetBegin\": 4543,\n",
            "          \"characterOffsetEnd\": 4546,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"died\",\n",
            "          \"originalText\": \"died\",\n",
            "          \"characterOffsetBegin\": 4547,\n",
            "          \"characterOffsetEnd\": 4551,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"in\",\n",
            "          \"originalText\": \"in\",\n",
            "          \"characterOffsetBegin\": 4552,\n",
            "          \"characterOffsetEnd\": 4554,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 4555,\n",
            "          \"characterOffsetEnd\": 4558,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"flames\",\n",
            "          \"originalText\": \"flames\",\n",
            "          \"characterOffsetBegin\": 4559,\n",
            "          \"characterOffsetEnd\": 4565,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"while\",\n",
            "          \"originalText\": \"while\",\n",
            "          \"characterOffsetBegin\": 4566,\n",
            "          \"characterOffsetEnd\": 4571,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 4572,\n",
            "          \"characterOffsetEnd\": 4575,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"other\",\n",
            "          \"originalText\": \"other\",\n",
            "          \"characterOffsetBegin\": 4576,\n",
            "          \"characterOffsetEnd\": 4581,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"succumbed\",\n",
            "          \"originalText\": \"succumbed\",\n",
            "          \"characterOffsetBegin\": 4582,\n",
            "          \"characterOffsetEnd\": 4591,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 4592,\n",
            "          \"characterOffsetEnd\": 4594,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"severe\",\n",
            "          \"originalText\": \"severe\",\n",
            "          \"characterOffsetBegin\": 4595,\n",
            "          \"characterOffsetEnd\": 4601,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"burns\",\n",
            "          \"originalText\": \"burns\",\n",
            "          \"characterOffsetBegin\": 4602,\n",
            "          \"characterOffsetEnd\": 4607,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"on\",\n",
            "          \"originalText\": \"on\",\n",
            "          \"characterOffsetBegin\": 4608,\n",
            "          \"characterOffsetEnd\": 4610,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"the\",\n",
            "          \"originalText\": \"the\",\n",
            "          \"characterOffsetBegin\": 4611,\n",
            "          \"characterOffsetEnd\": 4614,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"way\",\n",
            "          \"originalText\": \"way\",\n",
            "          \"characterOffsetBegin\": 4615,\n",
            "          \"characterOffsetEnd\": 4618,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 4619,\n",
            "          \"characterOffsetEnd\": 4621,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"hospital\",\n",
            "          \"originalText\": \"hospital\",\n",
            "          \"characterOffsetBegin\": 4622,\n",
            "          \"characterOffsetEnd\": 4630,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 79,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"@highlight\",\n",
            "          \"originalText\": \"@highlight\",\n",
            "          \"characterOffsetBegin\": 4632,\n",
            "          \"characterOffsetEnd\": 4642,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 80,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Initial\",\n",
            "          \"originalText\": \"Initial\",\n",
            "          \"characterOffsetBegin\": 4644,\n",
            "          \"characterOffsetEnd\": 4651,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"investigation\",\n",
            "          \"originalText\": \"investigation\",\n",
            "          \"characterOffsetBegin\": 4652,\n",
            "          \"characterOffsetEnd\": 4665,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"suggests\",\n",
            "          \"originalText\": \"suggests\",\n",
            "          \"characterOffsetBegin\": 4666,\n",
            "          \"characterOffsetEnd\": 4674,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"fire\",\n",
            "          \"originalText\": \"fire\",\n",
            "          \"characterOffsetBegin\": 4675,\n",
            "          \"characterOffsetEnd\": 4679,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"was\",\n",
            "          \"originalText\": \"was\",\n",
            "          \"characterOffsetBegin\": 4680,\n",
            "          \"characterOffsetEnd\": 4683,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"not\",\n",
            "          \"originalText\": \"not\",\n",
            "          \"characterOffsetBegin\": 4684,\n",
            "          \"characterOffsetEnd\": 4687,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"suspicious\",\n",
            "          \"originalText\": \"suspicious\",\n",
            "          \"characterOffsetBegin\": 4688,\n",
            "          \"characterOffsetEnd\": 4698,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"but\",\n",
            "          \"originalText\": \"but\",\n",
            "          \"characterOffsetBegin\": 4699,\n",
            "          \"characterOffsetEnd\": 4702,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"twins\",\n",
            "          \"originalText\": \"twins\",\n",
            "          \"characterOffsetBegin\": 4703,\n",
            "          \"characterOffsetEnd\": 4708,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"had\",\n",
            "          \"originalText\": \"had\",\n",
            "          \"characterOffsetBegin\": 4709,\n",
            "          \"characterOffsetEnd\": 4712,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"tried\",\n",
            "          \"originalText\": \"tried\",\n",
            "          \"characterOffsetBegin\": 4713,\n",
            "          \"characterOffsetEnd\": 4718,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 4719,\n",
            "          \"characterOffsetEnd\": 4721,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"kill\",\n",
            "          \"originalText\": \"kill\",\n",
            "          \"characterOffsetBegin\": 4722,\n",
            "          \"characterOffsetEnd\": 4726,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"one\",\n",
            "          \"originalText\": \"one\",\n",
            "          \"characterOffsetBegin\": 4727,\n",
            "          \"characterOffsetEnd\": 4730,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"another\",\n",
            "          \"originalText\": \"another\",\n",
            "          \"characterOffsetBegin\": 4731,\n",
            "          \"characterOffsetEnd\": 4738,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"before\",\n",
            "          \"originalText\": \"before\",\n",
            "          \"characterOffsetBegin\": 4739,\n",
            "          \"characterOffsetEnd\": 4745,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 81,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"@highlight\",\n",
            "          \"originalText\": \"@highlight\",\n",
            "          \"characterOffsetBegin\": 4747,\n",
            "          \"characterOffsetEnd\": 4757,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 82,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Pair\",\n",
            "          \"originalText\": \"Pair\",\n",
            "          \"characterOffsetBegin\": 4759,\n",
            "          \"characterOffsetEnd\": 4763,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"were\",\n",
            "          \"originalText\": \"were\",\n",
            "          \"characterOffsetBegin\": 4764,\n",
            "          \"characterOffsetEnd\": 4768,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"addicted\",\n",
            "          \"originalText\": \"addicted\",\n",
            "          \"characterOffsetBegin\": 4769,\n",
            "          \"characterOffsetEnd\": 4777,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 4778,\n",
            "          \"characterOffsetEnd\": 4780,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"long\",\n",
            "          \"originalText\": \"long\",\n",
            "          \"characterOffsetBegin\": 4781,\n",
            "          \"characterOffsetEnd\": 4785,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"distance\",\n",
            "          \"originalText\": \"distance\",\n",
            "          \"characterOffsetBegin\": 4786,\n",
            "          \"characterOffsetEnd\": 4794,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"running\",\n",
            "          \"originalText\": \"running\",\n",
            "          \"characterOffsetBegin\": 4795,\n",
            "          \"characterOffsetEnd\": 4802,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"and\",\n",
            "          \"originalText\": \"and\",\n",
            "          \"characterOffsetBegin\": 4803,\n",
            "          \"characterOffsetEnd\": 4806,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"fractured\",\n",
            "          \"originalText\": \"fractured\",\n",
            "          \"characterOffsetBegin\": 4807,\n",
            "          \"characterOffsetEnd\": 4816,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"their\",\n",
            "          \"originalText\": \"their\",\n",
            "          \"characterOffsetBegin\": 4817,\n",
            "          \"characterOffsetEnd\": 4822,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"feet\",\n",
            "          \"originalText\": \"feet\",\n",
            "          \"characterOffsetBegin\": 4823,\n",
            "          \"characterOffsetEnd\": 4827,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"endlessly\",\n",
            "          \"originalText\": \"endlessly\",\n",
            "          \"characterOffsetBegin\": 4828,\n",
            "          \"characterOffsetEnd\": 4837,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"attempting\",\n",
            "          \"originalText\": \"attempting\",\n",
            "          \"characterOffsetBegin\": 4838,\n",
            "          \"characterOffsetEnd\": 4848,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"marathons\",\n",
            "          \"originalText\": \"marathons\",\n",
            "          \"characterOffsetBegin\": 4849,\n",
            "          \"characterOffsetEnd\": 4858,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 83,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"@highlight\",\n",
            "          \"originalText\": \"@highlight\",\n",
            "          \"characterOffsetBegin\": 4860,\n",
            "          \"characterOffsetEnd\": 4870,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 84,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Authorities\",\n",
            "          \"originalText\": \"Authorities\",\n",
            "          \"characterOffsetBegin\": 4872,\n",
            "          \"characterOffsetEnd\": 4883,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"considered\",\n",
            "          \"originalText\": \"considered\",\n",
            "          \"characterOffsetBegin\": 4884,\n",
            "          \"characterOffsetEnd\": 4894,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"locking\",\n",
            "          \"originalText\": \"locking\",\n",
            "          \"characterOffsetBegin\": 4895,\n",
            "          \"characterOffsetEnd\": 4902,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"sisters\",\n",
            "          \"originalText\": \"sisters\",\n",
            "          \"characterOffsetBegin\": 4903,\n",
            "          \"characterOffsetEnd\": 4910,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"up\",\n",
            "          \"originalText\": \"up\",\n",
            "          \"characterOffsetBegin\": 4911,\n",
            "          \"characterOffsetEnd\": 4913,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"to\",\n",
            "          \"originalText\": \"to\",\n",
            "          \"characterOffsetBegin\": 4914,\n",
            "          \"characterOffsetEnd\": 4916,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"stop\",\n",
            "          \"originalText\": \"stop\",\n",
            "          \"characterOffsetBegin\": 4917,\n",
            "          \"characterOffsetEnd\": 4921,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"them\",\n",
            "          \"originalText\": \"them\",\n",
            "          \"characterOffsetBegin\": 4922,\n",
            "          \"characterOffsetEnd\": 4926,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"starving\",\n",
            "          \"originalText\": \"starving\",\n",
            "          \"characterOffsetBegin\": 4927,\n",
            "          \"characterOffsetEnd\": 4935,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"themselves\",\n",
            "          \"originalText\": \"themselves\",\n",
            "          \"characterOffsetBegin\": 4936,\n",
            "          \"characterOffsetEnd\": 4946,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"as\",\n",
            "          \"originalText\": \"as\",\n",
            "          \"characterOffsetBegin\": 4947,\n",
            "          \"characterOffsetEnd\": 4949,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"each\",\n",
            "          \"originalText\": \"each\",\n",
            "          \"characterOffsetBegin\": 4950,\n",
            "          \"characterOffsetEnd\": 4954,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"weighed\",\n",
            "          \"originalText\": \"weighed\",\n",
            "          \"characterOffsetBegin\": 4955,\n",
            "          \"characterOffsetEnd\": 4962,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"little\",\n",
            "          \"originalText\": \"little\",\n",
            "          \"characterOffsetBegin\": 4963,\n",
            "          \"characterOffsetEnd\": 4969,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 15,\n",
            "          \"word\": \"more\",\n",
            "          \"originalText\": \"more\",\n",
            "          \"characterOffsetBegin\": 4970,\n",
            "          \"characterOffsetEnd\": 4974,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 16,\n",
            "          \"word\": \"than\",\n",
            "          \"originalText\": \"than\",\n",
            "          \"characterOffsetBegin\": 4975,\n",
            "          \"characterOffsetEnd\": 4979,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 17,\n",
            "          \"word\": \"four\",\n",
            "          \"originalText\": \"four\",\n",
            "          \"characterOffsetBegin\": 4980,\n",
            "          \"characterOffsetEnd\": 4984,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 18,\n",
            "          \"word\": \"stone\",\n",
            "          \"originalText\": \"stone\",\n",
            "          \"characterOffsetBegin\": 4985,\n",
            "          \"characterOffsetEnd\": 4990,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 85,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"@highlight\",\n",
            "          \"originalText\": \"@highlight\",\n",
            "          \"characterOffsetBegin\": 4993,\n",
            "          \"characterOffsetEnd\": 5003,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \"\\n\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 86,\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Twins\",\n",
            "          \"originalText\": \"Twins\",\n",
            "          \"characterOffsetBegin\": 5005,\n",
            "          \"characterOffsetEnd\": 5010,\n",
            "          \"before\": \"\\n\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"predicted\",\n",
            "          \"originalText\": \"predicted\",\n",
            "          \"characterOffsetBegin\": 5011,\n",
            "          \"characterOffsetEnd\": 5020,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 5021,\n",
            "          \"characterOffsetEnd\": 5025,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"would\",\n",
            "          \"originalText\": \"would\",\n",
            "          \"characterOffsetBegin\": 5026,\n",
            "          \"characterOffsetEnd\": 5031,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"`\",\n",
            "          \"originalText\": \"'\",\n",
            "          \"characterOffsetBegin\": 5032,\n",
            "          \"characterOffsetEnd\": 5033,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \"die\",\n",
            "          \"originalText\": \"die\",\n",
            "          \"characterOffsetBegin\": 5033,\n",
            "          \"characterOffsetEnd\": 5036,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 7,\n",
            "          \"word\": \"together\",\n",
            "          \"originalText\": \"together\",\n",
            "          \"characterOffsetBegin\": 5037,\n",
            "          \"characterOffsetEnd\": 5045,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 8,\n",
            "          \"word\": \"'\",\n",
            "          \"originalText\": \"'\",\n",
            "          \"characterOffsetBegin\": 5045,\n",
            "          \"characterOffsetEnd\": 5046,\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 9,\n",
            "          \"word\": \"as\",\n",
            "          \"originalText\": \"as\",\n",
            "          \"characterOffsetBegin\": 5047,\n",
            "          \"characterOffsetEnd\": 5049,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 10,\n",
            "          \"word\": \"they\",\n",
            "          \"originalText\": \"they\",\n",
            "          \"characterOffsetBegin\": 5050,\n",
            "          \"characterOffsetEnd\": 5054,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 11,\n",
            "          \"word\": \"transformed\",\n",
            "          \"originalText\": \"transformed\",\n",
            "          \"characterOffsetBegin\": 5055,\n",
            "          \"characterOffsetEnd\": 5066,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 12,\n",
            "          \"word\": \"into\",\n",
            "          \"originalText\": \"into\",\n",
            "          \"characterOffsetBegin\": 5067,\n",
            "          \"characterOffsetEnd\": 5071,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 13,\n",
            "          \"word\": \"living\",\n",
            "          \"originalText\": \"living\",\n",
            "          \"characterOffsetBegin\": 5072,\n",
            "          \"characterOffsetEnd\": 5078,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 14,\n",
            "          \"word\": \"skeletons\",\n",
            "          \"originalText\": \"skeletons\",\n",
            "          \"characterOffsetBegin\": 5079,\n",
            "          \"characterOffsetEnd\": 5088,\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhYTpVfJ4jxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rm /home/stanford-corenlp-full-2018-10-05.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOyKIofabGU9",
        "colab_type": "code",
        "outputId": "c6008358-1a47-4bc9-aed5-542bb11a77e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls -ltr {TOKENIZED_PATH}| tail -2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 171144 May 15 02:01 5b76984e7d6c3ffb9e10b0fe9b356e90beac4c0b.story.json\n",
            "-rw-r--r-- 1 root root 219086 May 15 02:01 0f848ac3922e8deaf6a803b2713ed867bc1d19e3.story.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l5IPSOXSpvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mkdir {TOKENIZED_PATH}sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEwJtOuQSjSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cp {TOKENIZED_PATH}1faf623126af4beed4ea40802bcbbdfb7922d2f1.story.json {TOKENIZED_PATH}sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZxw-hd9TGn6",
        "colab_type": "code",
        "outputId": "c97cc376-a3fd-4b52-eb49-0bc872e9a9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#ls {TOKENIZED_PATH}sample/*.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/merged_stories_tokenized/sample/1faf623126af4beed4ea40802bcbbdfb7922d2f1.story.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NopKdSVHVqk",
        "colab_type": "code",
        "outputId": "b642d5e0-e3f0-439d-deba-7425f37e30c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls {MAP_PATH}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;32mcnn_mapping_test.txt\u001b[0m*   \u001b[01;32mcnn_mapping_valid.txt\u001b[0m*  \u001b[01;32mmapping_train.txt\u001b[0m*\n",
            "\u001b[01;32mcnn_mapping_train.txt\u001b[0m*  \u001b[01;32mmapping_test.txt\u001b[0m*       \u001b[01;32mmapping_valid.txt\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f83uLaLjR-Kq",
        "colab_type": "code",
        "outputId": "b37ad1ac-9f03-4a3a-cbb6-4dae81c71ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!tail {MAP_PATH}mapping_valid.txt*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://web.archive.org/web/20150804065113id_/http://www.dailymail.co.uk/femail/article-3011908/Pippa-Middleton-dressed-business-sports-geek-chic-glasses-smart-bloke-coat-London.html\n",
            "http://web.archive.org/web/20150804065121id_/http://www.dailymail.co.uk/travel/travel_news/article-2986167/Sheer-rock-walls-pitch-darkness-twice-height-Statue-Liberty-Intrepid-climbers-capture-amazing-photos-descent-America-s-deepest-pit.html\n",
            "http://web.archive.org/web/20150804065210id_/http://www.dailymail.co.uk/travel/travel_news/article-3009681/Replica-Chauvet-Pont-d-Arc-Cave-open-France.html\n",
            "http://web.archive.org/web/20150804065215id_/http://www.dailymail.co.uk/health/article-3009645/The-moment-world-turns-grey-shades-rainbow-Moving-video-shows-colour-blind-people-seeing-normally-time.html\n",
            "http://web.archive.org/web/20150804065443id_/http://www.dailymail.co.uk/sport/football/article-3010366/Players-Swedish-club-Dalkurd-FF-escaped-death-doomed-Dusseldorf-jet.html\n",
            "http://web.archive.org/web/20150804065710id_/http://www.dailymail.co.uk/travel/travel_news/article-3006473/If-fancy-bargain-head-Shenyang-night-market-China-largest-kind-Asia-stretching-MILE-long.html\n",
            "http://web.archive.org/web/20150804065904id_/http://www.dailymail.co.uk/travel/travel_news/article-3002131/Justin-Ross-Lee-shares-outrageous-travel-industry-hacks-feature-video.html\n",
            "http://web.archive.org/web/20150804065954id_/http://www.dailymail.co.uk/travel/travel_news/article-2995606/Springbok-comes-nose-nose-crocodile-Kruger-National-Park-South-Africa.html\n",
            "http://web.archive.org/web/20150804070348id_/http://www.dailymail.co.uk/travel/travel_news/article-3002143/Pigs-swim-boat-beg-food-tourists-Pig-Island-Bahamas.html\n",
            "http://web.archive.org/web/20150804072724id_/http://www.dailymail.co.uk/news/article-3013090/Teacher-25-facing-jail-sex-high-school-wrestler-school-bus-bragged-encounter-friends.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsWGBkjbSNYE",
        "colab_type": "code",
        "outputId": "4f6cff98-70eb-43eb-8919-719c71ea879a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls {TOKENIZED_PATH} | tail -2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f5b6b1729d6088f07e7c43afbea9bc685ae054f1.story.json\n",
            "ff0345888fafc20f762f3a74fac915a137b6a942.story.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ9kttC_zLBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /home/BertSum/src/preprocess.py -mode format_to_lines -raw_path {TOKENIZED_PATH} -save_path {SIMPLER_JSON_PATH} -map_path {MAP_PATH} -lower -corenlp_path {CORENLP_PATH}# &>/dev/null\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAT8L9lSP2Yv",
        "colab_type": "code",
        "outputId": "7c709337-0e59-434f-c4f5-a91bc50540a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/json_data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcnndm\u001b[0m/  cnndm.train.0.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbD_NfQkUdxh",
        "colab_type": "code",
        "outputId": "2d61f775-2780-462d-a1c3-7b581d42934e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cat /home/json_data/cnndm.train.0.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{\"src\": [[\"(\", \"cnn\", \")\", \"the\", \"pakistan\", \"taliban\", \"claimed\", \"responsibility\", \"for\", \"a\", \"friday\", \"attack\", \"on\", \"a\", \"shiite\", \"muslim\", \"mosque\", \"in\", \"peshawar\", \"in\", \"northwestern\", \"pakistan\", \"--\", \"a\", \"suicide\", \"bombing\", \"and\", \"gunfire\", \"assault\", \"that\", \"a\", \"hospital\", \"representative\", \"said\", \"killed\", \"19\", \"people\", \".\"], [\"the\", \"islamist\", \"militant\", \"group\", \"said\", \"the\", \"attack\", \"was\", \"orchestrated\", \"by\", \"a\", \"commander\", \"who\", \"was\", \"behind\", \"december\", \"'s\", \"massacre\", \"of\", \"145\", \"people\", \",\", \"including\", \"132\", \"children\", \",\", \"at\", \"a\", \"peshawar\", \"school\", \".\"], [\"sixty-seven\", \"people\", \"were\", \"injured\", \"friday\", \",\", \"said\", \"tauheed\", \"zulfiqar\", \",\", \"a\", \"representative\", \"of\", \"the\", \"hayatabad\", \"medical\", \"complex\", \"in\", \"peshawar\", \".\"], [\"the\", \"pakistan\", \"taliban\", \"attacked\", \"the\", \"mosque\", \",\", \"spokesman\", \"muhammad\", \"khurasan\", \"said\", \"in\", \"an\", \"email\", \"to\", \"cnn\", \",\", \"as\", \"revenge\", \"for\", \"the\", \"government\", \"'s\", \"december\", \"19\", \"execution\", \"of\", \"a\", \"militant\", \"who\", \"was\", \"allied\", \"with\", \"the\", \"group\", \".\"], [\"up\", \"to\", \"five\", \"attackers\", \"executed\", \"the\", \"assault\", \",\", \"including\", \"a\", \"suicide\", \"bomber\", \"and\", \"someone\", \"who\", \"was\", \"shooting\", \"in\", \"the\", \"mosque\", \",\", \"nasir\", \"khan\", \"durrani\", \",\", \"the\", \"city\", \"'s\", \"police\", \"inspector\", \"general\", \",\", \"told\", \"reporters\", \".\"], [\"one\", \"would-be\", \"suicide\", \"bomber\", \"was\", \"stopped\", \"by\", \"people\", \"in\", \"the\", \"mosque\", \"who\", \"held\", \"him\", \"by\", \"the\", \"throat\", \",\", \"durrani\", \"said\", \".\"], [\"pakistan\", \"has\", \"seen\", \"plenty\", \"of\", \"violence\", \",\", \"much\", \"of\", \"it\", \"involving\", \"militants\", \"targeting\", \"restive\", \"regions\", \"in\", \"northwest\", \"pakistan\", \"along\", \"the\", \"border\", \"with\", \"afghanistan\", \".\"], [\"it\", \"is\", \"the\", \"home\", \"base\", \"of\", \"the\", \"pakistan\", \"taliban\", \",\", \"known\", \"as\", \"the\", \"tehrik-e-taliban\", \"pakistan\", \",\", \"or\", \"ttp\", \",\", \"which\", \"seeks\", \"to\", \"enforce\", \"its\", \"conservative\", \"version\", \"of\", \"islam\", \"in\", \"that\", \"nation\", \".\"], [\"the\", \"group\", \"has\", \"battled\", \"pakistani\", \"troops\", \"and\", \"attacked\", \"civilians\", \",\", \"including\", \"in\", \"peshawar\", \",\", \"an\", \"ancient\", \"city\", \"of\", \"more\", \"than\", \"3\", \"million\", \"people\", \".\"], [\"khurasan\", \",\", \"the\", \"ttp\", \"spokesman\", \",\", \"said\", \"the\", \"attack\", \"was\", \"orchestrated\", \"by\", \"ttp\", \"commander\", \"kalifa\", \"omar\", \"mansoor\", \".\"], [\"the\", \"militant\", \"group\", \"and\", \"the\", \"pakistani\", \"army\", \"said\", \"previously\", \"that\", \"mansoor\", \"commanded\", \"the\", \"december\", \"16\", \"massacre\", \"at\", \"the\", \"army\", \"public\", \"school\", \"and\", \"degree\", \"college\", \",\", \"which\", \"largely\", \"teaches\", \"sons\", \"and\", \"daughters\", \"of\", \"army\", \"personnel\", \"from\", \"around\", \"peshawar\", \".\"], [\"in\", \"the\", \"school\", \"attack\", \",\", \"gunmen\", \"burst\", \"in\", \"and\", \"gunned\", \"down\", \"children\", \"and\", \"staff\", \",\", \"including\", \"in\", \"an\", \"auditorium\", \"filled\", \"with\", \"students\", \"taking\", \"exams\", \".\"], [\"khurasan\", \"said\", \"friday\", \"'s\", \"mosque\", \"attack\", \"was\", \"revenge\", \"for\", \"the\", \"government\", \"'s\", \"december\", \"19\", \"execution\", \"of\", \"mohammed\", \"aqeel\", \",\", \"a\", \"man\", \"condemned\", \"in\", \"part\", \"for\", \"his\", \"role\", \"in\", \"an\", \"attack\", \"on\", \"an\", \"army\", \"headquarters\", \"in\", \"2009\", \".\"], [\"aqeel\", \"was\", \"a\", \"member\", \"of\", \"a\", \"militant\", \"group\", \"allied\", \"with\", \"the\", \"ttp\", \".\"], [\"cnn\", \"'s\", \"jason\", \"hanna\", \",\", \"faith\", \"karimi\", \"and\", \"greg\", \"botelho\", \"contributed\", \"to\", \"this\", \"report\", \".\"]], \"tgt\": [[\"gunmen\", \"and\", \"a\", \"suicide\", \"bomber\", \"attack\", \"a\", \"mosque\", \"in\", \"peshawar\", \",\", \"police\", \"say\"], [\"attack\", \"is\", \"revenge\", \"for\", \"the\", \"government\", \"'s\", \"december\", \"execution\", \"of\", \"a\", \"militant\", \",\", \"pakistan\", \"taliban\", \"say\"], [\"the\", \"group\", \"says\", \"the\", \"attack\", \"'s\", \"orchestrator\", \"is\", \"a\", \"commander\", \"who\", \"it\", \"previously\", \"said\", \"planned\", \"a\", \"december\", \"school\", \"massacre\"]]}, {\"src\": [[\"(\", \"cnn\", \")\", \"--\", \"an\", \"ohio\", \"driver\", \"recently\", \"made\", \"a\", \"confession\", \"that\", \"he\", \"caused\", \"a\", \"fatal\", \"wrong-way\", \"crash\", \"after\", \"drinking\", \"heavily\", \".\"], [\"that\", \",\", \"by\", \"itself\", \",\", \"nothing\", \"new\", \".\"], [\"after\", \"all\", \",\", \"every\", \"day\", \",\", \"hundreds\", \"of\", \"suspects\", \"sign\", \"full\", \"confessions\", \",\", \"and\", \"many\", \"more\", \"defendants\", \"plead\", \"guilty\", \"before\", \"a\", \"judge\", \".\"], [\"a\", \"person\", \"volunteering\", \"to\", \"take\", \"criminal\", \"responsibility\", \"is\", \"not\", \"a\", \"novel\", \"concept\", \".\"], [\"in\", \"fact\", \",\", \"it\", \"'s\", \"commonplace\", \"procedure\", \"in\", \"police\", \"interview\", \"rooms\", \"and\", \"courthouses\", \".\"], [\"nor\", \"is\", \"it\", \"unique\", \"that\", \"these\", \"admissions\", \"are\", \"videotaped\", \";\", \"police\", \"often\", \"tape\", \"interview\", \"confessions\", \"and\", \"courtrooms\", \"record\", \"most\", \"guilty\", \"pleas\", \".\"], [\"what\", \"'s\", \"novel\", \"in\", \"this\", \"case\", \"is\", \"the\", \"way\", \"he\", \"took\", \"that\", \"responsibility\", \":\", \"1\", \")\", \"on\", \"his\", \"own\", \".\"], [\"2\", \")\", \"online\", \".\"], [\"via\", \"video\", \".\"], [\"he\", \"took\", \"the\", \"initiative\", \".\"], [\"in\", \"a\", \"remarkably\", \"well-produced\", \",\", \"three-and-a-half\", \"minute\", \"video\", \",\", \"matthew\", \"cordle\", \"admitted\", \"he\", \"killed\", \"a\", \"suburban\", \"columbus\", \"man\", \",\", \"said\", \"he\", \"\\\"\", \"made\", \"a\", \"mistake\", \"\\\"\", \"in\", \"deciding\", \"to\", \"drive\", \"that\", \"evening\", \"and\", \"urged\", \"people\", \"not\", \"to\", \"drink\", \"and\", \"drive\", \".\"], [\"in\", \"making\", \"this\", \"video\", \",\", \"it\", \"clearly\", \"appeared\", \"he\", \"was\", \"not\", \"coerced\", \".\"], [\"he\", \"was\", \"not\", \"being\", \"interrogated\", \"or\", \"even\", \"interviewed\", \"by\", \"police\", \".\"], [\"in\", \"fact\", \",\", \"he\", \"volunteered\", \"this\", \"admission\", \",\", \"stating\", \"that\", \"he\", \"was\", \"fully\", \"aware\", \"of\", \"the\", \"consequences\", \".\"], [\"skeptics\", \"have\", \"suggested\", \"the\", \"online\", \"confession\", \"is\", \"not\", \"sincere\", \"and\", \"he\", \"only\", \"produced\", \"the\", \"video\", \"to\", \"curry\", \"favor\", \"with\", \"the\", \"court\", \"to\", \"receive\", \"a\", \"lesser\", \"sentence\", \".\"], [\"quite\", \"the\", \"contrary\", \",\", \"cordle\", \"has\", \"risked\", \"sealing\", \"his\", \"fate\", \"with\", \"a\", \"maximum\", \"sentence\", \"by\", \"giving\", \"this\", \"confession\", \".\"], [\"many\", \"others\", \"have\", \"applauded\", \"this\", \"act\", \"by\", \"cordle\", \".\"], [\"undoubtedly\", \"there\", \"is\", \"a\", \"benefit\", \"to\", \"society\", \"in\", \"taking\", \"responsibility\", \".\"], [\"he\", \"not\", \"only\", \"gives\", \"the\", \"victim\", \"'s\", \"family\", \"closure\", \",\", \"but\", \"he\", \"also\", \"saves\", \"the\", \"people\", \"of\", \"the\", \"state\", \"of\", \"ohio\", \"the\", \"cost\", \"and\", \"burden\", \"of\", \"a\", \"trial\", \".\"], [\"even\", \"the\", \"victim\", \"'s\", \"ex-wife\", \"has\", \"opined\", \"that\", \"perhaps\", \"cordle\", \"deserves\", \"some\", \"consideration\", \"for\", \"his\", \"video\", \"apology\", \".\"], [\"bravo\", \",\", \"right\", \"?\"], [\"wrong\", \".\"], [\"while\", \"this\", \"may\", \"have\", \"appeared\", \"a\", \"morally\", \"correct\", \"thing\", \"for\", \"cordle\", \"to\", \"do\", \",\", \"our\", \"justice\", \"system\", \"can\", \"actually\", \"penalize\", \"those\", \"who\", \"\\\"\", \"do\", \"the\", \"right\", \"thing\", \"\\\"\", \"and\", \"volunteer\", \"admissions\", \".\"], [\"cordle\", \"'s\", \"case\", \"just\", \"might\", \"end\", \"up\", \"as\", \"an\", \"example\", \"of\", \"this\", \".\"], [\"in\", \"a\", \"criminal\", \"prosecution\", \",\", \"the\", \"strongest\", \"playing\", \"card\", \"in\", \"the\", \"hand\", \"of\", \"the\", \"defendant\", \"is\", \"a\", \"guilty\", \"plea\", \".\"], [\"sometimes\", \",\", \"it\", \"'s\", \"the\", \"only\", \"card\", \"he\", \"holds\", \".\"], [\"in\", \"cordle\", \"'s\", \"case\", \",\", \"assuming\", \"for\", \"the\", \"moment\", \"that\", \"this\", \"case\", \"was\", \"what\", \"we\", \"defense\", \"attorneys\", \"call\", \"a\", \"\\\"\", \"dead-bang\", \"loser\", \"\\\"\", \"(\", \"one\", \"likely\", \"to\", \"be\", \"lost\", \"at\", \"trial\", \")\", \",\", \"then\", \"his\", \"defense\", \"attorney\", \"could\", \"have\", \"approached\", \"the\", \"prosecution\", \"and\", \"made\", \"an\", \"offer\", \":\", \"in\", \"exchange\", \"for\", \"a\", \"lesser\", \"sentence\", \"or\", \"charges\", \",\", \"cordle\", \"would\", \"plead\", \"guilty\", \",\", \"apologize\", \"to\", \"the\", \"family\", \"and\", \"save\", \"the\", \"prosecutor\", \"'s\", \"office\", \"the\", \"manpower\", \"and\", \"the\", \"burden\", \"of\", \"trying\", \"the\", \"case\", \",\", \"and\", \"ultimately\", \",\", \"the\", \"risk\", \"of\", \"losing\", \"at\", \"trial\", \".\"], [\"in\", \"return\", \",\", \"the\", \"prosecution\", \"would\", \"agree\", \"to\", \"a\", \"lesser\", \"sentence\", \"or\", \"a\", \"lesser\", \"charge\", \".\"], [\"in\", \"this\", \"case\", \",\", \"cordle\", \"'s\", \"voluntary\", \"mea\", \"culpa\", \"actually\", \"eliminated\", \"his\", \"strongest\", \"bargaining\", \"chip\", \".\"], [\"the\", \"defense\", \"now\", \"has\", \"nothing\", \"to\", \"offer\", \"the\", \"prosecution\", \",\", \"so\", \"the\", \"prosecution\", \"has\", \"no\", \"incentive\", \"to\", \"make\", \"any\", \"concessions\", \"in\", \"a\", \"plea\", \"offer\", \".\"], [\"now\", \",\", \"if\", \"this\", \"were\", \"a\", \"business\", \"transaction\", \"or\", \"negotiation\", \",\", \"you\", \"'d\", \"say\", \":\", \"well\", \",\", \"that\", \"'s\", \"the\", \"way\", \"business\", \"is\", \".\"], [\"too\", \"bad\", \",\", \"so\", \"sad\", \".\"], [\"that\", \"'s\", \"just\", \"market\", \"forces\", \"and\", \"economic\", \"darwinism\", \"at\", \"work\", \".\"], [\"but\", \"prosecutors\", \"have\", \"a\", \"greater\", \"moral\", \"obligation\", \"than\", \"a\", \"ceo\", \"or\", \"an\", \"investor\", \".\"], [\"they\", \"can\", \"not\", \"simply\", \"exploit\", \"any\", \"weakness\", \"in\", \"the\", \"defendant\", \"'s\", \"bartering\", \"position\", \".\"], [\"prosecutors\", \"have\", \"a\", \"greater\", \"moral\", \"obligation\", \":\", \"justice\", \".\"], [\"it\", \"'s\", \"not\", \"an\", \"easy\", \"job\", \"at\", \"all\", \".\"], [\"so\", \",\", \"has\", \"cordle\", \"'s\", \"prosecutor\", \"adequately\", \"considered\", \"the\", \"video\", \"?\"], [\"the\", \"prosecutor\", \"in\", \"this\", \"case\", \"has\", \"said\", \"he\", \"will\", \"seek\", \"the\", \"maximum\", \"sentence\", \"and\", \"suggested\", \"that\", \"his\", \"office\", \"has\", \"enough\", \"evidence\", \"to\", \"seek\", \"a\", \"conviction\", \"notwithstanding\", \"the\", \"video\", \"confession\", \".\"], [\"the\", \"maximum\", \"sentence\", \"?\"], [\"is\", \"that\", \"how\", \"we\", \"should\", \"incentivize\", \"taking\", \"responsibility\", \"?\"], [\"after\", \"all\", \",\", \"cordle\", \"is\", \"partially\", \"correct\", \"in\", \"that\", \"he\", \"has\", \"spared\", \"resources\", \"and\", \"offered\", \"a\", \"judicially\", \"efficient\", \"resolution\", \".\"], [\"in\", \"the\", \"video\", \",\", \"cordle\", \"says\", \":\", \"\\\"\", \"i\", \"consulted\", \"some\", \"high-powered\", \"attorneys\", \",\", \"who\", \"told\", \"me\", \"stories\", \"about\", \"similar\", \"cases\", \"where\", \"the\", \"drivers\", \"got\", \"off\", \".\"], [\"they\", \"were\", \"convinced\", \"that\", \"they\", \"could\", \"get\", \"my\", \"blood\", \"test\", \"thrown\", \"out\", \".\"], [\"and\", \"all\", \"i\", \"would\", \"have\", \"to\", \"do\", \"for\", \"that\", \"was\", \"lie\", \".\"], [\"well\", \",\", \"i\", \"wo\", \"n't\", \"go\", \"down\", \"that\", \"path\", \".\", \"\\\"\"], [\"first\", \",\", \"it\", \"'s\", \"doubtful\", \"any\", \"experienced\", \"dui\", \"lawyer\", \"really\", \"told\", \"cordle\", \"to\", \"lie\", \".\"], [\"the\", \"obvious\", \"reason\", \"is\", \"that\", \"it\", \"'s\", \"unethical\", \".\"], [\"but\", \"second\", \",\", \"thanks\", \"to\", \"modern\", \"defense\", \"strategies\", \"and\", \"technology\", \",\", \"challenging\", \"breath\", \"and\", \"blood\", \"testing\", \"has\", \"arguably\", \"never\", \"been\", \"more\", \"effective\", \".\"], [\"recently\", \",\", \"courts\", \"have\", \"invalidated\", \"scores\", \"of\", \"breath\", \"and\", \"blood\", \"tests\", \"for\", \"a\", \"myriad\", \"of\", \"reasons\", \"related\", \"to\", \"the\", \"reliability\", \"of\", \"the\", \"science\", \"as\", \"applied\", \",\", \"including\", \"improper\", \"blood\", \"draws/retention\", \",\", \"administration\", \"of\", \"tests\", \",\", \"noncalibration\", \"of\", \"blood\", \"testing\", \"devices\", \"and\", \"improper\", \"lab\", \"procedures\", \".\"], [\"because\", \"dui\", \"trials\", \"can\", \"be\", \"burdensome\", \"for\", \"the\", \"prosecution\", \",\", \"most\", \"jurisdictions\", \"have\", \"adopted\", \"some\", \"version\", \"of\", \"a\", \"pretrial\", \"program\", \"to\", \"treat\", \"this\", \"crime\", \"differently\", \"than\", \"most\", \"others\", \".\"], [\"the\", \"quid\", \"pro\", \"quo\", \"is\", \"usually\", \"this\", \":\", \"the\", \"defendant\", \"admits\", \"to\", \"what\", \"he\", \"did\", \",\", \"gets\", \"a\", \"one-time\", \"admission\", \"to\", \"probation\", \"and\", \"a\", \"ton\", \"of\", \"fines\", \"and\", \"fees\", \",\", \"avoiding\", \"a\", \"criminal\", \"conviction\", \",\", \"and\", \"the\", \"prosecution\", \"is\", \"saved\", \"the\", \"burden\", \"of\", \"calling\", \"breathalyzer\", \"operators\", \"and\", \"lab\", \"technicians\", \"to\", \"trial\", \"and\", \"avoids\", \"the\", \"risk\", \"of\", \"loss\", \"at\", \"trial\", \".\"], [\"as\", \"in\", \"the\", \"court\", \"system\", \",\", \"drinking\", \"and\", \"driving\", \"is\", \"also\", \"treated\", \"differently\", \"than\", \"other\", \"taboos\", \"in\", \"society\", \".\"], [\"certainly\", \"in\", \"public\", \",\", \"we\", \"all\", \"agree\", \"it\", \"'s\", \"wrong\", \".\"], [\"but\", \"when\", \"a\", \"rustic\", \"pub\", \"is\", \"isolated\", \"on\", \"a\", \"winding\", \"country\", \"back\", \"road\", \"and\", \"the\", \"parking\", \"lot\", \"is\", \"filled\", \"with\", \"cars\", \"on\", \"a\", \"friday\", \"night\", \",\", \"does\", \"anyone\", \"doubt\", \"how\", \"the\", \"patrons\", \"arrived\", \"?\"], [\"or\", \"how\", \"they\", \"plan\", \"to\", \"leave\", \"?\"], [\"there\", \"are\", \"not\", \"a\", \"lot\", \"of\", \"subway\", \"stops\", \",\", \"taxis\", \"or\", \"bus\", \"routes\", \"out\", \"at\", \"the\", \"more\", \"rural\", \"watering\", \"holes\", \"or\", \"even\", \"in\", \"the\", \"city\", \"suburbs\", \".\"], [\"we\", \"publicly\", \"oppose\", \"drinking\", \"and\", \"driving\", \",\", \"but\", \"we\", \"also\", \"are\", \"all\", \"aware\", \"it\", \"'s\", \"going\", \"on\", \"everywhere\", \"--\", \"and\", \"with\", \"our\", \"tacit\", \"permission\", \".\"], [\"until\", \"we\", \"resolve\", \"the\", \"social\", \"paradox\", \"that\", \"is\", \"drinking\", \"and\", \"driving\", \",\", \"more\", \"than\", \"a\", \"few\", \"citizens\", \"will\", \"read\", \"about\", \"cordle\", \"'s\", \"case\", \"and\", \"admit\", \",\", \"to\", \"themselves\", \":\", \"\\\"\", \"coulda\", \"been\", \"me\", \".\", \"\\\"\"], [\"or\", \"worse\", \":\", \"\\\"\", \"that\", \"was\", \"me\", \"last\", \"saturday\", \".\", \"\\\"\"], [\"what\", \"happens\", \"when\", \"or\", \"if\", \"it\", \"is\", \"you\", \"that\", \"is\", \"in\", \"cordle\", \"'s\", \"position\", \"?\"], [\"will\", \"you\", \"still\", \"be\", \"an\", \"anti-dui\", \"crusader\", \"?\"], [\"will\", \"you\", \"fight\", \"the\", \"breath\", \"and\", \"blood\", \"tests\", \"tooth-and-nail\", \"?\"], [\"or\", \"will\", \"you\", \"take\", \"personal\", \"responsibility\", \"?\"], [\"youtube\", \"video\", \",\", \"perhaps\", \"?\"], [\"the\", \"opinions\", \"expressed\", \"in\", \"this\", \"commentary\", \"are\", \"solely\", \"those\", \"of\", \"danny\", \"cevallos\", \".\"]], \"tgt\": [[\"ohio\", \"man\", \"confesses\", \"online\", \"to\", \"killing\", \"someone\", \"while\", \"driving\", \"drunk\"], [\"danny\", \"cevallos\", \":\", \"matthew\", \"cordle\", \"'s\", \"video\", \"was\", \"an\", \"admirable\", \"effort\", \"to\", \"take\", \"responsibility\"], [\"he\", \"says\", \"cordle\", \"'s\", \"candor\", \"could\", \"lead\", \"to\", \"a\", \"more\", \"severe\", \"sentence\", \"than\", \"usual\"], [\"cevallos\", \":\", \"prosecutor\", \"should\", \"take\", \"interests\", \"of\", \"justice\", \"into\", \"account\"]]}, {\"src\": [[\"by\"], [\"deni\", \"kirkova\"], [\"published\", \":\"], [\"10:13\", \"est\", \",\", \"23\", \"may\", \"2013\"], [\"|\"], [\"updated\", \":\"], [\"10:13\", \"est\", \",\", \"23\", \"may\", \"2013\"], [\"when\", \"someone\", \"overweight\", \"claims\", \"to\", \"eat\", \"healthily\", \"and\", \"exercise\", \"but\", \"sill\", \"ca\", \"n't\", \"shift\", \"the\", \"pounds\", \",\", \"it\", \"can\", \"be\", \"difficult\", \"to\", \"believe\", \"them\", \".\"], [\"but\", \"that\", \"is\", \"just\", \"the\", \"situation\", \"londoner\", \"sidone\", \"price\", \"found\", \"herself\", \"in\", \"after\", \"doctors\", \"said\", \"her\", \"almost\", \"permanent\", \"bloating\", \"and\", \"excessive\", \"weight\", \"was\", \"down\", \"to\", \"ibs\", \".\"], [\"fortunately\", \"sidoine\", \"did\", \"not\", \"give\", \"up\", \"the\", \"fight\", \"to\", \"find\", \"a\", \"diagnosis\", \"and\", \",\", \"on\", \"a\", \"colleague\", \"'s\", \"advice\", \",\", \"sought\", \"a\", \"food\", \"intolerance\", \"test\", \"-\", \"and\", \"found\", \"the\", \"real\", \"reason\", \"behind\", \"her\", \"size\", \".\"], [\"airport\", \"lounge\", \"manager\", \"sidone\", \",\", \"26\", \",\", \"says\", \":\", \"`\", \"i\", \"had\", \"always\", \"been\", \"quite\", \"curvy\", \",\", \"but\", \"over\", \"the\", \"last\", \"few\", \"years\", \"i\", \"had\", \"noticed\", \"my\", \"weight\", \"just\", \"kept\", \"creeping\", \"up\", \",\", \"so\", \"i\", \"decided\", \"to\", \"do\", \"something\", \"about\", \"it\", \".\", \"'\"], [\"she\", \"started\", \"eating\", \"much\", \"healthier\", \",\", \"sticking\"], [\"to\", \"five\", \"small\", \"meals\", \"a\", \"day\", \"with\", \"a\", \"mixture\", \"of\", \"lean\", \"protein\", \"and\", \"lots\", \"of\"], [\"vegetables\", \"and\", \"salads\", \".\"], [\"but\", \"sidone\", \"struggled\", \"to\", \"shift\", \"much\", \"of\", \"her\", \"uncomfortable\", \"14st\", \"weight\", \".\"], [\"doctors\", \"said\", \"sidoine\", \"'s\", \"incessant\", \"bloating\", \"and\", \"excessive\", \"weight\", \"was\", \"down\", \"to\", \"ibs\"], [\"`\", \"i\", \"also\", \"joined\", \"a\", \"gym\", \"and\", \"started\", \"working\", \"out\", \"three\", \"times\", \"a\", \"week\", \".\"], [\"i\", \"noticed\", \"that\", \"some\", \"of\", \"the\", \"weight\", \"came\", \"off\", \",\", \"but\", \"it\", \"just\", \"seemed\", \"really\", \"slow\", \"and\", \"i\", \"was\", \"n't\", \"seeing\", \"the\", \"results\", \"i\", \"was\", \"expecting\", \"considering\", \"how\", \"healthily\", \"i\", \"was\", \"eating\", \"and\", \"how\", \"much\", \"effort\", \"i\", \"was\", \"putting\", \"in\", \"at\", \"the\", \"gym\", \".\"], [\"`\", \"i\", \"was\", \"also\", \"constantly\", \"bloated\", \"every\", \"time\", \"i\", \"'d\", \"eat\", \"which\", \"was\", \"causing\", \"me\", \"a\", \"lot\", \"of\", \"pain\", \"and\", \"discomfort\", \",\", \"not\", \"to\", \"mention\", \"left\", \"me\", \"feeling\", \"i\", \"felt\", \"embarrassed\", \".\"], [\"`\", \"months\", \"of\", \"healthy\", \"eating\", \"and\", \"exercise\"], [\"were\", \"proving\", \"to\", \"have\", \"little\", \"effect\", \",\", \"especially\", \"on\", \"my\", \"stomach\", \"area\", \"which\"], [\"remained\", \"stubbornly\", \"bloated\", \".\"], [\"`\", \"i\", \"became\", \"quite\", \"anxious\", \"about\", \"the\", \"situation\", \"and\", \"so\", \"i\", \"went\", \"to\", \"see\", \"my\", \"gp\", \".\"], [\"he\", \"was\", \"n't\", \"very\", \"helpful\", \"and\", \"just\", \"told\", \"me\", \"to\", \"carry\", \"on\", \"as\", \"i\", \"was\", \",\", \"telling\", \"me\", \"that\", \"the\", \"bloating\", \"was\", \"probably\", \"just\", \"ibs\", \"and\", \"that\", \"the\", \"weight\", \"would\", \"fall\", \"off\", \"eventually\", \".\"], [\"`\", \"in\", \"desperation\", \"i\", \"tried\", \"a\", \"radical\"], [\"diet\", \"that\", \"involved\", \"eating\", \"mainly\", \"raw\", \"carrots\", \"and\", \"houmous\", \"at\", \"lunch\", \",\", \"but\", \"again\", \"this\", \"proved\", \"to\", \"have\", \"little\", \"effect\", \".\"], [\"`\", \"it\"], [\"was\", \"then\", \"that\", \"a\", \"colleague\", \"suggested\", \"that\", \"i\", \"might\", \"have\", \"a\", \"food\"], [\"intolerance\", \"and\", \"recommended\", \"that\", \"i\", \"try\", \"a\", \"food\", \"intolerance\", \"test\", \".\"], [\"sidoine\", \"was\", \"able\", \"to\", \"shed\", \"those\", \"stubborn\", \"excess\", \"pounds\", \"she\", \"'d\", \"been\", \"trying\", \"so\", \"hard\", \"to\", \"get\", \"rid\", \"of\"], [\"`\", \"when\", \"the\", \"results\", \"came\", \"back\", \"they\", \"revealed\", \"that\", \"i\", \"had\", \"intolerances\", \"to\", \"a\", \"number\", \"of\", \"the\", \"`\", \"healthy\", \"'\", \"foods\", \"i\", \"'d\", \"been\", \"munching\", \"for\", \"the\", \"past\", \"few\", \"months\", \"-\", \"including\", \"carrots\", \"!\", \"'\"], [\"`\", \"cutting\", \"these\", \"from\", \"my\", \"diet\", \"had\", \"an\", \"almost\", \"instantaneous\", \"effect\", \".\"], [\"within\", \"days\", \"my\", \"stomach\", \"was\", \"noticeably\", \"flatter\", \"and\", \"my\", \"ibs\", \"symptoms\", \"had\", \"greatly\", \"reduced\", \".\", \"'\"], [\"`\", \"as\", \"well\", \"as\", \"carrots\", \",\", \"i\", \"also\", \"found\", \"out\"], [\"that\", \"i\", \"am\", \"intolerant\", \"to\", \"cow\", \"'s\", \"milk\", \"and\", \"yeast\", \".\"], [\"i\", \"have\", \"cut\", \"these\"], [\"completely\", \"from\", \"my\", \"diet\", \".\"], [\"three\", \"months\", \"later\", \"i\", \"am\", \"still\", \"sticking\", \"to\", \"my\"], [\"new\", \"diet\", \".\"], [\"the\", \"yorktest\", \"intolerance\", \"test\", \"made\", \"sidone\", \"change\", \"her\", \"eating\", \"habits\", \"immediately\", \"and\", \"while\", \"she\", \"carries\", \"on\", \"with\", \"her\", \"healthy\", \"diet\", \"from\", \"before\", \",\", \"she\", \"has\", \"had\", \"to\", \"find\", \"alternatives\", \"to\", \"her\", \"reactive\", \"foods\", \":\", \"carrots\", \",\", \"cow\", \"'s\", \"milk\", \"and\", \"yeast\", \".\"], [\"`\", \"i\"], [\"eat\", \"quite\", \"a\", \"lot\", \"of\", \"stir-fried\", \"vegetables\", \"as\", \"i\", \"find\", \"just\", \"boiled\", \"veg\", \"can\"], [\"be\", \"quite\", \"dull\", \".\"], [\"where\", \"before\", \"i\", \"'d\", \"always\", \"throw\", \"in\", \"some\", \"carrots\", \",\", \"now\", \"i\", \"'ve\"], [\"replaced\", \"this\", \"with\", \"other\", \"veg\", \"such\", \"as\", \"courgettes\", \".\"], [\"`\", \"as\", \"i\", \"'ve\", \"completely\"], [\"cut\", \"out\", \"cow\", \"'s\", \"milk\", \",\", \"i\", \"now\", \"use\", \"almond\", \"and\", \"soya\", \"milk\", \"for\", \"smoothies\", \"and\"], [\"soups\", \".\"], [\"`\", \"wine\", \"contains\", \"yeast\", \"so\", \"i\", \"have\", \"also\", \"replaced\", \"my\", \"usual\", \"white\", \"wine\", \"for\", \"spirits\", \".\"], [\"i\", \"'ll\", \"go\", \"for\", \"a\", \"g&t\", \"now\", \"instead\", \"of\", \"a\", \"glass\", \"of\", \"wine\", \".\"], [\"`\", \"i\", \"also\", \"steer\", \"clear\", \"of\", \"using\", \"dressings\", \"and\", \"sauces\", \"because\", \"they\", \"can\", \"contain\", \"yeast\", \"too\", \".\"], [\"i\", \"'ve\", \"stopped\", \"using\", \"soya\", \"sauce\", \"when\", \"i\", \"make\", \"stir-fry\", \"as\", \"i\", \"'ve\", \"struggled\", \"to\", \"find\", \"a\", \"yeast-free\", \"version\", \".\", \"'\"], [\"in\", \"the\", \"months\", \"that\", \"followed\", \"sidoine\", \"was\", \"able\", \"to\", \"shed\", \"the\", \"stubborn\", \"excess\", \"pounds\", \"she\", \"had\", \"been\", \"trying\", \"so\", \"hard\", \"to\", \"get\", \"rid\", \"of\", \".\"], [\"`\", \"i\", \"do\", \"n't\", \"yet\", \"have\", \"a\", \"boyfriend\", \"but\", \"i\", \"am\"], [\"dating\", \".\"], [\"i\", \"did\", \"n't\", \"feel\", \"confident\", \"about\", \"dating\", \"before\", \"but\", \"i\", \"do\", \"much\", \"more\"], [\"now\", \".\"], [\"also\", \"i\", \"'m\", \"much\", \"more\", \"confident\", \"on\"], [\"holiday\", \"and\", \"in\", \"summer\", \"clothes\", \"-\", \"i\", \"can\", \"finally\", \"enjoy\", \"clothes\", \"shopping\", \"now\", \"whereas\"], [\"before\", \"i\", \"would\", \"avoid\", \"it\", \",\", \"'\", \"she\", \"said\", \".\"], [\"she\"], [\"now\", \"weighs\", \"in\", \"at\", \"a\", \"svelte\", \"9st\", \"7\", \"lbs\", \",\", \"losing\", \"well\", \"over\", \"4st\", \",\", \"and\", \"wears\", \"a\", \"size\", \"12\", \"-\", \"three\", \"sizes\", \"down\", \"from\", \"her\", \"peak\", \"at\", \"size\", \"18\", \".\"], [\"`\", \"it\", \"feels\", \"like\", \"my\", \"entire\", \"life\", \"has\", \"changed\", \"'\", \",\", \"she\", \"says\", \".\"], [\"`\", \"i\", \"would\", \"n't\", \"go\", \"back\", \"to\", \"my\", \"old\", \"diet\", \"and\", \"i\", \"'ll\", \"make\", \"sure\", \"i\", \"never\", \"have\", \"trigger\"], [\"foods\", \"again\", \"...\", \"i\", \"'ve\", \"gone\", \"cold\", \"turkey\", \".\", \"'\"], [\"recent\", \"research\", \"suggests\", \"a\", \"direct\", \"correlation\", \"between\", \"food\", \"intolerance\", \"and\", \"weight\", \"gain\", \".\"], [\"last\", \"year\", \"'s\", \"obesity\", \"and\", \"weight\", \"loss\", \"therapy\", \"research\"], [\"entitled\", \":\", \"eliminating\", \"immunologically-reactive\", \"foods\", \"from\", \"the\", \"diet\", \"and\"], [\"its\", \"effect\", \"on\", \"body\", \"composition\", \"and\", \"quality\", \"of\", \"life\", \"in\", \"overweight\", \"persons\"], [\"found\", \"that\", \"by\", \"simply\", \"cutting\", \"trigger\", \"foods\", \"out\", \"of\", \"your\", \"diet\", \",\", \"the\", \"battle\"], [\"of\", \"the\", \"bulge\", \"could\", \"be\", \"over\", \".\"], [\"`\", \"cutting\", \"carrots\", \",\", \"cow\", \"'s\", \"milk\", \"and\", \"yeast\", \"had\", \"an\", \"almost\", \"instant\", \"effect\", \"and\", \"within\", \"days\", \"my\", \"stomach\", \"was\", \"flatter\", \"'\"], [\"breakfast\", \":\", \"porridge\", \"or\", \"a\", \"smoothie\", \"for\", \"breakfast\", \"(\", \"strawberry\", \",\", \"banana\", \"and\", \"blueberry\", \"with\", \"some\", \"natural\", \"live\", \"yogurt\", \")\"], [\"lunch\", \":\", \"carrots\", \"and\", \"reduced\", \"fat\", \"hummus\", \"(\", \"which\", \"she\", \"later\", \"learned\", \"was\", \"no\", \"good\", \"for\", \"her\", \")\"], [\"dinner\", \":\", \"chicken\", \"with\", \"stir\", \"fried\", \"vegetables\", \"(\", \"onions\", \",\", \"mushrooms\", \",\", \"bean\"], [\"sprouts\", \",\", \"carrots\", \",\", \"peppers\", \")\", \",\", \"salmon\", \"and\", \"salad\", \",\", \"omelette\", \"(\", \"usually\", \"using\"], [\"skimmed\", \"milk\", \")\"], [\"breakfast\", \":\", \"smoothie\", \"(\", \"strawberry\", \",\", \"blueberries\", \",\", \"banana\", \"and\", \"soya\", \"or\", \"almond\", \"milk\", \")\"], [\"lunch\", \":\", \"soup\", \"(\", \"homemade\", \"potato\", \"and\", \"leek\", \"or\", \"root\", \"vegetable\", \"minus\", \"carrots\", \"and\", \"using\", \"dairy-free\", \"cream\", \")\"], [\"dinner\", \":\", \"omelette\", \"minus\", \"milk\", \",\", \"jacket\", \"potato\", \"with\", \"tuna\", \"and\", \"sweetcorn\", \",\", \"chicken\", \"with\", \"vegetables\", \"(\", \"broccoli\", \",\", \"cauliflower\", \")\"], [\"with\", \"even\", \"seemingly\", \"healthy\", \"foods\", \"such\", \"as\", \"lettuce\", \"having\", \"the\", \"potential\", \"to\", \"cause\", \"a\", \"reaction\", \",\", \"the\", \"study\", \"shows\", \"it\", \"could\", \"be\", \"beneficial\", \"to\", \"find\", \"out\", \"your\", \"own\", \"personal\", \"triggers\", \".\"], [\"the\", \"research\", \"assessed\", \"the\", \"effect\", \"that\", \"igg\", \"food\", \"sensitivity\", \"tests\", \"and\", \"the\", \"subsequent\", \"elimination\", \"of\", \"trigger\", \"foods\", \"for\", \"a\", \"period\", \"of\", \"90\", \"days\", \"had\", \"on\", \"the\", \"body\", \"composition\", \"and\", \"quality\", \"of\", \"life\", \"of\", \"120\", \"overweight\", \"people\", \".\"], [\"the\", \"results\", \"showed\", \"that\", \"the\", \"subjects\", \"who\", \"eliminated\", \"the\", \"foods\", \"they\", \"reacted\", \"to\", \"`\", \"had\", \"reductions\", \"in\", \"weight\", \",\", \"body\", \"mass\", \"index\", \",\", \"waist\", \"and\", \"hip\", \"circumference\\u00e2\", \"\\u20ac\", \"\\u00a6\", \"and\", \"all\", \"indicators\", \"of\", \"quality\", \"of\", \"life\", \"'\", \".\"], [\"`\", \"food\", \"intolerance\", \"is\", \"a\", \"serious\", \"issue\", \"and\", \"is\", \"often\", \"misdiagnosed\", \".\"], [\"i\", \"meet\", \"numerous\", \"clients\", \"who\", \"consistently\", \"eat\", \"healthy\", \"and\", \"exercise\", \"regularly\", \"but\", \"still\", \"find\", \"it\", \"hard\", \"to\", \"shift\", \"those\", \"last\", \"few\", \"pounds\", \".\"], [\"`\", \"food\", \"intolerances\", \"can\", \"cause\", \"an\", \"immune\", \"response\", \"in\", \"the\", \"body\", \"which\", \"can\", \"lead\", \"to\", \"weight\", \"gain\", \"or\", \"difficulty\", \"in\", \"losing\", \"weight\", \"'\", \"nutritionist\", \",\", \"alli\", \"godbold\", \".\"], [\"dr.\", \"gill\", \"hart\", \",\", \"scientific\", \"director\", \"at\", \"yorktest\", \",\", \"explains\", \":\", \"`\", \"a\", \"stressed\", \"digestive\", \"system\", \"is\", \"less\", \"able\", \"to\", \"process\", \"energy\", \"and\", \"nutrients\", \"for\", \"the\", \"rest\", \"of\", \"the\", \"body\", \".\"], [\"it\", \"will\", \"also\", \"be\", \"less\", \"able\", \"to\", \"repel\", \"harmful\", \"invaders\", \"and\", \"may\", \"be\", \"more\", \"prone\", \"to\", \"disorders\", \"such\", \"as\", \"ibs\", \".\"], [\"`\", \"once\", \"the\", \"immune\", \"response\", \"is\", \"triggered\", \"in\", \"the\", \"body\", \",\", \"it\", \"feels\", \"that\", \"it\", \"is\", \"under\", \"siege\", \"thus\", \"hoarding\", \"supplies\", \",\", \"storing\", \"fluid\", \"and\", \"increasing\", \"weight\", \".\", \"'\"]], \"tgt\": [[\"sidoine\", \"price\", \",\", \"26\", \",\", \"ate\", \"healthily\", \"but\", \"was\", \"bloated\", \"and\", \"overweight\"], [\"gp\", \"blamed\", \"her\", \"14st\", \"weight\", \"on\", \"ibs\", \"and\", \"said\", \"to\", \"carry\", \"on\", \"as\", \"normal\"], [\"colleague\", \"recommended\", \"she\", \"try\", \"food\", \"intolerance\", \"testing\"], [\"discovered\", \"she\", \"reacted\", \"badly\", \"to\", \"carrots\", \",\", \"yeast\", \"and\", \"cow\", \"'s\", \"milk\"], [\"cut\", \"foods\", \"from\", \"diet\", \"and\", \"slimmed\", \"three\", \"dress\", \"sizes\", \"in\", \"three\", \"months\"], [\"research\", \"shows\", \"cutting\", \"reaction\", \"foods\", \"result\", \"in\", \"`\", \"reductions\", \"in\", \"weight\", \",\", \"body\", \"mass\", \"index\", \",\", \"waist\", \"and\", \"hip\", \"circumference\", \"'\"]]}, {\"src\": [[\"at\", \"the\", \"former\", \"concentration\", \"camp\", \"auschwitz\", \",\", \"poland\", \"(\", \"cnn\", \")\", \"menachem\", \"bodner\", \"has\", \"been\", \"to\", \"auschwitz\", \"three\", \"times\", \":\"], [\"first\", \"as\", \"a\", \"child\", \"prisoner\", \"of\", \"just\", \"4\", \"years\", \"old\", \".\"], [\"he\", \"has\", \"only\", \"fragments\", \"of\", \"memories\", \"from\", \"his\", \"time\", \"here\", \":\", \"staring\", \"out\", \"at\", \"barbed\", \"wire\", \",\", \"running\", \"down\", \"concrete\", \"stairs\", \",\", \"hiding\", \"in\", \"a\", \"corner\", \".\"], [\"and\", \"one\", \"especially\", \"vivid\", \"memory\", \"that\", \"still\", \"haunts\", \"his\", \"dreams\", \",\", \"the\", \"bloodied\", \"face\", \"of\", \"an\", \"elderly\", \"man\", \".\"], [\"last\", \"year\", \",\", \"he\", \"visited\", \"the\", \"german\", \"nazi-run\", \"concentration\", \"camp\", \"as\", \"a\", \"survivor\", \".\"], [\"he\", \"came\", \"for\", \"closure\", \"and\", \"says\", \"he\", \"felt\", \"an\", \"overwhelming\", \"sense\", \"of\", \"relief\", \".\"], [\"during\", \"the\", \"70-year\", \"memorial\", \"for\", \"the\", \"liberation\", \"of\", \"auschwitz\", \"on\", \"tuesday\", \",\", \"he\", \"visited\", \"the\", \"camp\", \"in\", \"search\", \"for\", \"his\", \"identical\", \"twin\", \"brother\", \",\", \"separated\", \"shortly\", \"after\", \"liberation\", \".\"], [\"\\\"\", \"sometimes\", \"i\", \"stare\", \"at\", \"people\", \"in\", \"the\", \"street\", \",\", \"and\", \"i\", \"look\", \"for\", \"someone\", \"who\", \"looks\", \"like\", \"me\", \",\", \"\\\"\", \"he\", \"said\", \".\"], [\"\\\"\", \"my\", \"biggest\", \"hope\", \"is\", \"my\", \"brother\", \"will\", \"be\", \"here\", \".\"], [\"maybe\", \"from\", \"another\", \"country\", \".\"], [\"or\", \"maybe\", \"another\", \"survivor\", \"can\", \"tell\", \"me\", \"something\", \",\", \"recognize\", \"me\", \"or\", \"remember\", \"us\", \"both\", \".\"], [\"anything\", \".\", \"\\\"\"], [\"but\", \"as\", \"he\", \"stands\", \"in\", \"the\", \"snow\", \",\", \"next\", \"to\", \"the\", \"barbed\", \"wire\", \"and\", \"the\", \"brick\", \"buildings\", \",\", \"his\", \"heart\", \"pounds\", \".\"], [\"he\", \"turns\", \"away\", \"to\", \"hide\", \"his\", \"tears\", \".\"], [\"seventy\", \"years\", \"ago\", \",\", \"menachem\", \"bodner\", \"was\", \"known\", \"as\", \"elias\", \"gottesman\", \",\", \"auschwitz\", \"id\", \"a-7733\", \".\"], [\"auschwitz\", \"records\", \"show\", \"he\", \"had\", \"a\", \"twin\", \",\", \"jeno\", \"gottesman\", \"a-7734\", \".\"], [\"both\", \"were\", \"subjected\", \"to\", \"the\", \"medical\", \"experiments\", \"of\", \"nazi\", \"doctor\", \"josef\", \"mengele\", \".\"], [\"and\", \"both\", \"survived\", \"auschwitz\", \"to\", \"be\", \"taken\", \"into\", \"a\", \"care\", \"home\", \"for\", \"children\", \"after\", \"liberation\", \".\"], [\"but\", \"elias\", \"was\", \"adopted\", \"by\", \"a\", \"man\", \"searching\", \"for\", \"his\", \"own\", \"wife\", \"and\", \"children\", \"in\", \"the\", \"chaos\", \"and\", \"eventually\", \"taken\", \"to\", \"israel\", \".\"], [\"he\", \"was\", \"given\", \"the\", \"new\", \"name\", \"of\", \"menachem\", \"bodner\", \".\"], [\"what\", \"happened\", \"to\", \"jeno\", \"gottesman\", \"is\", \"unknown\", \".\"], [\"for\", \"decades\", \",\", \"menachem\", \"did\", \"n't\", \"even\", \"know\", \"he\", \"had\", \"a\", \"twin\", \".\"], [\"his\", \"only\", \"memory\", \"came\", \"to\", \"him\", \"in\", \"the\", \"form\", \"of\", \"dreams\", \"of\", \"another\", \"boy\", \",\", \"blond\", \"like\", \"him\", \",\", \"sleeping\", \"in\", \"bed\", \"beside\", \"him\", \".\"], [\"it\", \"took\", \"nearly\", \"70\", \"years\", \"and\", \"the\", \"help\", \"of\", \"his\", \"genealogist\", \",\", \"ayana\", \"kimron\", \",\", \"poring\", \"over\", \"documents\", \"for\", \"him\", \"to\", \"prove\", \"his\", \"instinct\", \"was\", \"real\", \".\"], [\"in\", \"one\", \"sense\", \",\", \"she\", \"says\", \",\", \"his\", \"lack\", \"of\", \"memory\", \"is\", \"a\", \"blessing\", \":\"], [\"\\\"\", \"i\", \"'m\", \"really\", \"happy\", \"for\", \"him\", \"that\", \"he\", \"lost\", \"his\", \"memory\", \".\", \"\\\"\"], [\"she\", \"told\", \"cnn\", \",\", \"\\\"\", \"it\", \"'s\", \"so\", \"much\", \"easier\", \"to\", \"establish\", \"a\", \"normal\", \"life\", \".\"], [\"but\", \"even\", \"then\", \",\", \"he\", \"had\", \"nightmares\", \".\"], [\"just\", \"imagine\", \"if\", \"he\", \"remembered\", \".\", \"\\\"\"], [\"but\", \"it\", \"means\", \"she\", \"must\", \"rely\", \"on\", \"the\", \"memories\", \"of\", \"other\", \"survivors\", \"to\", \"try\", \"and\", \"track\", \"down\", \"his\", \"twin\", \".\"], [\"\\\"\", \"it\", \"'s\", \"all\", \"about\", \"the\", \"awareness\", \"of\", \"friends\", \"and\", \"neighbors\", \".\"], [\"and\", \"courage\", \".\"], [\"if\", \"he\", \"'s\", \"there\", \"and\", \"he\", \"knows\", \"about\", \"the\", \"search\", \",\", \"he\", \"'ll\", \"need\", \"some\", \"courage\", \"to\", \"come\", \"forward\", \",\", \"and\", \"then\", \"he\", \"'ll\", \"find\", \"he\", \"has\", \"a\", \"wonderful\", \"family\", \".\", \"\\\"\"], [\"menachem\", \"has\", \"launched\", \"a\", \"facebook\", \"page\", \"titled\", \"a-7734\", \"in\", \"the\", \"hope\", \"that\", \"social\", \"media\", \"will\", \"spread\", \"word\", \"of\", \"his\", \"search\", \".\"], [\"there\", \"has\", \"been\", \"progress\", \":\", \"a\", \"dna\", \"match\", \"found\", \"menachem\", \"'s\", \"first\", \"cousins\", \"in\", \"the\", \"united\", \"states\", \"--\", \"the\", \"only\", \"relatives\", \"from\", \"his\", \"birth\", \"family\", \".\"], [\"they\", \"had\", \"no\", \"idea\", \"that\", \"menachem\", \"was\", \"alive\", \"and\", \"had\", \"assumed\", \"that\", \"he\", \"and\", \"his\", \"entirely\", \"family\", \"had\", \"perished\", \"in\", \"auschwitz\", \".\"], [\"it\", \"was\", \"an\", \"emotional\", \"reunion\", \",\", \"and\", \"his\", \"cousins\", \"gave\", \"him\", \"a\", \"gift\", \":\", \"the\", \"only\", \"photo\", \"of\", \"his\", \"birth\", \"parents\", \".\"], [\"\\\"\", \"they\", \"told\", \"me\", \"so\", \"much\", \"about\", \"my\", \"mother\", \",\", \"\\\"\", \"he\", \"says\", \",\", \"smiling\", \".\"], [\"\\\"\", \"the\", \"most\", \"important\", \"thing\", \":\", \"now\", \"i\", \"know\", \"my\", \"mother\", \"'s\", \"face\", \".\"], [\"before\", \"i\", \"had\", \"remembered\", \"only\", \"her\", \"blond\", \"hair\", \".\"], [\"now\", \"i\", \"can\", \"see\", \"her\", \".\", \"\\\"\"], [\"but\", \"he\", \"still\", \"searches\", \"for\", \"jeno\", \".\"], [\"and\", \"he\", \"comes\", \"to\", \"auschwitz\", \"in\", \"the\", \"hope\", \"he\", \"will\", \"remember\", \"some\", \"clue\", \",\", \"another\", \"survivor\", \"will\", \"recognize\", \"him\", \".\"], [\"maybe\", \"even\", \"his\", \"own\", \"brother\", \".\"], [\"he\", \"says\", \"he\", \"has\", \"new\", \"dream\", \"now\", \":\", \"he\", \"sees\", \"himself\", \"walking\", \"in\", \"a\", \"forest\", \"with\", \"his\", \"brother\", \",\", \"wearing\", \"identical\", \"clothes\", \"--\", \"black\", \"trousers\", \"--\", \"and\", \"a\", \"short\", \"sleeve\", \"blue\", \"shirt\", \".\"], [\"is\", \"it\", \"a\", \"dream\", \"or\", \"does\", \"he\", \"believe\", \"it\", \"will\", \"become\", \"a\", \"reality\", \"?\"], [\"\\\"\", \"maybe\", \",\", \"i\", \"do\", \"n't\", \"know\", \".\", \"\\\"\"], [\"he\", \"says\", \".\"], [\"but\", \"standing\", \"in\", \"the\", \"snow\", \",\", \"outside\", \"the\", \"very\", \"building\", \"he\", \"was\", \"held\", \"as\", \"a\", \"prisoner\", \"as\", \"a\", \"young\", \"boy\", \",\", \"menachem\", \"insists\", \"he\", \"has\", \"not\", \"given\", \"up\", \".\"], [\"\\\"\", \"no\", \",\", \"not\", \"at\", \"all\", \".\", \"\\\"\"], [\"he\", \"says\", \",\", \"\\\"\", \"it\", \"only\", \"makes\", \"me\", \"want\", \"to\", \"search\", \"for\", \"him\", \"even\", \"more\", \".\", \"\\\"\"], [\"in\", \"auschwitz\", \",\", \"the\", \"place\", \"of\", \"his\", \"nightmares\", \",\", \"menachem\", \"still\", \"finds\", \"hope\", \"to\", \"dream\", \".\"]], \"tgt\": [[\"menachem\", \"bodner\", \",\", \"then\", \"called\", \"elias\", \"gottesman\", \",\", \"was\", \"separated\", \"from\", \"his\", \"twin\", \",\", \"jeno\", \",\", \"after\", \"liberation\"], [\"\\\"\", \"sometimes\", \"i\", \"stare\", \"at\", \"people\", \"in\", \"the\", \"street\", \",\", \"and\", \"i\", \"look\", \"for\", \"someone\", \"who\", \"looks\", \"like\", \"me\", \",\", \"\\\"\", \"he\", \"says\"], [\"cnn\", \"goes\", \"with\", \"him\", \"to\", \"auschwitz\", \",\", \"where\", \"he\", \"hopes\", \"memories\", \"might\", \"be\", \"triggered\"]]}, {\"src\": [[\"(\", \"cnn\", \")\", \"--\", \"in\", \"what\", \"country\", \"would\", \"the\", \"presidential\", \"palace\", \"auction\", \"off\", \"some\", \"of\", \"its\", \"finest\", \"wines\", \"to\", \"buy\", \"more\", \"wine\", \"and\", \"boost\", \"state\", \"coffers\", \"?\"], [\"france\", \",\", \"bien\", \"s\\u00fbr\", \"!\"], [\"the\", \"\\u00e9lys\\u00e9e\", \"palace\", \"--\", \"the\", \"official\", \"residence\", \"of\", \"the\", \"french\", \"president\", \"--\", \"has\", \"dipped\", \"into\", \"its\", \"wine\", \"cellars\", \"and\", \"put\", \"1,200\", \"bottles\", \"up\", \"for\", \"auction\", \"in\", \"paris\", \".\"], [\"mainly\", \"from\", \"bordeaux\", \"and\", \"burgundy\", \",\", \"the\", \"wines\", \"would\", \"normally\", \"have\", \"been\", \"served\", \"to\", \"ambassadors\", \"or\", \"visiting\", \"dignitaries\", \"as\", \"they\", \"dined\", \"with\", \"the\", \"president\", \",\", \"showing\", \"off\", \"the\", \"finest\", \"of\", \"french\", \"viticulture\", \"in\", \"the\", \"process\", \".\"], [\"the\", \"funds\", \"raised\", \"will\", \"allow\", \"the\", \"renewal\", \"of\", \"the\", \"palace\", \"'s\", \"wine\", \"stocks\", \",\", \"auction\", \"house\", \"drouot\", \"said\", \".\"], [\"\\\"\", \"more\", \"modest\", \"\\\"\", \"wines\", \"will\", \"be\", \"bought\", \"in\", \"place\", \"of\", \"some\", \"of\", \"the\", \"fine\", \"vintages\", \"put\", \"under\", \"the\", \"hammer\", \",\", \"with\", \"any\", \"extra\", \"money\", \"left\", \"poured\", \"back\", \"into\", \"the\", \"state\", \"budget\", \",\", \"it\", \"said\", \".\"], [\"the\", \"\\u00e9lys\\u00e9e\", \"'s\", \"head\", \"sommelier\", \",\", \"virginie\", \"routis\", \",\", \"picked\", \"the\", \"wines\", \"to\", \"be\", \"auctioned\", \"off\", \"in\", \"two\", \"sales\", \",\", \"on\", \"thursday\", \"and\", \"friday\", \".\"], [\"the\", \"1,200\", \"bottles\", \"represent\", \"about\", \"a\", \"tenth\", \"of\", \"the\", \"cellar\", \"'s\", \"total\", \"stocks\", \".\"], [\"some\", \"date\", \"back\", \"decades\", \",\", \"while\", \"others\", \"are\", \"newer\", \"but\", \"represent\", \"great\", \"vintages\", \".\"], [\"the\", \"auction\", \"house\", \"expects\", \"the\", \"bids\", \"to\", \"range\", \"between\", \"15\", \"euros\", \"(\", \"$\", \"19\", \")\", \"for\", \"the\", \"most\", \"ordinary\", \"and\", \"an\", \"estimated\", \"2,200\", \"euros\", \"(\", \"$\", \"2,800\", \")\", \"for\", \"a\", \"1990\", \"petrus\", \".\"], [\"many\", \"bidders\", \"should\", \"be\", \"able\", \"to\", \"get\", \"their\", \"hands\", \"on\", \"a\", \"bottle\", \"for\", \"less\", \"than\", \"100\", \"euros\", \",\", \"the\", \"auction\", \"house\", \"said\", \".\"], [\"all\", \"the\", \"bottles\", \"auctioned\", \"off\", \"will\", \"sport\", \"a\", \"label\", \"stating\", \"that\", \"they\", \"came\", \"from\", \"the\", \"\\u00e9lys\\u00e9e\", \"palace\", \",\", \"with\", \"the\", \"date\", \"of\", \"their\", \"sale\", \",\", \"according\", \"to\", \"cnn\", \"affiliate\", \"bfmtv\", \".\"], [\"the\", \"palace\", \"cellar\", \"was\", \"set\", \"up\", \"in\", \"1947\", \",\", \"during\", \"the\", \"presidency\", \"of\", \"vincent\", \"auriol\", \",\", \"and\", \"was\", \"reorganized\", \"in\", \"1995\", \"to\", \"ensure\", \"the\", \"best\", \"conditions\", \"for\", \"keeping\", \"the\", \"wines\", \".\"], [\"built\", \"up\", \"over\", \"decades\", \",\", \"it\", \"includes\", \"wines\", \"from\", \"alsace\", \",\", \"the\", \"champagne\", \"region\", \",\", \"the\", \"rh\\u00f4ne\", \"valley\", \"and\", \"the\", \"loire\", \"alongside\", \"those\", \"from\", \"burgundy\", \"and\", \"bordeaux\", \".\"], [\"\\\"\", \"all\", \"these\", \"wines\", \"were\", \"served\", \"at\", \"the\", \"table\", \"of\", \"the\", \"president\", \"of\", \"the\", \"republic\", \",\", \"and\", \"some\", \"of\", \"them\", \"have\", \"accompanied\", \"important\", \"moments\", \"in\", \"the\", \"history\", \"of\", \"the\", \"fifth\", \"republic\", \",\", \"\\\"\", \"said\", \"drouot\", \".\"], [\"the\", \"fifth\", \"republic\", \"was\", \"established\", \"in\", \"1958\", \"and\", \"continues\", \"to\", \"this\", \"day\", \",\", \"under\", \"president\", \"fran\\u00e7ois\", \"hollande\", \".\"], [\"elected\", \"last\", \"year\", \",\", \"he\", \"has\", \"sought\", \"to\", \"boost\", \"tax\", \"revenues\", \"from\", \"the\", \"wealthy\", \"to\", \"try\", \"to\", \"cut\", \"france\", \"'s\", \"large\", \"deficit\", \".\"], [\"the\", \"history\", \"and\", \"associations\", \"of\", \"the\", \"\\u00e9lys\\u00e9e\", \"wine\", \"may\", \"be\", \"expected\", \"to\", \"push\", \"prices\", \"up\", \"beyond\", \"what\", \"would\", \"be\", \"paid\", \"for\", \"similar\", \"bottles\", \"sold\", \"elsewhere\", \".\"], [\"chris\", \"smith\", \",\", \"investment\", \"manager\", \"at\", \"the\", \"wine\", \"investment\", \"fund\", \"in\", \"london\", \",\", \"told\", \"cnn\", \"the\", \"\\u00e9lys\\u00e9e\", \"palace\", \"auction\", \"would\", \"be\", \"likely\", \"to\", \"draw\", \"buyers\", \"who\", \"are\", \"attracted\", \"by\", \"the\", \"kudos\", \"of\", \"the\", \"wines\", \"'\", \"provenance\", \"rather\", \"than\", \"serious\", \"investors\", \".\"], [\"the\", \"wines\", \"are\", \"mostly\", \"mature\", \"and\", \"ready\", \"to\", \"be\", \"drunk\", \"sooner\", \"rather\", \"than\", \"later\", \",\", \"which\", \"makes\", \"them\", \"less\", \"of\", \"an\", \"investment\", \"buy\", \",\", \"he\", \"said\", \",\", \"although\", \"the\", \"1990\", \"petrus\", \"is\", \"a\", \"very\", \"good\", \"wine\", \".\"], [\"\\\"\", \"the\", \"wine\", \"is\", \"not\", \"especially\", \"rare\", \"or\", \"unusual\", \"--\", \"they\", \"are\", \"the\", \"sort\", \"of\", \"wines\", \"that\", \"crop\", \"up\", \"at\", \"auction\", \"every\", \"now\", \"or\", \"then\", \",\", \"\\\"\", \"he\", \"said\", \".\"], [\"\\\"\", \"the\", \"fact\", \"that\", \"they\", \"are\", \"from\", \"the\", \"\\u00e9lys\\u00e9e\", \"palace\", \"gives\", \"them\", \"a\", \"certain\", \"sort\", \"of\", \"extra\", \"kudos\", \"that\", \"people\", \"may\", \"be\", \"prepared\", \"to\", \"pay\", \"a\", \"small\", \"premium\", \"for\", \".\", \"\\\"\"], [\"a\", \"recent\", \"sale\", \"from\", \"the\", \"cellars\", \"of\", \"the\", \"uk\", \"government\", \"saw\", \"just\", \"that\", \"effect\", \",\", \"smith\", \"said\", \",\", \"with\", \"wines\", \"fetching\", \"more\", \"than\", \"might\", \"have\", \"been\", \"expected\", \".\"], [\"this\", \"was\", \"in\", \"part\", \"because\", \"of\", \"their\", \"good\", \"provenance\", \",\", \"he\", \"said\", \",\", \"but\", \"also\", \"because\", \"they\", \"\\\"\", \"have\", \"that\", \"slight\", \"cachet\", \"that\", \"wines\", \"from\", \"the\", \"same\", \"stock\", \"that\", \"'s\", \"been\", \"drunk\", \"by\", \"president\", \"obama\", \",\", \"or\", \"whoever\", \"'s\", \"drunk\", \"the\", \"poshest\", \"wine\", \"from\", \"the\", \"cellars\", \",\", \"have\", \".\", \"\\\"\"]], \"tgt\": [[\"1,200\", \"bottles\", \"from\", \"the\", \"wine\", \"cellar\", \"of\", \"the\", \"\\u00e9lys\\u00e9e\", \"palace\", \"are\", \"being\", \"auctioned\", \"off\"], [\"the\", \"money\", \"will\", \"pay\", \"for\", \"new\", \",\", \"\\\"\", \"more\", \"modest\", \"\\\"\", \"bottles\", \"to\", \"be\", \"bought\", \",\", \"says\", \"the\", \"auction\", \"house\"], [\"any\", \"extra\", \"funds\", \"will\", \"be\", \"poured\", \"into\", \"the\", \"state\", \"budget\", \",\", \"it\", \"says\"], [\"the\", \"presidential\", \"wine\", \"cellar\", \",\", \"set\", \"up\", \"in\", \"1947\", \",\", \"showcases\", \"some\", \"of\", \"france\", \"'s\", \"finest\", \"vintages\"]]}, {\"src\": [[\"the\", \"mother\", \"of\", \"an\", \"omaha\", \"toddler\", \"is\", \"defending\", \"her\", \"son\", \"after\", \"he\", \"unleashed\", \"a\", \"slew\", \"of\", \"obscenities\", \"in\", \"an\", \"online\", \"video\", \"that\", \"has\", \"gone\", \"viral\", \".\"], [\"in\", \"the\", \"video\", \",\", \"the\", \"diapered\", \"boy\", \"is\", \"taunted\", \"and\", \"cursed\", \"at\", \"by\", \"adults\", \",\", \"who\", \"coax\", \"him\", \"into\", \"using\", \"crude\", \"words\", \".\"], [\"the\", \"african-american\", \"toddler\", \"knocks\", \"down\", \"a\", \"chair\", \"and\", \"responds\", \"to\", \"some\", \"of\", \"the\", \"comments\", \"with\", \"a\", \"middle-finger\", \"salute\", \".\"], [\"\\\"\", \"shut\", \"up\", \",\", \"bitch\", \",\", \"\\\"\", \"he\", \"says\", \"in\", \"one\", \"of\", \"the\", \"responses\", \".\"], [\"the\", \"adults\", \"chuckle\", \",\", \"prompting\", \"him\", \"to\", \"unleash\", \"more\", \"obscenities\", \"at\", \"them\", \".\"], [\"despite\", \"the\", \"video\", \",\", \"he\", \"'s\", \"not\", \"an\", \"anomaly\", \",\", \"according\", \"to\", \"his\", \"mother\", \".\"], [\"\\\"\", \"he\", \"had\", \"a\", \"clean\", \"diaper\", \",\", \"the\", \"house\", \"was\", \"clean\", \"and\", \"like\", \"they\", \"said\", \",\", \"kids\", \"curse\", \",\", \"every\", \"kid\", \"does\", \"it\", \",\", \"\\\"\", \"the\", \"mother\", \"told\", \"cnn\", \"affiliate\", \"ketv\", \"in\", \"an\", \"exclusive\", \"interview\", \".\"], [\"cnn\", \"does\", \"not\", \"identify\", \"juveniles\", \"in\", \"such\", \"stories\", \".\"], [\"the\", \"mother\", \"is\", \"16\", \".\"], [\"\\\"\", \"he\", \"'s\", \"a\", \"smart\", \"little\", \"boy\", \".\"], [\"all\", \"that\", \"cussing\", \"that\", \"he\", \"did\", \",\", \"he\", \"does\", \"n't\", \"do\", \"that\", \",\", \"\\\"\", \"she\", \"said\", \".\"], [\"\\\"\", \"somebody\", \"told\", \"him\", \"to\", \"do\", \"that\", \".\"], [\"my\", \"son\", \"does\", \"n't\", \"do\", \"that\", \".\"], [\"i\", \"do\", \"n't\", \"allow\", \"it\", \".\", \"\\\"\"], [\"she\", \"said\", \"a\", \"friend\", \"of\", \"her\", \"brother\", \"filmed\", \"the\", \"video\", \"while\", \"she\", \"was\", \"in\", \"another\", \"room\", \".\"], [\"\\\"\", \"he\", \"was\", \"wrong\", \"for\", \"doing\", \"that\", \"...\", \"posting\", \"the\", \"video\", \"up\", \"and\", \"getting\", \"us\", \"into\", \"this\", \"situation\", \",\", \"\\\"\", \"she\", \"said\", \".\"], [\"\\\"\", \"everybody\", \"that\", \"thinks\", \"i\", \"'m\", \"a\", \"bad\", \"mother\", \",\", \"i\", \"'m\", \"not\", \".\"], [\"i\", \"'m\", \"a\", \"good\", \"mother\", \"to\", \"my\", \"son\", \".\"], [\"i\", \"teach\", \"him\", \"a\", \"lot\", \".\"], [\"he\", \"'s\", \"very\", \"smart\", \".\", \"\\\"\"], [\"the\", \"police\", \"union\", \"in\", \"omaha\", \",\", \"nebraska\", \",\", \"posted\", \"the\", \"clip\", \"on\", \"its\", \"website\", \"to\", \"highlight\", \"what\", \"it\", \"called\", \"the\", \"\\\"\", \"cycle\", \"of\", \"violence\", \"and\", \"thuggery\", \"\\\"\", \"the\", \"community\", \"faces\", \".\"], [\"the\", \"omaha\", \"police\", \"officers\", \"'\", \"association\", \"is\", \"under\", \"fire\", \"from\", \"the\", \"city\", \"'s\", \"police\", \"chief\", \",\", \"the\", \"aclu\", \"and\", \"at\", \"least\", \"one\", \"community\", \"leader\", \".\"], [\"they\", \"say\", \"the\", \"move\", \"needlessly\", \"antagonizes\", \"minority\", \"communities\", \",\", \"which\", \"make\", \"up\", \"about\", \"a\", \"quarter\", \"of\", \"omaha\", \"'s\", \"409,000\", \"residents\", \".\"], [\"sgt.\", \"john\", \"wells\", \",\", \"the\", \"union\", \"'s\", \"president\", \",\", \"said\", \"the\", \"video\", \"was\", \"\\\"\", \"disturbing\", \"\\\"\", \"and\", \"\\\"\", \"offensive\", \".\", \"\\\"\"], [\"\\\"\", \"the\", \"focus\", \"here\", \"is\", \"n't\", \"on\", \"any\", \"particular\", \"ethnic\", \"group\", \".\"], [\"the\", \"focus\", \"here\", \"is\", \"on\", \"the\", \"troubling\", \"behavior\", \"toward\", \"this\", \"child\", \",\", \"\\\"\", \"wells\", \"said\", \".\"], [\"\\\"\", \"this\", \"behavior\", \"is\", \"going\", \"to\", \"potentially\", \"lead\", \"this\", \"child\", \"down\", \"a\", \"path\", \"that\", \"is\", \"completely\", \"unhealthy\", \".\", \"\\\"\"], [\"on\", \"the\", \"website\", \"where\", \"the\", \"video\", \"is\", \"posted\", \",\", \"the\", \"union\", \"said\", \"the\", \"clip\", \"came\", \"from\", \"\\\"\", \"a\", \"local\", \"thug\", \"'s\", \"public\", \"facebook\", \"page\", \".\", \"\\\"\"], [\"\\\"\", \"we\", \"here\", \"at\", \"omahapoa.com\", \"viewed\", \"the\", \"video\", \"and\", \"we\", \"knew\", \"that\", \"despite\", \"the\", \"fact\", \"that\", \"it\", \"is\", \"sickening\", \",\", \"heartbreaking\", \"footage\", \",\", \"we\", \"have\", \"an\", \"obligation\", \"to\", \"share\", \"it\", \"to\", \"continue\", \"to\", \"educate\", \"the\", \"law\", \"abiding\", \"public\", \"about\", \"the\", \"terrible\", \"cycle\", \"of\", \"violence\", \"and\", \"thuggery\", \"that\", \"some\", \"young\", \"innocent\", \"children\", \"find\", \"themselves\", \"helplessly\", \"trapped\", \"in\", \",\", \"\\\"\", \"the\", \"police\", \"union\", \"wrote\", \"in\", \"a\", \"post\", \"accompanying\", \"the\", \"video\", \".\"], [\"\\\"\", \"now\", \"while\", \"we\", \"did\", \"n't\", \"see\", \"anything\", \"in\", \"this\", \"video\", \"that\", \"is\", \"blatantly\", \"`\", \"illegal\", \",\", \"'\", \"we\", \"sure\", \"did\", \"see\", \"a\", \"lot\", \"that\", \"is\", \"flat\", \"out\", \"immoral\", \"and\", \"completely\", \"unhealthy\", \"for\", \"this\", \"little\", \"child\", \"from\", \"a\", \"healthy\", \"upbringing\", \"standpoint\", \",\", \"\\\"\", \"it\", \"added\", \".\"], [\"wells\", \"said\", \"one\", \"of\", \"the\", \"adults\", \"mentions\", \"a\", \"local\", \"street\", \"gang\", \"in\", \"the\", \"video\", \".\"], [\"\\\"\", \"that\", \"is\", \"why\", \"when\", \"we\", \"talk\", \"about\", \"the\", \"culture\", \",\", \"the\", \"criminal\", \"culture\", \",\", \"that\", \"this\", \"is\", \"to\", \"try\", \"to\", \"break\", \"the\", \"cycle\", \"and\", \"deal\", \"with\", \"the\", \"culture\", \"of\", \"violence\", \"and\", \"the\", \"culture\", \"of\", \"gang\", \"activity\", \",\", \"\\\"\", \"he\", \"said\", \".\"], [\"willie\", \"hamilton\", \",\", \"president\", \"of\", \"the\", \"community\", \"activist\", \"group\", \"black\", \"men\", \"united\", \",\", \"said\", \"the\", \"union\", \"crossed\", \"a\", \"line\", \".\"], [\"\\\"\", \"for\", \"them\", \"to\", \"take\", \"a\", \"video\", \"out\", \"of\", \"context\", \"--\", \"a\", \"2-year-old\", \"who\", \"does\", \"n't\", \"have\", \"the\", \"brain\", \"capacity\", \"to\", \"know\", \"what\", \"'s\", \"going\", \"on\", \"--\", \"and\", \"to\", \"say\", \"that\", \"this\", \"child\", \",\", \"because\", \"two\", \"adults\", \"acted\", \"inappropriately\", \",\", \"is\", \"going\", \"to\", \"end\", \"up\", \"in\", \"a\", \"life\", \"of\", \"crime\", \"is\", \"totally\", \"inappropriate\", \",\", \"\\\"\", \"hamilton\", \"said\", \".\"], [\"the\", \"american\", \"civil\", \"liberties\", \"union\", \"of\", \"nebraska\", \",\", \"which\", \"filed\", \"an\", \"excessive\", \"force\", \"suit\", \"against\", \"the\", \"omaha\", \"police\", \"department\", \"on\", \"behalf\", \"of\", \"an\", \"african-american\", \"family\", \"monday\", \",\", \"said\", \"the\", \"union\", \"'s\", \"use\", \"of\", \"\\\"\", \"racially\", \"charged\", \"language\", \"\\\"\", \"was\", \"\\\"\", \"very\", \"disconcerting\", \".\", \"\\\"\"], [\"\\\"\", \"officers\", \"should\", \"be\", \"working\", \"to\", \"build\", \"a\", \"culture\", \"where\", \"anyone\", \"feels\", \"comfortable\", \"calling\", \"law\", \"enforcement\", \",\", \"\\\"\", \"aclu\", \"of\", \"nebraska\", \"executive\", \"director\", \"becki\", \"brenner\", \"said\", \"in\", \"a\", \"prepared\", \"statement\", \".\"], [\"\\\"\", \"the\", \"manner\", \"in\", \"which\", \"the\", \"officers\", \"association\", \"has\", \"discussed\", \"this\", \"incident\", \"has\", \"done\", \"nothing\", \"but\", \"further\", \"erode\", \"community\", \"trust\", \"and\", \"reinforce\", \"the\", \"need\", \"for\", \"independent\", \"oversight\", \",\", \"trainings\", \",\", \"and\", \"other\", \"reforms\", \".\", \"\\\"\"], [\"police\", \"chief\", \"todd\", \"schmaderer\", \"tried\", \"to\", \"distance\", \"his\", \"agency\", \"from\", \"the\", \"controversy\", \"tuesday\", \",\", \"saying\", \"that\", \"the\", \"union\", \"'s\", \"website\", \"and\", \"facebook\", \"page\", \"are\", \"separate\", \"from\", \"those\", \"of\", \"the\", \"omaha\", \"police\", \"department\", \".\"], [\"he\", \"said\", \"he\", \"has\", \"little\", \"authority\", \"over\", \"the\", \"public\", \"statements\", \"of\", \"union\", \"members\", \".\"], [\"\\\"\", \"with\", \"that\", \"background\", \"and\", \"understanding\", \",\", \"i\", \"want\", \"to\", \"make\", \"it\", \"explicit\", \"and\", \"clear\", \"that\", \"the\", \"views\", \"expressed\", \"on\", \"the\", \"opoa\", \"facebook\", \"page\", \"do\", \"not\", \"necessarily\", \"reflect\", \"the\", \"official\", \"stance\", \"of\", \"the\", \"omaha\", \"police\", \"department\", \",\", \"\\\"\", \"schmaderer\", \"said\", \".\"], [\"\\\"\", \"i\", \"strongly\", \"disagree\", \"with\", \"any\", \"postings\", \"that\", \"may\", \"cause\", \"a\", \"divide\", \"in\", \"our\", \"community\", \"or\", \"an\", \"obstacle\", \"to\", \"police\", \"community\", \"relations\", \".\", \"\\\"\"], [\"wells\", \"said\", \"union\", \"members\", \"have\", \"turned\", \"the\", \"video\", \"over\", \"to\", \"the\", \"department\", \"'s\", \"child\", \"victim\", \"unit\", \".\"], [\"the\", \"child\", \"and\", \"his\", \"mother\", \"are\", \"in\", \"protective\", \"custody\", \"for\", \"safety\", \"reasons\", \".\"], [\"court\", \"records\", \"obtained\", \"by\", \"the\", \"affiliate\", \"show\", \"the\", \"toddler\", \"was\", \"among\", \"five\", \"injured\", \"in\", \"october\", \"when\", \"shots\", \"were\", \"fired\", \"at\", \"a\", \"home\", \".\"], [\"the\", \"boy\", \"and\", \"his\", \"mother\", \"have\", \"been\", \"relocated\", \"by\", \"the\", \"state\", \"in\", \"the\", \"past\", \"over\", \"gang\", \"activity\", \"fears\", \",\", \"according\", \"to\", \"the\", \"affiliate\", \".\"], [\"all\", \"that\", \"cussing\", \"that\", \"he\", \"did\", \",\", \"he\", \"does\", \"n't\", \"do\", \"that\", \",\", \"\\\"\", \"mom\", \"says\"]], \"tgt\": [[\"video\", \"shows\", \"toddler\", \"bombarded\", \"with\", \"obscenities\", \",\", \"coaxed\", \"to\", \"respond\", \"in\", \"kind\"], [\"\\\"\", \"he\", \"'s\", \"a\", \"smart\", \"little\", \"boy\", \".\"], [\"mom\", \"says\", \"as\", \"long\", \"as\", \"his\", \"diaper\", \"and\", \"house\", \"were\", \"clean\", \",\", \"there\", \"was\", \"no\", \"reason\", \"to\", \"worry\"], [\"aclu\", \"and\", \"african-american\", \"leaders\", \"blast\", \"the\", \"video\"]]}, {\"src\": [[\"by\"], [\"richard\", \"shears\"], [\"published\", \":\"], [\"22:02\", \"est\", \",\", \"27\", \"august\", \"2012\"], [\"|\"], [\"updated\", \":\"], [\"06:45\", \"est\", \",\", \"28\", \"august\", \"2012\"], [\"a\", \"pair\", \"of\", \"identical\", \"twins\", \",\", \"who\", \"became\", \"famous\", \"through\", \"their\", \"desperate\", \"battle\", \"with\", \"anorexia\", \",\", \"have\", \"died\", \"in\", \"a\", \"house\", \"fire\", \".\"], [\"clare\", \"and\", \"rachel\", \"wallmeyer\", \",\", \"42\", \",\", \"were\", \"killed\", \"after\", \"a\", \"fire\", \"broke\", \"out\", \"in\", \"their\", \"home\", \"in\", \"geelong\", \",\", \"near\", \"melbourne\", \",\", \"one\", \"perishing\", \"in\", \"the\", \"flames\", \",\", \"the\", \"other\", \"succumbing\", \"to\", \"her\", \"severe\", \"burns\", \"on\", \"the\", \"way\", \"to\", \"hospital\", \".\"], [\"it\", \"was\", \"a\", \"tragic\", \"end\", \"to\", \"two\", \"turbulent\", \"lives\", \",\", \"for\", \"the\", \"sisters\", \"had\", \"appeared\", \"on\", \"australian\", \"tv\", \"several\", \"times\", \"to\", \"talk\", \"about\", \"the\", \"anorexia\", \"which\", \"had\", \"turned\", \"both\", \"into\", \"virtual\", \"living\", \"skeletons\", \"and\", \"a\", \"problem\", \"pair\", \"for\", \"their\", \"parents\", \",\", \"social\", \"workers\", \"and\", \"the\", \"police\", \".\"], [\"scroll\", \"down\", \"for\", \"video\"], [\"tragic\", \":\", \"identical\", \"twins\", \"clare\", \"and\", \"rachel\", \"wallmeyer\", \",\", \"42\", \",\", \"died\", \"when\", \"fire\", \"swept\", \"through\", \"their\", \"home\", \"in\", \"geelong\", \",\", \"near\", \"melbourne\"], [\"`\", \"at\", \"least\", \"we\", \"'ll\", \"die\", \"together\", \"'\", \":\", \"twins\", \"predicted\", \"their\", \"sad\", \"demise\"], [\"in\", \"a\", \"poignant\", \"review\", \"of\", \"their\", \"lives\", \"they\", \"said\", \"in\", \"recent\", \"years\", \"that\", \"they\", \"had\", \"never\", \"been\", \"in\", \"love\", \",\", \"never\", \"had\", \"a\", \"job\", \"and\", \"they\", \"believed\", \"that\", \"it\", \"was\", \"only\", \"a\", \"matter\", \"of\", \"time\", \"before\", \"they\", \"died\", \"--\", \"and\", \"they\", \"would\", \"die\", \"together\", \".\"], [\"their\", \"deaths\", \"in\", \"the\", \"fire\", \"are\", \"believed\", \"to\", \"have\", \"been\", \"accidental\", \",\", \"according\", \"to\", \"detectives\", \"from\", \"the\", \"geelong\", \"crime\", \"investigation\", \"unit\", \"who\", \"said\", \"that\", \"initial\", \"checks\", \"did\", \"not\", \"reveal\", \"any\", \"suspicious\", \"activity\", \".\"], [\"yet\", \"there\", \"had\", \"been\", \"reports\", \"over\", \"the\", \"years\", \"of\", \"the\", \"women\", \"each\", \"trying\", \"to\", \"kill\", \"one\", \"another\", \".\"], [\"rachel\", \"was\", \"charged\", \"with\", \"the\", \"attempted\"], [\"murder\", \"of\", \"clare\", \"after\", \"police\", \",\", \"who\", \"were\", \"called\", \"to\", \"their\", \"home\", \",\", \"claimed\"], [\"they\", \"witnessed\", \"rachel\", \"with\", \"her\", \"hands\", \"locked\", \"around\", \"her\", \"sister\", \"'s\", \"throat\", \".\"], [\"the\", \"charge\", \"was\", \"later\", \"withdrawn\", \".\"], [\"their\", \"existence\", \",\", \"balanced\", \"between\"], [\"life\", \"and\", \"death\", \",\", \"had\", \"resulted\", \"in\", \"tv\", \"companies\", \"searching\", \"them\", \"out\", \"for\"], [\"interviews\", \"after\", \"authorities\", \"considered\", \"jailing\", \"them\", \"in\", \"an\", \"attempt\", \"to\"], [\"stop\", \"the\", \"women\", \"starving\", \"themselves\", \"to\", \"death\", \"and\", \"`\", \"turn\", \"their\", \"lives\"], [\"around\", \".\", \"'\"], [\"in\", \"fact\", \"clare\"], [\"was\", \"later\", \"jailed\", \"by\", \"a\", \"geelong\", \"court\", \"for\", \"a\", \"series\", \"of\", \"thefts\", \"--\", \"but\", \"only\"], [\"after\", \"magistrate\", \"ian\", \"von\", \"einem\", \"said\", \"he\", \"saw\", \"no\", \"option\", \"but\", \"to\", \"send\", \"her\", \"to\"], [\"prison\", \"to\", \"stop\", \"her\", \"from\", \"self-destructing\", \".\"], [\"her\"], [\"sister\", \"also\", \"presented\", \"a\", \"headache\", \"for\", \"the\", \"authorities\", \"when\", \"she\", \"was\"], [\"arrested\", \"for\", \"driving\", \"under\", \"the\", \"influence\", \"of\", \"drugs\", \"and\", \"was\", \"also\", \"accused\"], [\"of\", \"pushing\", \"a\", \"victim\", \"on\", \"to\", \"train\", \"tracks\", \".\"], [\"she\", \"received\", \"a\", \"21-month\", \"suspended\"], [\"jail\", \"sentence\", \".\"], [\"the\"], [\"women\", \",\", \"who\", \"were\", \"compulsive\", \"long-distance\", \"runners\", \",\", \"described\", \"themselves\"], [\"as\", \"perfectionists\", \"in\", \"biomedical\", \"science\", \"and\", \"physical\", \"education\", \",\", \"topics\"], [\"they\", \"studied\", \"avidly\", \",\", \"side\", \"by\", \"side\", \".\"], [\"but\", \"as\", \"they\", \"started\", \"to\", \"waste\", \"away\"], [\"over\", \"two\", \"decades\", \",\", \"the\", \"weight\", \"of\", \"each\", \"of\", \"them\", \"dropped\", \"to\", \"little\", \"more\", \"than\"], [\"four\", \"stone\", \".\"], [\"doctors\", \"said\", \"they\", \"had\", \"the\", \"bone\", \"structure\", \"of\", \"women\", \"aged\"], [\"between\", \"70\", \"and\", \"100\", \".\"], [\"the\"], [\"twins\", \"developed\", \"severe\", \"eating\", \"disorders\", \"in\", \"their\", \"early\", \"teens\", \"losing\", \"more\"], [\"weight\", \"when\", \"they\", \"became\", \"addicted\", \"to\", \"long-distance\", \"running\", \".\"], [\"they\", \"were\", \"so\"], [\"obsessed\", \"with\", \"marathons\", \"that\", \"they\", \"each\", \"suffered\", \"stress\", \"fractures\", \"in\"], [\"their\", \"feet\", \".\"], [\"problem\", \"pair\", \":\", \"clare\", \"and\", \"rachel\", \"wallmeyer\", \"pictured\", \"leaving\", \"a\", \"police\", \"station\", \"in\", \"geelong\", \",\", \"australia\", \"in\", \"2007\"], [\"blaze\", \":\", \"the\", \"home\", \"in\", \"geelong\", \",\", \"australia\", \",\", \"where\", \"the\", \"anorexic\", \"twin\", \"sisters\", \"died\"], [\"inseparable\", \"to\", \"their\", \"tragic\", \"end\", \",\"], [\"there\", \"was\", \"the\", \"time\", \"in\", \"1996\", \"when\", \"rachel\", \"was\", \"so\", \"ill\", \"she\", \"was\", \"admitted\", \"to\", \"a\"], [\"psychiatric\", \"unit\", \"at\", \"the\", \"royal\", \"melbourne\", \"hospital\", \"--\", \"followed\", \"by\", \"clare\", \"who\"], [\"voluntarily\", \"admitted\", \"herself\", \"too\", \".\"], [\"rachel\", \"told\", \"melbourne\", \"'s\", \"herald\", \"sun\"], [\"newspaper\", \"on\", \"one\", \"occasion\", \"that\", \"no-one\", \"understood\", \"anorexia\", \"until\", \"they\"], [\"have\", \"lived\", \"it\", \".\"], [\"`\", \"it\", \"'s\", \"like\", \"the\", \"grim\", \"reaper\", \"--\", \"a\", \"black\", \"hole\", \"in\", \"your\", \"soul\", \",\", \"'\"], [\"she\", \"said\", \".\"], [\"their\"], [\"parents\", \",\", \"bob\", \"and\", \"moya\", \"admitted\", \"that\", \"when\", \"the\", \"twins\", \"were\", \"teenagers\", \"they\"], [\"feared\", \"they\", \"would\", \"find\", \"them\", \"dead\", \"in\", \"bed\", \"because\", \"of\", \"their\", \"disorder\", \".\"], [\"happier\", \"times\", \":\", \"but\", \"behind\", \"the\", \"smiles\", \"the\", \"two\", \"sisters\", \"led\", \"a\", \"turbulent\", \",\", \"troubled\", \"existence\", \".\"], [\"pictured\", \"with\", \"friend\", \"rachael\", \"walker\"], [\"desperate\", \":\", \"authorities\", \"considered\", \"jailing\", \"them\", \"in\", \"an\", \"attempt\", \"to\", \"stop\", \"rachel\", \",\", \"left\", \",\", \"and\", \"clare\", \",\", \"right\", \",\", \"starving\", \"themselves\", \"to\", \"death\"], [\"in\", \"an\", \"interview\", \"with\", \"australia\", \"'s\", \"60\", \"minutes\", \"programme\", \"the\", \"twins\", \"gave\", \"a\", \"startling\", \"insight\", \"into\", \"their\", \"eating\", \"habits\", \".\"], [\"said\", \"clare\", \":\", \"`\", \"essentially\", \",\", \"we\", \"do\", \"n't\", \"eat\", \"anything\", \".\"], [\"we\", \"might\", \"have\", \"a\", \"piece\", \"of\", \"watermelon\", \".\", \"'\"], [\"rachel\", \"added\", \":\", \"`\", \"and\", \"diet\", \"coke\", \"we\", \"have\", \",\", \"and\", \"coffee\", \".\", \"'\"], [\"they\", \"also\", \"revealed\", \"they\", \"took\", \"at\", \"least\", \"20\", \"laxatives\", \".\"], [\"rachel\", \"said\", \"that\", \"clare\", \"was\", \"the\", \"only\", \"person\", \"who\", \"remained\", \"by\", \"her\", \"side\", \".\"], [\"`\", \"and\", \"at\", \"least\", \"we\", \"'ll\", \"die\", \"together\", \".\", \"'\"], [\"clare\", \"said\", \":\", \"`\", \"being\", \"with\", \"rachel\", \"...\", \"makes\", \"it\", \"somewhat\", \"easier\", \"to\", \"die\", \".\", \"'\"]], \"tgt\": [[\"clare\", \"and\", \"rachel\", \"wallmeyer\", \",\", \"42\", \",\", \"died\", \"when\", \"fire\", \"swept\", \"through\", \"their\", \"home\", \"in\", \"geelong\", \",\", \"near\", \"melbourne\"], [\"one\", \"died\", \"in\", \"the\", \"flames\", \"while\", \"the\", \"other\", \"succumbed\", \"to\", \"severe\", \"burns\", \"on\", \"the\", \"way\", \"to\", \"hospital\"], [\"initial\", \"investigation\", \"suggests\", \"fire\", \"was\", \"not\", \"suspicious\", \"but\", \"twins\", \"had\", \"tried\", \"to\", \"kill\", \"one\", \"another\", \"before\"], [\"pair\", \"were\", \"addicted\", \"to\", \"long\", \"distance\", \"running\", \"and\", \"fractured\", \"their\", \"feet\", \"endlessly\", \"attempting\", \"marathons\"], [\"authorities\", \"considered\", \"locking\", \"sisters\", \"up\", \"to\", \"stop\", \"them\", \"starving\", \"themselves\", \"as\", \"each\", \"weighed\", \"little\", \"more\", \"than\", \"four\", \"stone\"], [\"twins\", \"predicted\", \"they\", \"would\", \"`\", \"die\", \"together\", \"'\", \"as\", \"they\", \"transformed\", \"into\", \"living\", \"skeletons\"]]}, {\"src\": [[\"by\"], [\"christopher\", \"stevens\"], [\"almost\", \"everything\", \"you\", \"could\", \"wish\", \"for\", \"from\", \"a\", \"new\", \"sitcom\", \"is\", \"crammed\", \"into\", \"family\", \"tree\", \"(\", \"bbc2\", \")\", \".\"], [\"it\", \"'s\", \"co-written\", \"by\", \"christopher\", \"guest\", \",\", \"who\", \"pioneered\", \"comedy\", \"mock-umentaries\", \"with\", \"this\", \"is\", \"spinal\", \"tap\", \",\", \"hailed\", \"by\", \"many\", \"film-lovers\", \"as\", \"the\", \"funniest\", \"movie\", \"ever\", \"made\", \".\"], [\"co-produced\", \"by\", \"the\", \"u.s.\", \"cable\", \"channel\", \"hbo\", \"(\", \"home\", \"of\", \"the\", \"sopranos\", \")\", \",\", \"family\", \"tree\", \"features\", \"a\", \"zany\", \"cast\", \"of\", \"characters\", \",\", \"a\", \"coherent\", \"storyline\", \"and\", \",\", \"in\", \"chris\", \"o'dowd\", \",\", \"a\", \"rising\", \"hollywood\", \"star\", \"in\", \"the\", \"main\", \"role\", \".\"], [\"no\", \"laughs\", \":\", \"family\", \"tree\", \"features\", \"rising\", \"hollywood\", \"star\", \"chris\", \"o'dowd\", \"(\", \"left\", \")\", \"as\", \"tom\", \"chadwick\", \"and\", \"ventriloquist\", \"nina\", \"conti\", \"(\", \"right\", \")\", \",\", \"as\", \"the\", \"hero\", \"'s\", \"sister\", \"bea\", \",\", \"who\", \"is\", \"obsessed\", \"with\", \"a\", \"glove\", \"puppet\", \"called\", \"monkey\"], [\"this\", \"surgically\", \"engineered\", \"sitcom\", \"made\", \"just\", \"one\", \"slip\", \"in\", \"its\", \"pilot\", \"episode\", \".\"], [\"it\", \"forgot\", \"to\", \"leave\", \"room\", \"for\", \"the\", \"jokes\", \".\"], [\"the\", \"script\", \"is\", \"mainly\", \"improvised\", \",\", \"in\", \"the\", \"style\", \"of\", \"outnumbered\", \"or\", \"the\", \"american\", \"comedy\", \"curb\", \"your\", \"enthusiasm\", \".\"], [\"but\", \"in\", \"the\", \"flurry\", \"of\", \"expositions\", \"and\", \"introductions\", \"last\", \"night\", \",\", \"laying\", \"out\", \"the\", \"plotlines\", \",\", \"the\", \"actors\", \"had\", \"no\", \"time\", \"to\", \"make\", \"us\", \"laugh\", \".\"], [\"a\", \"sizeable\", \"slice\", \"of\", \"the\", \"episode\", \"was\", \"devoted\", \"to\", \"explaining\", \"why\", \"o'dowd\", \"--\", \"born\", \"in\", \"the\", \"emerald\", \"isle\", \"--\", \"has\", \"a\", \"rural\", \"irish\", \"accent\", \"when\", \"his\", \"sister\", \",\", \"father\", \"and\", \"best\", \"mate\", \"were\", \"all\", \"patently\", \"londoners\", \".\"], [\"instead\", \"of\", \"boring\", \"us\", \"with\", \"a\", \"laboured\", \"story\", \"of\", \"the\", \"character\", \"'s\", \"single-parent\", \"upbringing\", \"in\", \"ireland\", \",\", \"guest\", \"should\", \"have\", \"shrugged\", \"off\", \"the\", \"anomaly\", \"and\", \"left\", \"us\", \"to\", \"invent\", \"our\", \"own\", \"explanations\", \".\"], [\"in\", \"an\", \"interview\", \"to\", \"promote\", \"the\", \"show\", \",\", \"he\", \"was\", \"much\", \"wittier\", \".\"], [\"announcing\", \"that\", \"he\", \"was\", \"`\", \"delighted\", \"to\", \"welcome\", \"myself\", \"to\", \"bbc\", \"television\", \"'\", \",\", \"guest\", \"added\", \"that\", \"`\", \"chris\", \"o'dowd\", \"has\", \"been\", \"on\", \"my\", \"radar\", \"since\", \"he\", \"was\", \"a\", \"child\", \"actor\", \"in\", \"wales\", \"'\", \".\"], [\"it\", \"'s\", \"always\", \"a\", \"worry\", \"when\", \"the\", \"publicity\", \"material\", \"is\", \"funnier\", \"than\", \"the\", \"show\", \"itself\", \".\"], [\"family\", \"tree\", \"does\", \"have\", \"potential\", \".\"], [\"ventriloquist\", \"nina\", \"conti\", \",\", \"as\", \"the\", \"hero\", \"'s\", \"sister\", \"bea\", \",\", \"is\", \"obsessed\", \"with\", \"a\", \"glove\", \"puppet\", \"called\", \"monkey\", \",\", \"who\", \"delivers\", \"a\", \"tart\", \"commentary\", \"on\", \"the\", \"other\", \"characters\", \".\"], [\"their\", \"father\", \",\", \"keith\", \",\", \"has\", \"remarried\", \",\", \"to\", \"an\", \"east\", \"european\", \"peasant\", \"called\", \"luba\", \".\"], [\"keith\", \"is\", \"obsessed\", \"with\", \"forgotten\", \"seventies\", \"sitcoms\", \",\", \"with\", \"titles\", \"like\", \"there\", \"goes\", \"the\", \"neighbourhood\", \",\", \"which\", \"guest\", \"carefully\", \"invents\", \"and\", \"shoots\", \",\", \"so\", \"that\", \"clips\", \"can\", \"be\", \"dropped\", \"into\", \"the\", \"show\", \".\"], [\"that\", \"'s\", \"a\", \"painstaking\", \"approach\", \"to\", \"comedy\", \"which\", \"is\", \"n't\", \"actually\", \"funny\", \".\"], [\"a\", \"bit\", \"like\", \"this\", \"whole\", \"show\", \",\", \"in\", \"fact\", \".\"], [\"painstaking\", \"is\", \"an\", \"inadequate\", \"word\", \"to\", \"describe\", \"the\", \"care\", \"that\", \"went\", \"into\", \"designing\", \"princess\", \"diana\", \"'s\", \"wardrobe\", \".\"], [\"she\", \"expressed\", \"her\", \"whole\", \"life\", \"through\", \"the\", \"medium\", \"of\", \"her\", \"clothes\", \".\"], [\"princess\", \"diana\", \"'s\", \"dresses\", \":\", \"the\", \"auction\"], [\"(\", \"c4\", \")\", \"was\", \"a\", \"brilliant\", \"way\", \"to\", \"retell\", \"her\", \"biography\", \".\"], [\"taking\", \"ten\", \"dresses\"], [\"recently\", \"put\", \"up\", \"for\", \"sale\", \"with\", \"a\", \"guide\", \"price\", \"of\", \"\\u00a3\", \"1\", \"million\", \",\", \"this\"], [\"absorbing\", \"documentary\", \"followed\", \"her\", \"evolution\", \"from\", \"gauche\", \",\", \"19-year-old\"], [\"nanny\", \"to\", \"landmine\", \"campaigner\", \"and\", \"people\", \"'s\", \"princess\", \".\"], [\"famous\", \":\", \"in\", \"channel\", \"4\", \"'s\", \"princess\", \"diana\", \"'s\", \"dresses\", \":\", \"the\", \"auction\", \",\", \"there\", \"was\", \"the\", \"dress\", \"from\", \"that\", \"night\", \"at\", \"the\", \"white\", \"house\", \"in\", \"1985\", \"when\", \"diana\", \"danced\", \"with\", \"john\", \"travolta\"], [\"it\", \"revealed\", \"the\", \"fascinating\", \"story\", \"behind\", \"that\", \"frumpy\", \"blue\", \"suit\", \"she\", \"wore\", \"for\", \"her\", \"engagement\", \"photos\", \"and\", \"the\", \"notorious\", \"interview\", \"where\", \"her\", \"fiance\", \",\", \"charles\", \",\", \"harrumphed\", \"about\", \"`\", \"whatever\", \"\\\"\", \"in\", \"love\", \"\\\"\", \"means\", \"'\", \".\"], [\"announcing\", \"herself\", \"simply\", \"as\", \"diana\", \"spencer\", \",\", \"she\", \"had\", \"paid\", \"a\", \"call\", \"in\", \"her\", \"marks\", \"&\", \"sparks\", \"daywear\", \"to\", \"leading\", \"designer\", \"david\", \"sassoon\", \",\", \"whose\", \"formidable\", \"shop\", \"assistant\", \"had\", \"taken\", \"one\", \"look\", \"and\", \"sent\", \"her\", \"packing\", \".\"], [\"`\", \"if\", \"you\", \"must\", \"have\", \"something\", \"special\", \",\", \"'\", \"the\", \"dragon\", \"snarled\", \",\", \"`\", \"go\", \"to\", \"harrods\", \"!\", \"'\"], [\"so\", \"diana\", \"did\", \",\", \"and\", \"the\", \"result\", \"was\", \"an\", \"outfit\", \"that\", \"was\", \"more\", \"mrs\", \"thatcher\", \"than\", \"fairytale\", \"princess\", \".\"], [\"just\", \"as\", \"intriguing\", \"was\", \"the\", \"tale\", \"of\", \"the\", \"little\", \"black\", \"dress\", \"that\", \",\", \"weeks\", \"later\", \",\", \"announced\", \"her\", \"to\", \"the\", \"world\", \"as\", \"a\", \"fashion\", \"icon\", \".\"], [\"`\", \"diana\", \"the\", \"dazzler\", \"'\", \"gasped\", \"the\", \"tabloids\", \",\", \"as\", \"she\", \"was\", \"photographed\", \"with\", \"her\", \"cleavage\", \"spilling\", \"out\", \"of\", \"the\", \"low-cut\", \"cups\", \",\", \"designed\", \"by\", \"david\", \"and\", \"elizabeth\", \"emanuel\", \".\"], [\"diana\", \"'s\", \"image-maker\", \"was\", \"anna\", \"harvey\", \"of\", \"vogue\", \".\"], [\"`\", \"if\", \"elizabeth\", \"emanuel\", \"will\", \"forgive\", \"me\", \",\", \"that\", \"dress\", \"may\", \"not\", \"have\", \"fitted\", \"perfectly\", \",\", \"'\", \"sniffed\", \"harvey\", \".\"], [\"`\", \"nobody\", \"allowed\", \"that\", \"to\", \"happen\", \"a\", \"second\", \"time\", \".\", \"'\"], [\"there\", \"was\", \"the\", \"dress\", \",\", \"too\", \",\", \"from\", \"that\", \"night\", \"at\", \"the\", \"white\", \"house\", \"when\", \"diana\", \"danced\", \"with\", \"john\", \"travolta\", \".\"], [\"but\", \"who\", \"remembers\", \"that\", \"she\", \"also\", \"waltzed\", \"with\", \"clint\", \"eastwood\", \",\", \"tom\", \"selleck\", \"and\", \"neil\", \"diamond\", \"?\"], [\"as\", \"one\", \"acid-tongued\", \"politico\", \"commented\", \",\", \"no\", \"one\", \"even\", \"noticed\", \"that\", \"president\", \"ronald\", \"reagan\", \"was\", \"there\", \".\"], [\".\"], [\".\"], [\"or\", \"prince\", \"charles\", \".\"], [\"charles\", \"showed\", \"a\", \"flash\", \"of\", \"his\", \"nasty\", \"side\", \"at\", \"the\", \"next\", \"day\", \"'s\", \"press\", \"conference\", \".\"], [\"asked\", \"if\", \"his\", \"wife\", \"had\", \"enjoyed\", \"the\", \"ball\", \",\", \"he\", \"seethed\", \":\", \"`\", \"i\", \"'m\", \"not\", \"a\", \"glove\", \"puppet\", \"!\"], [\"she\", \"'d\", \"be\", \"an\", \"idiot\", \"if\", \"she\", \"did\", \"n't\", \"enjoy\", \"dancing\", \"with\", \"john\", \"travolta\", \".\", \"'\"], [\"who\", \"would\", \"have\", \"thought\", \"a\", \"few\", \"yards\", \"of\", \"taffeta\", \",\", \"silk\", \"and\", \"satin\", \"could\", \"reveal\", \"so\", \"much\", \"?\"], [\"rating\", \":\"]], \"tgt\": [[\"family\", \"tree\", \"(\", \"bbc2\", \")\"], [\"princess\", \"diana\", \"'s\", \"dresses\", \":\", \"the\", \"auction\", \"(\", \"c4\", \")\", \"rating\", \":\"]]}, {\"src\": [[\"by\"], [\"caroline\", \"graham\"], [\"great\", \"pretender\", \":\", \"bestselling\", \"bridget\", \"jones\", \"'s\", \"diary\", \"author\", \"helen\", \"fielding\", \"set\", \"up\", \"fake\", \"online\", \"dating\", \"profiles\", \"as\", \"research\", \"for\", \"her\", \"latest\", \"book\"], [\"her\", \"much-loved\", \"creation\", \"bridget\", \"jones\", \"is\", \"famously\", \"unlucky\", \"in\", \"love\", \"--\", \"but\", \"author\", \"helen\", \"fielding\", \"has\", \"discovered\", \"the\", \"formula\", \"for\", \"dating\", \"success\", \":\", \"being\", \"a\", \"`\", \"bitch\", \"'\", \".\"], [\"the\", \"bestselling\", \"writer\", \"has\", \"revealed\", \"that\", \"she\", \"set\", \"up\", \"fake\", \"profiles\", \"on\", \"internet\", \"dating\", \"sites\", \"--\", \"and\", \"found\", \"that\", \"a\", \"nastier\", \"persona\", \"attracted\", \"more\", \"male\", \"interest\", \".\"], [\"as\", \"research\", \"for\", \"her\", \"latest\", \"book\", \",\", \"fielding\", \"set\", \"up\", \"two\", \"accounts\", \":\", \"one\", \"featuring\", \"a\", \"photograph\", \"of\", \"a\", \"woman\", \"in\", \"a\", \"sensible\", \"sweater\", \"who\", \"loved\", \"to\", \"read\", \"and\", \"cook\", \";\", \"the\", \"other\", \"called\", \"`\", \"superluckybitch\", \"'\", \"who\", \"had\", \"a\", \"sexy\", \"photograph\", \"and\", \"a\", \"description\", \"which\", \"`\", \"portrayed\", \"her\", \"as\", \"a\", \"real\", \"bitch\", \"'\", \".\"], [\"fielding\", \"revealed\", \":\", \"`\", \"so\", \"i\", \"had\", \"superluckybitch\", \"who\", \"was\", \"glamorous\", \"but\", \"really\", \"horrible\", \"and\", \"it\", \"said\", \"on\", \"her\", \"profile\", \",\", \"\\\"\", \"you\", \"'d\", \"be\", \"lucky\", \"to\", \"get\", \"a\", \"date\", \"with\", \"me\", \"\\\"\", \",\", \"and\", \"the\", \"other\", \"woman\", \"was\", \"really\", \"nice\", \"and\", \"sweet\", \".\"], [\"`\", \"well\", \",\", \"i\", \"bet\", \"you\", \"can\", \"guess\", \"who\", \"was\", \"the\", \"most\", \"popular\", \".\"], [\"superluckybitch\", \"got\", \"all\", \"the\", \"replies\", \".\", \"'\"], [\"the\", \"55-year-old\", \"author\", \"was\", \"speaking\", \"about\", \"mad\", \"about\", \"the\", \"boy\", \",\", \"the\", \"third\", \"book\", \"in\", \"the\", \"bridget\", \"jones\", \"series\", \",\", \"at\", \"an\", \"event\", \"in\", \"los\", \"angeles\", \"hosted\", \"by\", \"her\", \"friend\", \",\", \"the\", \"star\", \"wars\", \"actress\", \"carrie\", \"fisher\", \".\"], [\"when\", \"fisher\", \"asked\", \"fielding\", \"--\", \"who\", \"split\", \"from\", \"former\", \"partner\", \"kevin\", \"curran\", \"in\", \"2009\", \"--\", \"if\", \"she\", \"had\", \"gone\", \"on\", \"dates\", \"with\", \"any\", \"of\", \"the\", \"men\", \"her\", \"fake\", \"profiles\", \"attracted\", \",\", \"she\", \"laughed\", \":\", \"`\", \"i\", \"did\", \"see\", \"one\", \"person\", \"but\", \"he\", \"realised\", \"that\", \"i\", \"was\", \"a\", \"writer\", \"and\", \"guessed\", \"who\", \"i\", \"was\", \".\"], [\"i\", \"went\", \"and\", \"met\", \"him\", \"and\", \"explained\", \"i\", \"was\", \"doing\", \"research\", \"and\", \"he\", \"was\", \"very\", \"nice\", \"about\", \"it\", \".\", \"'\"], [\"the\", \"writer\", \"admitted\", \"to\", \"sharing\", \"bridget\", \"'s\", \"obsessive\", \"streak\", \",\", \"which\", \"saw\", \"her\", \"counting\", \"every\", \"calorie\", \"she\", \"consumed\", \"and\", \"documenting\", \"her\", \"weight\", \"daily\", \".\"], [\"she\", \"also\", \"became\", \"so\", \"addicted\", \"to\", \"twitter\", \"that\", \"the\", \"social\", \"nework\", \"cut\", \"her\", \"off\", \"after\", \"she\", \"checked\", \"her\", \"own\", \"profile\", \"150\", \"times\", \"in\", \"one\", \"hour\", \".\"], [\"she\", \"explained\", \":\", \"`\", \"i\", \"had\", \"to\", \"stop\", \"with\", \"the\", \"twitter\", \".\"], [\"it\", \"is\", \"a\", \"bit\", \"of\", \"a\", \"giant\", \"popularity\", \"contest\", \".\"], [\"i\", \"think\", \"i\", \"got\", \"[\", \"blocked\", \"]\", \"as\", \"i\", \"checked\", \"how\", \"many\", \"followers\", \"i\", \"had\", \"too\", \"many\", \"times\", \".\", \"'\"], [\"fielding\", \"revealed\", \"many\", \"similarities\", \"between\", \"herself\", \"and\", \"bridget\", \"during\", \"the\", \"90-minute\", \"talk\", \",\", \"including\", \"a\", \"disorganised\", \"approach\", \"to\", \"work\", \".\"], [\"she\", \"said\", \"she\", \"spends\", \"hours\", \"`\", \"faffing\", \"about\", \"'\", \"before\", \"starting\", \"work\", \"--\", \"and\", \"often\", \"makes\", \"bridget-style\", \"gaffes\", \".\"], [\"`\", \"i\", \"was\", \"due\", \"to\", \"have\", \"my\", \"daughter\", \"by\", \"c-section\", \"at\", \"cedars\", \"sinai\", \"so\", \"i\", \"wrote\", \"out\", \"this\", \"mass\", \"email\", \"to\", \"everyone\", \"announcing\", \"the\", \"birth\", \"as\", \"it\", \"was\", \"all\", \"planned\", \",\", \"but\", \"i\", \"sent\", \"out\", \"the\", \"email\", \"by\", \"mistake\", \"beforehand\", \".\"], [\"then\", \"i\", \"had\", \"to\", \"let\", \"everyone\", \"know\", \"that\", \"i\", \"had\", \"n't\", \"even\", \"had\", \"the\", \"baby\", \"yet\", \",\", \"'\", \"she\", \"said\", \".\"], [\"`\", \"i\", \"do\", \"n't\", \"have\", \"a\", \"routine\", \"for\", \"writing\", \".\"], [\"instead\", \"i\", \"spend\", \"hours\", \"going\", \"to\", \"the\", \"fridge\", \"or\", \"cleaning\", \"out\", \"the\", \"cupboards\", \".\"], [\"i\", \"spend\", \"a\", \"lot\", \"of\", \"time\", \"faffing\", \"about\", \",\", \"then\", \"it\", \"'s\", \"time\", \"to\", \"pick\", \"the\", \"kids\", \"up\", \"from\", \"school\", \".\"], [\"i\", \"do\", \"a\", \"lot\", \"of\", \"late-night\", \"writing\", \"--\", \"and\", \"then\", \"i\", \"just\", \"steal\", \"the\", \"plot\", \"from\", \"jane\", \"austen\", \",\", \"'\", \"she\", \"joked\", \".\"], [\"romcom\", \":\", \"hugh\", \"grant\", \"and\", \"renee\", \"zellweger\", \"in\", \"the\", \"bridget\", \"jones\", \"film\", \".\"], [\"fielding\", \"revealed\", \"many\", \"similarities\", \"between\", \"herself\", \"and\", \"bridget\", \"during\", \"the\", \"90-minute\", \"talk\", \",\", \"including\", \"a\", \"disorganised\", \"approach\", \"to\", \"work\"], [\"she\", \"said\", \"that\", \"initially\", \",\", \"she\", \"did\", \"n't\", \"intend\", \"her\", \"latest\", \"book\", \"--\", \"which\", \"covers\", \"topics\", \"such\", \"as\", \"dating\", \"among\", \"50-somethings\", \"--\", \"to\", \"be\", \"about\", \"bridget\", \"jones\", \",\", \"but\", \"`\", \"along\", \"the\", \"way\", \"i\", \"realised\", \"it\", \"needed\", \"to\", \"be\", \"bridget\", \"'\", \".\"], [\"she\", \"said\", \":\", \"`\", \"it\", \"took\", \"me\", \"about\", \"three\", \"months\", \"to\", \"become\", \"bridget\", \"again\", \".\"], [\"it\", \"was\", \"easier\", \"to\", \"write\", \"it\", \"because\", \"i\", \"was\", \"in\", \"london\", \".\"], [\"there\", \"'s\", \"no\", \"way\", \"i\", \"could\", \"'ve\", \"written\", \"it\", \"if\", \"i\", \"was\", \"still\", \"living\", \"in\", \"la\", \".\"], [\"i\", \"never\", \"told\", \"anyone\", \"that\", \"i\", \"was\", \"doing\", \"a\", \"new\", \"bridget\", \"book\", \".\"], [\"i\", \"wanted\", \"to\", \"get\", \"it\", \"written\", \"without\", \"anyone\", \"else\", \"'s\", \"input\", \".\", \"'\"], [\"she\", \"has\", \"n't\", \"ruled\", \"another\", \"bridget\", \"jones\", \"book\", \",\", \"but\", \"said\", \":\", \"`\", \"i\", \"do\", \"n't\", \"know\", \"if\", \"i\", \"'ll\", \"bring\", \"bridget\", \"back\", \"again\", \"at\", \"61\", \"or\", \"65\", \".\"], [\"i\", \"would\", \"n't\", \"want\", \"to\", \"do\", \"it\", \"just\", \"for\", \"the\", \"sake\", \"of\", \"doing\", \"it\", \".\"], [\"i\", \"'d\", \"only\", \"do\", \"it\", \"if\", \"i\", \"had\", \"something\", \"to\", \"say\", \".\", \"'\"]], \"tgt\": [[\"she\", \"set\", \"up\", \"two\", \"online\", \"dating\", \"profiles\", \"as\", \"research\", \"for\", \"her\", \"latest\", \"book\"], [\"one\", \"was\", \"sweet\", \"and\", \"sensible\", \",\", \"the\", \"other\", \"glamorous\", \",\", \"sexy\", \"but\", \"horrible\"], [\"`\", \"i\", \"bet\", \"you\", \"can\", \"guess\", \"who\", \"was\", \"the\", \"most\", \"popular\", \",\", \"'\", \"she\", \"says\"], [\"fielding\", \"spoke\", \"in\", \"los\", \"angeles\", \"at\", \"an\", \"event\", \"promoted\", \"by\", \"carrie\", \"fisher\"]]}, {\"src\": [[\"snow\", \",\", \"sleet\", \"and\", \"blizzards\", \"have\", \"hit\", \"parts\", \"of\", \"the\", \"uk\", \"which\", \"could\", \"see\", \"areas\", \"of\", \"britain\", \"wake\", \"up\", \"to\", \"a\", \"blanket\", \"of\", \"snow\", \"on\", \"saturday\", \"morning\", \".\"], [\"an\", \"`\", \"area\", \"of\", \"rain\", \",\", \"sleet\", \"and\", \"snow\", \"'\", \"is\", \"moving\", \"east\", \"across\", \"northern\", \"ireland\", \",\", \"wales\", \",\", \"central\", \"and\", \"northern\", \"england\", \"and\", \"scotland\", \",\", \"forecasters\", \"said\", \".\"], [\"snow\", \"has\", \"forced\", \"liverpool\", \"'s\", \"john\", \"lennon\", \"airport\", \"to\", \"close\", \".\"], [\"planes\", \"from\", \"malta\", \",\", \"berlin\", \"and\", \"bucharest\", \"were\", \"sent\", \"to\", \"manchester\", \",\", \"while\", \"one\", \"from\", \"the\", \"isle\", \"of\", \"man\", \"had\", \"to\", \"turn\", \"back\", \".\"], [\"a\", \"spokesman\", \"for\", \"the\", \"airport\", \"said\", \":\", \"`\", \"we\", \"are\", \"clearing\", \"the\", \"runway\", \".\"], [\"we\", \"have\", \"had\", \"quite\", \"a\", \"deluge\", \".\", \"'\"], [\"scroll\", \"down\", \"for\", \"video\"], [\"drivers\", \"heave\", \"a\", \"stranded\", \"car\", \"up\", \"a\", \"steep\", \"hill\", \"in\", \"nottingham\", \",\", \"as\", \"it\", \"attempts\", \"to\", \"navigate\", \"its\", \"way\", \"through\", \"the\", \"slushy\", \"road\"], [\"snow\", \"fell\", \"across\", \"derbyshire\", \",\", \"leaving\", \"hundreds\", \"of\", \"people\", \"without\", \"electricity\", \"-\", \"motorists\", \"struggle\", \"on\", \"the\", \"a52\", \"near\", \"ashbourne\", \"(\", \"pictured\", \")\"], [\"the\", \"blizzard\", \"conditions\", \"made\", \"travelling\", \"conditions\", \"treacherous\", \"for\", \"motorists\", \"-\", \"a\", \"tractor\", \"with\", \"a\", \"rope\", \"tows\", \"a\", \"car\", \"on\", \"the\", \"a52\", \"in\", \"derbyshire\"], [\"gritters\", \"and\", \"snow\", \"ploughs\", \"were\", \"out\", \"in\", \"force\", \"in\", \"sheffield\", \"tonight\", \",\", \"with\", \"cars\", \"sliding\", \"around\", \"and\", \"stuck\", \"in\", \"the\", \"snow\"], [\"snow\", \"has\", \"forced\", \"liverpool\", \"'s\", \"john\", \"lennon\", \"airport\", \"to\", \"close\", \"and\", \"flights\", \"are\", \"being\", \"rerouted\", \"elsewhere\", \"to\", \"allow\", \"runways\", \"to\", \"be\", \"cleared\"], [\"maisy\", \"byrne\", \",\", \"aged\", \"nine\", \",\", \"(\", \"left\", \")\", \"and\", \"sister\", \"lulu\", \"byrne\", \",\", \"aged\", \"seven\", \",\", \"trudge\", \"through\", \"the\", \"snow\", \"as\", \"snow\", \"falls\", \"in\", \"gateacre\", \",\", \"liverpool\"], [\"two\", \"children\", \"build\", \"up\", \"snowmen\", \"on\", \"sledges\", \"as\", \"snow\", \"fell\", \"in\", \"mapperley\", \",\", \"nottingham\", \"-\", \"police\", \"issued\", \"warnings\", \"for\", \"the\", \"roads\", \"in\", \"the\", \"midlands\"], [\"a\", \"car\", \"lights\", \"up\", \"the\", \"snow\", \"falling\", \"tonight\", \"in\", \"childwall\", \",\", \"in\", \"liverpool\", \"-\", \"much\", \"of\", \"the\", \"uk\", \"will\", \"experience\", \"snow\", \"overnight\"], [\"a\", \"child\", \"poses\", \"with\", \"a\", \"snowball\", \"near\", \"towcester\", \"in\", \"northamptonshire\", \"as\", \"extreme\", \"weather\", \"struck\", \"the\", \"region\"], [\"the\", \"heavy\", \"snow\", \"meant\", \"motorists\", \"ended\", \"up\", \"abandoning\", \"their\", \"cars\", \"in\", \"parts\", \"of\", \"sheffield\", \",\", \"yorkshire\"], [\"referee\", \"mark\", \"clattenburg\", \"gazes\", \"up\", \"in\", \"to\", \"the\", \"stands\", \"as\", \"the\", \"snow\", \"fell\", \"heavily\", \"at\", \"the\", \"hawthorns\", \"in\", \"west\", \"bromwich\", \"today\"], [\"england\", \"and\", \"manchester\", \"city\", \"goalkeeper\", \"dives\", \"for\", \"the\", \"ball\", \"on\", \"the\", \"white\", \"ground\", \"under\", \"pressure\", \"from\", \"west\", \"brom\", \"striker\", \"brown\", \"ideye\"], [\"east\", \"midlands\", \"airport\", \"in\", \"leicestershire\", \"was\", \"forced\", \"to\", \"suspend\", \"flights\", \"because\", \"of\", \"snow\", \".\"], [\"staff\", \"trying\", \"to\", \"clear\", \"the\", \"runway\", \"at\", \"6.30\", \"pm\", \"were\", \"unable\", \"to\", \"say\", \"when\", \"it\", \"would\", \"reopen\", \".\"], [\"leeds\", \"bradford\", \"international\", \"airport\", \"was\", \"also\", \"forced\", \"to\", \"close\", \"tonight\", \".\"], [\"a\", \"message\", \"on\", \"its\", \"website\", \"read\", \":\", \"`\", \"as\", \"a\", \"result\", \"of\", \"adverse\", \"weather\", \"conditions\", \",\", \"leeds\", \"bradford\", \"international\", \"airport\", \"is\", \"presently\", \"closed\", \".\"], [\"`\", \"we\", \"recommend\", \"you\", \"contact\", \"your\", \"airline\", \"or\", \"tour\", \"operator\", \"and\", \"check\", \"the\", \"status\", \"of\", \"your\", \"flight\", \"before\", \"arriving\", \"at\", \"the\", \"airport\", \".\", \"'\"], [\"elsewhere\", \",\", \"passengers\", \"on\", \"board\", \"a\", \"flight\", \"to\", \"hamburg\", \"said\", \"there\", \"were\", \"stuck\", \"at\", \"manchester\", \"airport\", \"for\", \"more\", \"than\", \"three\", \"hours\", \"because\", \"of\", \"snow\", \".\"], [\"according\", \"to\", \"reports\", \",\", \"the\", \"flight\", \"was\", \"delayed\", \"because\", \"it\", \"needed\", \"to\", \"be\", \"`\", \"de-iced\", \"'\", \"by\", \"a\", \"special\", \"vehicle\", \"at\", \"the\", \"airport\", \".\"], [\"tonight\", \",\", \"snow\", \"fell\", \"across\", \"derbyshire\", \",\", \"with\", \"severe\", \"weather\", \"leaving\", \"hundreds\", \"of\", \"people\", \"without\", \"electricity\", \".\"], [\"some\", \"1,824\", \"homes\", \"were\", \"hit\", \"by\", \"a\", \"power\", \"cut\", \"in\", \"the\", \"ashbourne\", \"area\", \"of\", \"the\", \"derbyshire\", \"dales\", \".\"], [\"although\", \"power\", \"was\", \"restored\", \"to\", \"many\", \",\", \"western\", \"power\", \"distribution\", \"said\", \"staff\", \"were\", \"struggling\", \"to\", \"get\", \"to\", \"customers\", \"because\", \"of\", \"snow\", \"and\", \"bad\", \"weather\", \".\"], [\"police\", \"have\", \"issued\", \"warnings\", \"for\", \"roads\", \"in\", \"the\", \"midlands\", \"and\", \"north\", \"west\", \"because\", \"of\", \"snow\", \".\"], [\"staffordshire\", \"police\", \"tweeted\", \":\", \"`\", \"difficult\", \"road\", \"conditions\", \"across\", \"staffordshire\", \".\"], [\"only\", \"travel\", \"if\", \"necessary\", \".\", \"'\"], [\"cheshire\", \"police\", \"said\", \"some\", \"roads\", \"were\", \"`\", \"looking\", \"treacherous\", \"'\", \".\"], [\"the\", \"worst\", \"affected\", \"areas\", \"are\", \"in\", \"the\", \"north\", \"of\", \"england\", \",\", \"with\", \"manchester\", \",\", \"liverpool\", \"and\", \"sheffield\", \"experiencing\", \"extreme\", \"weather\", \".\"], [\"earlier\", \"today\", \",\", \"footballers\", \"played\", \"through\", \"a\", \"flurry\", \"of\", \"snow\", \"and\", \"fans\", \"shivered\", \"in\", \"the\", \"stands\", \"during\", \"a\", \"premier\", \"league\", \"match\", \"between\", \"west\", \"bromwich\", \"albion\", \"and\", \"manchester\", \"city\", \"at\", \"the\", \"hawthorns\", \".\"], [\"a\", \"driver\", \"nervously\", \"travels\", \"through\", \"the\", \"a5\", \"near\", \"towcester\", \"in\", \"northamptonshire\", \"in\", \"what\", \"were\", \"tricky\", \"conditions\", \"for\", \"motorists\"], [\"west\", \"yorkshire\", \"police\", \"tweeted\", \"a\", \"picture\", \"of\", \"the\", \"snow\", \"that\", \"closed\", \"the\", \"entry\", \"slip\", \"at\", \"junction\", \"22\", \"tonight\"], [\"a\", \"family\", \"walk\", \"through\", \"the\", \"snow\", \"in\", \"ashbourne\", \",\", \"in\", \"derbyshire\", \",\", \"this\", \"evening\", \",\", \"where\", \"snowfall\", \"has\", \"led\", \"to\", \"blocked\", \"roads\", \"in\", \"the\", \"area\"], [\"a\", \"group\", \"of\", \"men\", \"clamber\", \"out\", \"of\", \"a\", \"bmw\", \"in\", \"childwall\", \"in\", \"liverpool\", \"as\", \"heavy\", \"snow\", \"covered\", \"the\", \"cars\"], [\"a\", \"flurry\", \"of\", \"snow\", \"fell\", \"in\", \"birmingham\", \"-\", \"temperatures\", \"could\", \"also\", \"drop\", \"as\", \"low\", \"as\", \"-10\", \"c\", \"in\", \"some\", \"places\", \"at\", \"the\", \"start\", \"of\", \"next\", \"week\"], [\"twitter\", \"user\", \"@bookishbecca\", \"said\", \"`\", \"came\", \"home\", \"early\", \"to\", \"north\", \"wales\", \"and\", \"glad\", \"we\", \"left\", \"when\", \"we\", \"did\", \"'\"], [\"twitter\", \"user\", \"julie\", \"todd\", \"uploaded\", \"this\", \"picture\", \"of\", \"snow\", \"in\", \"flint\", \",\", \"north\", \"wales\", \"where\", \"snow\", \"fell\", \"overnight\"], [\"manchester\", \"city\", \"defender\", \"martin\", \"demichelis\", \"signals\", \"as\", \"the\", \"snow\", \"falls\", \"during\", \"the\", \"premier\", \"league\", \"match\", \"at\", \"the\", \"hawthorns\", \"today\"], [\"the\", \"premier\", \"league\", \"match\", \"between\", \"west\", \"bromwich\", \"albion\", \"and\", \"manchester\", \"city\", \"was\", \"played\", \"in\", \"snow\", \"-\", \"the\", \"away\", \"side\", \"won\", \"3-1\"], [\"today\", \",\", \"the\", \"met\", \"office\", \"upgraded\", \"its\", \"cold\", \"weather\", \"alert\", \"to\", \"level\", \"3\", \"amber\", \"-\", \"one\", \"below\", \"a\", \"national\", \"emergency\", \"-\", \"saying\", \"there\", \"was\", \"a\", \"90\", \"per\", \"cent\", \"chance\", \"of\", \"severe\", \"cold\", \",\", \"ice\", \"or\", \"snow\", \"in\", \"parts\", \"of\", \"england\", \"in\", \"the\", \"run\", \"up\", \"to\", \"new\", \"year\", \"'s\", \"eve\", \".\"], [\"temperatures\", \"could\", \"also\", \"drop\", \"as\", \"low\", \"as\", \"-10\", \"c\", \"in\", \"some\", \"places\", \"at\", \"the\", \"start\", \"of\", \"next\", \"week\", \".\"], [\"a\", \"spokeswoman\", \"said\", \":\", \"`\", \"low\", \"pressure\", \",\", \"tracking\", \"southeastwards\", \"over\", \"central\", \"and\", \"southern\", \"england\", \"later\", \"boxing\", \"day\", \"and\", \"overnight\", \"into\", \"saturday\", \",\", \"is\", \"expected\", \"to\", \"give\", \"a\", \"period\", \"of\", \"sleet\", \"and\", \"snow\", \"across\", \"the\", \"midlands\", \",\", \"some\", \"northern\", \"parts\", \"of\", \"east\", \"anglia\", \"and\", \"southern\", \"parts\", \"of\", \"both\", \"nw\", \"england\", \"and\", \"yorks\", \"and\", \"humberside\", \".\"], [\"`\", \"some\", \"significant\", \"snow\", \"accumulations\", \"are\", \"possible\", \",\", \"especially\", \"on\", \"higher\", \"ground\", \".\"], [\"strong\", \"winds\", \"will\", \"develop\", \"in\", \"places\", \"for\", \"a\", \"time\", \",\", \"exacerbating\", \"the\", \"chill\", \".\"], [\"`\", \"as\", \"the\", \"low\", \"clears\", \"away\", \"southeastwards\", \",\", \"cold\", \"air\", \"will\", \"move\", \"south\", \"across\", \"the\", \"whole\", \"of\", \"england\", \"giving\", \"a\", \"risk\", \"of\", \"wintry\", \"showers\", \"and\", \"icy\", \"conditions\", \"across\", \"england\", \"for\", \"the\", \"rest\", \"of\", \"saturday\", \".\", \"'\"], [\"after\", \"a\", \"warmer\", \"than\", \"average\", \"christmas\", \"day\", \"yesterday\", \",\", \"a\", \"thick\", \"frost\", \"was\", \"on\", \"the\", \"ground\", \"this\", \"morning\", \"and\", \"temperatures\", \"will\", \"continue\", \"to\", \"drop\"], [\"it\", \"was\", \"a\", \"frosty\", \"morning\", \"on\", \"the\", \"river\", \"frome\", \"in\", \"dorset\", \"this\", \"morning\", \"but\", \"the\", \"clear\", \"skies\", \"made\", \"way\", \"for\", \"snow\", \"clouds\", \"in\", \"many\", \"areas\", \"tonight\"], [\"a\", \"huge\", \"band\", \"of\", \"rain\", \",\", \"sleet\", \"and\", \"snow\", \"is\", \"moving\", \"in\", \"from\", \"the\", \"atlantic\", \"and\", \"will\", \"move\", \"across\", \"the\", \"whole\", \"of\", \"england\", \"and\", \"southern\", \"scotland\", \"today\", \".\"], [\"an\", \"area\", \"including\", \"scotland\", \",\", \"northern\", \"ireland\", \",\", \"wales\", \",\", \"the\", \"midlands\", \",\", \"east\", \"anglia\", \",\", \"the\", \"north\", \"west\", \",\", \"yorkshire\", \"and\", \"as\", \"far\", \"south\", \"east\", \"as\", \"london\", \"and\", \"kent\", \"has\", \"been\", \"put\", \"on\", \"a\", \"separate\", \"yellow\", \"alert\", \"for\", \"snow\", \"by\", \"the\", \"met\", \"office\"], [\"krista\", \"mitchell\", \",\", \"a\", \"meteorologist\", \"at\", \"the\", \"met\", \"office\", \",\", \"said\", \":\", \"`\", \"this\", \"year\", \"as\", \"a\", \"whole\", \"has\", \"been\", \"very\", \"mild\", \",\", \"so\", \"this\", \"could\", \"be\", \"the\", \"coldest\", \"spell\", \"that\", \"we\", \"'ve\", \"had\", \".\"], [\"`\", \"as\", \"we\", \"go\", \"towards\", \"monday\", \"and\", \"tuesday\", \",\", \"we\", \"will\", \"see\", \"overnight\", \"temperatures\", \"really\", \"drop\", \",\", \"with\", \"widespread\", \"frost\", \"and\", \"most\", \"places\", \"dropping\", \"below\", \"freezing\", \".\", \"'\"], [\"the\", \"nhs\", \"and\", \"local\", \"councils\", \"have\", \"been\", \"put\", \"on\", \"alert\", \"and\", \"public\", \"health\", \"england\", \"have\", \"urged\", \"people\", \"to\", \"check\", \"on\", \"elderly\", \"neighbours\", \".\"], [\"the\", \"authority\", \"'s\", \"dr\", \"angie\", \"bone\", \"said\", \":\", \"`\", \"we\", \"encourage\", \"people\", \"to\", \"keep\", \"an\", \"eye\", \"on\", \"the\", \"forecast\", \"and\", \"take\", \"the\", \"weather\", \"into\", \"account\", \"when\", \"planning\", \"activities\", \"over\", \"the\", \"following\", \"days\", \".\"], [\"`\", \"we\", \"also\", \"advise\", \"people\", \"to\", \"keep\", \"active\", \"in\", \"the\", \"home\", \",\", \"have\", \"plenty\", \"of\", \"warm\", \"food\", \"and\", \"drinks\", \",\", \"and\", \"maintain\", \"indoor\", \"temperatures\", \"to\", \"at\", \"least\", \"18\", \"\\u00b0c\", \",\", \"particularly\", \"the\", \"older\", \"or\", \"very\", \"young\", \",\", \"people\", \"with\", \"pre-existing\", \"health\", \"conditions\", \",\", \"or\", \"who\", \"are\", \"not\", \"very\", \"mobile\", \".\"], [\"these\", \"groups\", \"can\", \"be\", \"particularly\", \"vulnerable\", \"to\", \"the\", \"ill-effects\", \"of\", \"cold\", \".\", \"'\"], [\"she\", \"added\", \":\", \"`\", \"also\", \"take\", \"some\", \"time\", \"to\", \"think\", \"about\", \"how\", \"the\", \"bad\", \"weather\", \"may\", \"affect\", \"your\", \"friends\", \",\", \"family\", \"and\", \"neighbours\", \":\", \"it\", \"'s\", \"important\", \"we\", \"all\", \"do\", \"what\", \"we\", \"can\", \"to\", \"protect\", \"those\", \"around\", \"us\", \".\"], [\"`\", \"if\", \"you\", \"do\", \"need\", \"to\", \"go\", \"out\", \",\", \"remember\", \"to\", \"wear\", \"lots\", \"of\", \"thin\", \"layers\", \"and\", \"shoes\", \"with\", \"a\", \"good\", \",\", \"slip-resistant\", \"grip\", \"to\", \"prevent\", \"any\", \"accidental\", \"falls\", \".\", \"'\"], [\"rain\", \",\", \"sleet\", \"and\", \"snow\", \"may\", \"also\", \"affect\", \"south\", \"west\", \"scotland\", \"and\", \"the\", \"strathclyde\", \"and\", \"lothian\", \"and\", \"borders\", \",\", \"forecasters\", \"said\", \".\"], [\"the\", \"sudden\", \"cold\", \"snap\", \"is\", \"in\", \"contrast\", \"to\", \"yesterday\", \",\", \"when\", \"above-average\", \"temperatures\", \"saw\", \"daffodils\", \"blooming\", \"in\", \"guildford\", \",\", \"surrey\"], [\"sun\", \"in\", \"southern\", \"england\", \"saw\", \"many\", \"britons\", \"enjoy\", \"a\", \"seaside\", \"walk\", \",\", \"with\", \"some\", \"even\", \"eating\", \"their\", \"christmas\", \"lunch\", \"next\", \"to\", \"the\", \"beach\"]], \"tgt\": [[\"warmer\", \"than\", \"average\", \"christmas\", \"day\", \"gave\", \"way\", \"to\", \"a\", \"freezing\", \"boxing\", \"day\", \",\", \"with\", \"snowfall\", \"across\", \"the\", \"uk\"], [\"`\", \"area\", \"of\", \"rain\", \",\", \"sleet\", \"and\", \"snow\", \"'\", \"moving\", \"east\", \"across\", \"northern\", \"ireland\", \",\", \"wales\", \",\", \"central\", \",\", \"northern\", \"england\", \"and\", \"scotland\"], [\"snow\", \"has\", \"forced\", \"liverpool\", \"'s\", \"john\", \"lennon\", \"airport\", \"to\", \"close\", \"and\", \"flights\", \"in\", \"to\", \"the\", \"airport\", \"have\", \"been\", \"rerouted\", \"elsewhere\"], [\"east\", \"midlands\", \"airport\", \"in\", \"leicestershire\", \"and\", \"leeds\", \"bradford\", \"airport\", \"forced\", \"to\", \"suspend\", \"flights\", \"because\", \"of\", \"snow\"], [\"cold\", \"snap\", \"set\", \"to\", \"last\", \"well\", \"into\", \"next\", \"week\", \"as\", \"freezing\", \"weekend\", \"of\", \"snow\", \",\", \"ice\", \"and\", \"-10\", \"c\", \"temperatures\", \"forecast\"]]}]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPHurDTaUnUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "# This is a list of dictionaries where each dictionary refers to an article and has the keys 'src' and 'tgt'\n",
        "# 'src' refers to the source article while 'tgt' refers to the target summaries (those shown after the @higlight tags in the original document)\n",
        "# The value for both keys is a list of lists. The outer list has elements corresponding to each sentence within an article.\n",
        "# The inner lists correspond to sentences where their elements correspond to indidvidual tokens found in each sentence.\n",
        "test = json.load(open('/home/json_data/cnndm.train.0.json', 'r'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kwjyDq2VHP4",
        "colab_type": "code",
        "outputId": "2a52ec8e-9834-4ff8-aae0-169df5bf39c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cxfnfUsVMML",
        "colab_type": "code",
        "outputId": "4961188e-4f90-4028-cfde-20b1caee041b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "test[0]['tgt']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['gunmen',\n",
              "  'and',\n",
              "  'a',\n",
              "  'suicide',\n",
              "  'bomber',\n",
              "  'attack',\n",
              "  'a',\n",
              "  'mosque',\n",
              "  'in',\n",
              "  'peshawar',\n",
              "  ',',\n",
              "  'police',\n",
              "  'say'],\n",
              " ['attack',\n",
              "  'is',\n",
              "  'revenge',\n",
              "  'for',\n",
              "  'the',\n",
              "  'government',\n",
              "  \"'s\",\n",
              "  'december',\n",
              "  'execution',\n",
              "  'of',\n",
              "  'a',\n",
              "  'militant',\n",
              "  ',',\n",
              "  'pakistan',\n",
              "  'taliban',\n",
              "  'say'],\n",
              " ['the',\n",
              "  'group',\n",
              "  'says',\n",
              "  'the',\n",
              "  'attack',\n",
              "  \"'s\",\n",
              "  'orchestrator',\n",
              "  'is',\n",
              "  'a',\n",
              "  'commander',\n",
              "  'who',\n",
              "  'it',\n",
              "  'previously',\n",
              "  'said',\n",
              "  'planned',\n",
              "  'a',\n",
              "  'december',\n",
              "  'school',\n",
              "  'massacre']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-JjCM-FWfwz",
        "colab_type": "code",
        "outputId": "df2d06d6-0037-4622-8368-73729f0002b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6936
        }
      },
      "source": [
        "test[0]['src']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['(',\n",
              "  'cnn',\n",
              "  ')',\n",
              "  'the',\n",
              "  'pakistan',\n",
              "  'taliban',\n",
              "  'claimed',\n",
              "  'responsibility',\n",
              "  'for',\n",
              "  'a',\n",
              "  'friday',\n",
              "  'attack',\n",
              "  'on',\n",
              "  'a',\n",
              "  'shiite',\n",
              "  'muslim',\n",
              "  'mosque',\n",
              "  'in',\n",
              "  'peshawar',\n",
              "  'in',\n",
              "  'northwestern',\n",
              "  'pakistan',\n",
              "  '--',\n",
              "  'a',\n",
              "  'suicide',\n",
              "  'bombing',\n",
              "  'and',\n",
              "  'gunfire',\n",
              "  'assault',\n",
              "  'that',\n",
              "  'a',\n",
              "  'hospital',\n",
              "  'representative',\n",
              "  'said',\n",
              "  'killed',\n",
              "  '19',\n",
              "  'people',\n",
              "  '.'],\n",
              " ['the',\n",
              "  'islamist',\n",
              "  'militant',\n",
              "  'group',\n",
              "  'said',\n",
              "  'the',\n",
              "  'attack',\n",
              "  'was',\n",
              "  'orchestrated',\n",
              "  'by',\n",
              "  'a',\n",
              "  'commander',\n",
              "  'who',\n",
              "  'was',\n",
              "  'behind',\n",
              "  'december',\n",
              "  \"'s\",\n",
              "  'massacre',\n",
              "  'of',\n",
              "  '145',\n",
              "  'people',\n",
              "  ',',\n",
              "  'including',\n",
              "  '132',\n",
              "  'children',\n",
              "  ',',\n",
              "  'at',\n",
              "  'a',\n",
              "  'peshawar',\n",
              "  'school',\n",
              "  '.'],\n",
              " ['sixty-seven',\n",
              "  'people',\n",
              "  'were',\n",
              "  'injured',\n",
              "  'friday',\n",
              "  ',',\n",
              "  'said',\n",
              "  'tauheed',\n",
              "  'zulfiqar',\n",
              "  ',',\n",
              "  'a',\n",
              "  'representative',\n",
              "  'of',\n",
              "  'the',\n",
              "  'hayatabad',\n",
              "  'medical',\n",
              "  'complex',\n",
              "  'in',\n",
              "  'peshawar',\n",
              "  '.'],\n",
              " ['the',\n",
              "  'pakistan',\n",
              "  'taliban',\n",
              "  'attacked',\n",
              "  'the',\n",
              "  'mosque',\n",
              "  ',',\n",
              "  'spokesman',\n",
              "  'muhammad',\n",
              "  'khurasan',\n",
              "  'said',\n",
              "  'in',\n",
              "  'an',\n",
              "  'email',\n",
              "  'to',\n",
              "  'cnn',\n",
              "  ',',\n",
              "  'as',\n",
              "  'revenge',\n",
              "  'for',\n",
              "  'the',\n",
              "  'government',\n",
              "  \"'s\",\n",
              "  'december',\n",
              "  '19',\n",
              "  'execution',\n",
              "  'of',\n",
              "  'a',\n",
              "  'militant',\n",
              "  'who',\n",
              "  'was',\n",
              "  'allied',\n",
              "  'with',\n",
              "  'the',\n",
              "  'group',\n",
              "  '.'],\n",
              " ['up',\n",
              "  'to',\n",
              "  'five',\n",
              "  'attackers',\n",
              "  'executed',\n",
              "  'the',\n",
              "  'assault',\n",
              "  ',',\n",
              "  'including',\n",
              "  'a',\n",
              "  'suicide',\n",
              "  'bomber',\n",
              "  'and',\n",
              "  'someone',\n",
              "  'who',\n",
              "  'was',\n",
              "  'shooting',\n",
              "  'in',\n",
              "  'the',\n",
              "  'mosque',\n",
              "  ',',\n",
              "  'nasir',\n",
              "  'khan',\n",
              "  'durrani',\n",
              "  ',',\n",
              "  'the',\n",
              "  'city',\n",
              "  \"'s\",\n",
              "  'police',\n",
              "  'inspector',\n",
              "  'general',\n",
              "  ',',\n",
              "  'told',\n",
              "  'reporters',\n",
              "  '.'],\n",
              " ['one',\n",
              "  'would-be',\n",
              "  'suicide',\n",
              "  'bomber',\n",
              "  'was',\n",
              "  'stopped',\n",
              "  'by',\n",
              "  'people',\n",
              "  'in',\n",
              "  'the',\n",
              "  'mosque',\n",
              "  'who',\n",
              "  'held',\n",
              "  'him',\n",
              "  'by',\n",
              "  'the',\n",
              "  'throat',\n",
              "  ',',\n",
              "  'durrani',\n",
              "  'said',\n",
              "  '.'],\n",
              " ['pakistan',\n",
              "  'has',\n",
              "  'seen',\n",
              "  'plenty',\n",
              "  'of',\n",
              "  'violence',\n",
              "  ',',\n",
              "  'much',\n",
              "  'of',\n",
              "  'it',\n",
              "  'involving',\n",
              "  'militants',\n",
              "  'targeting',\n",
              "  'restive',\n",
              "  'regions',\n",
              "  'in',\n",
              "  'northwest',\n",
              "  'pakistan',\n",
              "  'along',\n",
              "  'the',\n",
              "  'border',\n",
              "  'with',\n",
              "  'afghanistan',\n",
              "  '.'],\n",
              " ['it',\n",
              "  'is',\n",
              "  'the',\n",
              "  'home',\n",
              "  'base',\n",
              "  'of',\n",
              "  'the',\n",
              "  'pakistan',\n",
              "  'taliban',\n",
              "  ',',\n",
              "  'known',\n",
              "  'as',\n",
              "  'the',\n",
              "  'tehrik-e-taliban',\n",
              "  'pakistan',\n",
              "  ',',\n",
              "  'or',\n",
              "  'ttp',\n",
              "  ',',\n",
              "  'which',\n",
              "  'seeks',\n",
              "  'to',\n",
              "  'enforce',\n",
              "  'its',\n",
              "  'conservative',\n",
              "  'version',\n",
              "  'of',\n",
              "  'islam',\n",
              "  'in',\n",
              "  'that',\n",
              "  'nation',\n",
              "  '.'],\n",
              " ['the',\n",
              "  'group',\n",
              "  'has',\n",
              "  'battled',\n",
              "  'pakistani',\n",
              "  'troops',\n",
              "  'and',\n",
              "  'attacked',\n",
              "  'civilians',\n",
              "  ',',\n",
              "  'including',\n",
              "  'in',\n",
              "  'peshawar',\n",
              "  ',',\n",
              "  'an',\n",
              "  'ancient',\n",
              "  'city',\n",
              "  'of',\n",
              "  'more',\n",
              "  'than',\n",
              "  '3',\n",
              "  'million',\n",
              "  'people',\n",
              "  '.'],\n",
              " ['khurasan',\n",
              "  ',',\n",
              "  'the',\n",
              "  'ttp',\n",
              "  'spokesman',\n",
              "  ',',\n",
              "  'said',\n",
              "  'the',\n",
              "  'attack',\n",
              "  'was',\n",
              "  'orchestrated',\n",
              "  'by',\n",
              "  'ttp',\n",
              "  'commander',\n",
              "  'kalifa',\n",
              "  'omar',\n",
              "  'mansoor',\n",
              "  '.'],\n",
              " ['the',\n",
              "  'militant',\n",
              "  'group',\n",
              "  'and',\n",
              "  'the',\n",
              "  'pakistani',\n",
              "  'army',\n",
              "  'said',\n",
              "  'previously',\n",
              "  'that',\n",
              "  'mansoor',\n",
              "  'commanded',\n",
              "  'the',\n",
              "  'december',\n",
              "  '16',\n",
              "  'massacre',\n",
              "  'at',\n",
              "  'the',\n",
              "  'army',\n",
              "  'public',\n",
              "  'school',\n",
              "  'and',\n",
              "  'degree',\n",
              "  'college',\n",
              "  ',',\n",
              "  'which',\n",
              "  'largely',\n",
              "  'teaches',\n",
              "  'sons',\n",
              "  'and',\n",
              "  'daughters',\n",
              "  'of',\n",
              "  'army',\n",
              "  'personnel',\n",
              "  'from',\n",
              "  'around',\n",
              "  'peshawar',\n",
              "  '.'],\n",
              " ['in',\n",
              "  'the',\n",
              "  'school',\n",
              "  'attack',\n",
              "  ',',\n",
              "  'gunmen',\n",
              "  'burst',\n",
              "  'in',\n",
              "  'and',\n",
              "  'gunned',\n",
              "  'down',\n",
              "  'children',\n",
              "  'and',\n",
              "  'staff',\n",
              "  ',',\n",
              "  'including',\n",
              "  'in',\n",
              "  'an',\n",
              "  'auditorium',\n",
              "  'filled',\n",
              "  'with',\n",
              "  'students',\n",
              "  'taking',\n",
              "  'exams',\n",
              "  '.'],\n",
              " ['khurasan',\n",
              "  'said',\n",
              "  'friday',\n",
              "  \"'s\",\n",
              "  'mosque',\n",
              "  'attack',\n",
              "  'was',\n",
              "  'revenge',\n",
              "  'for',\n",
              "  'the',\n",
              "  'government',\n",
              "  \"'s\",\n",
              "  'december',\n",
              "  '19',\n",
              "  'execution',\n",
              "  'of',\n",
              "  'mohammed',\n",
              "  'aqeel',\n",
              "  ',',\n",
              "  'a',\n",
              "  'man',\n",
              "  'condemned',\n",
              "  'in',\n",
              "  'part',\n",
              "  'for',\n",
              "  'his',\n",
              "  'role',\n",
              "  'in',\n",
              "  'an',\n",
              "  'attack',\n",
              "  'on',\n",
              "  'an',\n",
              "  'army',\n",
              "  'headquarters',\n",
              "  'in',\n",
              "  '2009',\n",
              "  '.'],\n",
              " ['aqeel',\n",
              "  'was',\n",
              "  'a',\n",
              "  'member',\n",
              "  'of',\n",
              "  'a',\n",
              "  'militant',\n",
              "  'group',\n",
              "  'allied',\n",
              "  'with',\n",
              "  'the',\n",
              "  'ttp',\n",
              "  '.'],\n",
              " ['cnn',\n",
              "  \"'s\",\n",
              "  'jason',\n",
              "  'hanna',\n",
              "  ',',\n",
              "  'faith',\n",
              "  'karimi',\n",
              "  'and',\n",
              "  'greg',\n",
              "  'botelho',\n",
              "  'contributed',\n",
              "  'to',\n",
              "  'this',\n",
              "  'report',\n",
              "  '.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOU5hW8TWW_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGQc5XUYUYBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir {BERT_DATA_PATH}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr3cOfAJ6-It",
        "colab_type": "code",
        "outputId": "6d3b54bb-d5b3-41c5-9cbc-d60180739f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Colab notebook has 2 cpus (!grep -c ^processor /proc/cpuinfo)\n",
        "# Convert json file to pytorch file (.pt)\n",
        "# Json file is converted to a pytorch dataset object in the format that is required by the BERT module object that it will later be put into\n",
        "# This includes word-piece tokenization, adding BERT tags (i.e. [CLS], [SEP]), indexing word-piece tokens,\n",
        "# and deriving oracle labels (actual sentence summaries), segment Ids, [CLS] ids, source text (found in input), and target text (found in input)\n",
        "!python /home/BertSum/src/preprocess.py -mode format_to_bert -raw_path /home/json_data/ -save_path {BERT_DATA_PATH} -oracle_mode greedy -n_cpus 2 -log_file /logs/preprocess.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('/home/json_data/cnndm.train.0.json', Namespace(corenlp_path='../stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar', dataset='', log_file='/logs/preprocess.log', lower=True, map_path='../data/', max_nsents=100, max_src_ntokens=200, min_nsents=3, min_src_ntokens=5, mode='format_to_bert', n_cpus=2, oracle_mode='greedy', raw_path='/home/json_data/', save_path='/home/bert_data/', shard_size=2000), '/home/bert_data/cnndm.train.0.bert.pt')]\n",
            "[2019-05-15 02:38:31,963 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpiug03myk\n",
            "100% 231508/231508 [00:00<00:00, 913105.76B/s]\n",
            "[2019-05-15 02:38:32,607 INFO] copying /tmp/tmpiug03myk to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "[2019-05-15 02:38:32,608 INFO] creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "[2019-05-15 02:38:32,608 INFO] removing temp file /tmp/tmpiug03myk\n",
            "[2019-05-15 02:38:32,609 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "[2019-05-15 02:38:32,647 INFO] Processing /home/json_data/cnndm.train.0.json\n",
            "[2019-05-15 02:38:32,863 INFO] Saving to /home/bert_data/cnndm.train.0.bert.pt\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g37zVL43ULu3",
        "colab_type": "code",
        "outputId": "440f1424-4926-4140-cb82-c75989081f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls {BERT_DATA_PATH}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnndm.train.0.bert.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gRhwkG3XtLT",
        "colab_type": "code",
        "outputId": "8dccef33-80d5-4699-8bce-6c4924046e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# Can't print insides of saved pytorch dataset\n",
        "cat {BERT_DATA_PATH}cnndm.train.0.bert.pt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-ec52dab5bf53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat {BERT_DATA_PATH}cnndm.train.0.bert.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/alias.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, rest)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m#-----------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0mraw_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PTY_READ_MAX_BYTES_FOR_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0mdecoded_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uoSgF4e_PCb",
        "colab_type": "text"
      },
      "source": [
        "## Additional pyrouge setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl9kUGHasNFq",
        "colab_type": "code",
        "outputId": "4ff2ea8b-fdd2-4fbf-b9f3-652be7aadc22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /home"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIFVL8kGzLi_",
        "colab_type": "code",
        "outputId": "cdd9e008-8668-416d-ac2f-ef733643a6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/andersjo/pyrouge.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyrouge'...\n",
            "remote: Enumerating objects: 393, done.\u001b[K\n",
            "remote: Total 393 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
            "Receiving objects: 100% (393/393), 298.73 KiB | 1009.00 KiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQiCNMdEzQST",
        "colab_type": "code",
        "outputId": "e6ef2fbf-f32e-43db-d588-9288b03679aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd pyrouge"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/pyrouge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMl8VHRc2AvF",
        "colab_type": "code",
        "outputId": "7c269a70-aa6c-4ef3-d1b1-9151be96d2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/pyrouge/tools/ROUGE-1.5.5/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  README.txt  RELEASE-NOTE.txt  \u001b[01;32mROUGE-1.5.5.pl\u001b[0m*  \u001b[01;32mrunROUGE-test.pl\u001b[0m*  \u001b[01;34mXML\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvheSmwIzQd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't need to run assuming pyrogue is already installed \n",
        "#!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY_CG_mqzqC6",
        "colab_type": "code",
        "outputId": "0a42e314-1d11-402d-d1d5-5d3824aa0e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pyrouge_set_rouge_path /home/pyrouge/tools/ROUGE-1.5.5/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-02 06:04:51,661 [MainThread  ] [INFO ]  Set ROUGE home directory to /home/pyrouge/tools/ROUGE-1.5.5/.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7eeaddae-95a0-4db0-a002-e9653e81494d",
        "id": "JVLoN0L2E7_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3369
        }
      },
      "source": [
        "# This makes sure that XML/Parser.pm is installed (required by pyrouge)\n",
        "!sudo apt-get install libxml-parser-perl"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl netbase perl-openssl-defaults\n",
            "Suggested packages:\n",
            "  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libauthen-ntlm-perl\n",
            "The following NEW packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl libxml-parser-perl netbase perl-openssl-defaults\n",
            "0 upgraded, 31 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 1,710 kB of archives.\n",
            "After this operation, 5,567 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 netbase all 5.4 [12.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdata-dump-perl all 1.23-1 [27.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-listing-perl all 6.04-1 [9,774 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfont-afm-perl all 1.20-2 [13.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-form-perl all 6.03-1 [23.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tree-perl all 5.07-1 [200 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-format-perl all 2.12-1 [41.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-cookies-perl all 6.04-1 [17.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-daemon-perl all 6.01-1 [17.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-negotiate-perl all 6.00-2 [13.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 perl-openssl-defaults amd64 3build1 [7,012 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-ssleay-perl amd64 1.84-1build1 [282 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-socket-ssl-perl all 2.056-1 [172 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-http-perl all 6.17-1 [22.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtry-tiny-perl all 0.30-1 [20.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwww-robotrules-perl all 6.01-1 [14.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwww-perl all 6.31-1 [137 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-protocol-https-perl all 6.07-2 [8,284 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5,948 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmailtools-perl all 2.18-1 [74.0 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxml-parser-perl amd64 2.44-2build3 [199 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 libauthen-sasl-perl all 2.1600-1 [48.7 kB]\n",
            "Fetched 1,710 kB in 0s (11.1 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 31.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 130911 files and directories currently installed.)\n",
            "Preparing to unpack .../00-netbase_5.4_all.deb ...\n",
            "Unpacking netbase (5.4) ...\n",
            "Selecting previously unselected package libdata-dump-perl.\n",
            "Preparing to unpack .../01-libdata-dump-perl_1.23-1_all.deb ...\n",
            "Unpacking libdata-dump-perl (1.23-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../02-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../03-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../04-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libfile-listing-perl.\n",
            "Preparing to unpack .../05-libfile-listing-perl_6.04-1_all.deb ...\n",
            "Unpacking libfile-listing-perl (6.04-1) ...\n",
            "Selecting previously unselected package libfont-afm-perl.\n",
            "Preparing to unpack .../06-libfont-afm-perl_1.20-2_all.deb ...\n",
            "Unpacking libfont-afm-perl (1.20-2) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../07-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../08-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../09-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../10-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../11-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../12-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libhtml-form-perl.\n",
            "Preparing to unpack .../13-libhtml-form-perl_6.03-1_all.deb ...\n",
            "Unpacking libhtml-form-perl (6.03-1) ...\n",
            "Selecting previously unselected package libhtml-tree-perl.\n",
            "Preparing to unpack .../14-libhtml-tree-perl_5.07-1_all.deb ...\n",
            "Unpacking libhtml-tree-perl (5.07-1) ...\n",
            "Selecting previously unselected package libhtml-format-perl.\n",
            "Preparing to unpack .../15-libhtml-format-perl_2.12-1_all.deb ...\n",
            "Unpacking libhtml-format-perl (2.12-1) ...\n",
            "Selecting previously unselected package libhttp-cookies-perl.\n",
            "Preparing to unpack .../16-libhttp-cookies-perl_6.04-1_all.deb ...\n",
            "Unpacking libhttp-cookies-perl (6.04-1) ...\n",
            "Selecting previously unselected package libhttp-daemon-perl.\n",
            "Preparing to unpack .../17-libhttp-daemon-perl_6.01-1_all.deb ...\n",
            "Unpacking libhttp-daemon-perl (6.01-1) ...\n",
            "Selecting previously unselected package libhttp-negotiate-perl.\n",
            "Preparing to unpack .../18-libhttp-negotiate-perl_6.00-2_all.deb ...\n",
            "Unpacking libhttp-negotiate-perl (6.00-2) ...\n",
            "Selecting previously unselected package perl-openssl-defaults:amd64.\n",
            "Preparing to unpack .../19-perl-openssl-defaults_3build1_amd64.deb ...\n",
            "Unpacking perl-openssl-defaults:amd64 (3build1) ...\n",
            "Selecting previously unselected package libnet-ssleay-perl.\n",
            "Preparing to unpack .../20-libnet-ssleay-perl_1.84-1build1_amd64.deb ...\n",
            "Unpacking libnet-ssleay-perl (1.84-1build1) ...\n",
            "Selecting previously unselected package libio-socket-ssl-perl.\n",
            "Preparing to unpack .../21-libio-socket-ssl-perl_2.056-1_all.deb ...\n",
            "Unpacking libio-socket-ssl-perl (2.056-1) ...\n",
            "Selecting previously unselected package libnet-http-perl.\n",
            "Preparing to unpack .../22-libnet-http-perl_6.17-1_all.deb ...\n",
            "Unpacking libnet-http-perl (6.17-1) ...\n",
            "Selecting previously unselected package libtry-tiny-perl.\n",
            "Preparing to unpack .../23-libtry-tiny-perl_0.30-1_all.deb ...\n",
            "Unpacking libtry-tiny-perl (0.30-1) ...\n",
            "Selecting previously unselected package libwww-robotrules-perl.\n",
            "Preparing to unpack .../24-libwww-robotrules-perl_6.01-1_all.deb ...\n",
            "Unpacking libwww-robotrules-perl (6.01-1) ...\n",
            "Selecting previously unselected package libwww-perl.\n",
            "Preparing to unpack .../25-libwww-perl_6.31-1_all.deb ...\n",
            "Unpacking libwww-perl (6.31-1) ...\n",
            "Selecting previously unselected package liblwp-protocol-https-perl.\n",
            "Preparing to unpack .../26-liblwp-protocol-https-perl_6.07-2_all.deb ...\n",
            "Unpacking liblwp-protocol-https-perl (6.07-2) ...\n",
            "Selecting previously unselected package libnet-smtp-ssl-perl.\n",
            "Preparing to unpack .../27-libnet-smtp-ssl-perl_1.04-1_all.deb ...\n",
            "Unpacking libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Selecting previously unselected package libmailtools-perl.\n",
            "Preparing to unpack .../28-libmailtools-perl_2.18-1_all.deb ...\n",
            "Unpacking libmailtools-perl (2.18-1) ...\n",
            "Selecting previously unselected package libxml-parser-perl.\n",
            "Preparing to unpack .../29-libxml-parser-perl_2.44-2build3_amd64.deb ...\n",
            "Unpacking libxml-parser-perl (2.44-2build3) ...\n",
            "Selecting previously unselected package libauthen-sasl-perl.\n",
            "Preparing to unpack .../30-libauthen-sasl-perl_2.1600-1_all.deb ...\n",
            "Unpacking libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libtry-tiny-perl (0.30-1) ...\n",
            "Setting up libfont-afm-perl (1.20-2) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up perl-openssl-defaults:amd64 (3build1) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libdata-dump-perl (1.23-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libnet-http-perl (6.17-1) ...\n",
            "Setting up libwww-robotrules-perl (6.01-1) ...\n",
            "Setting up libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up netbase (5.4) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libnet-ssleay-perl (1.84-1build1) ...\n",
            "Setting up libio-socket-ssl-perl (2.056-1) ...\n",
            "Setting up libhtml-tree-perl (5.07-1) ...\n",
            "Setting up libfile-listing-perl (6.04-1) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libhttp-negotiate-perl (6.00-2) ...\n",
            "Setting up libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Setting up libhtml-format-perl (2.12-1) ...\n",
            "Setting up libhttp-cookies-perl (6.04-1) ...\n",
            "Setting up libhttp-daemon-perl (6.01-1) ...\n",
            "Setting up libhtml-form-perl (6.03-1) ...\n",
            "Setting up libmailtools-perl (2.18-1) ...\n",
            "Setting up liblwp-protocol-https-perl (6.07-2) ...\n",
            "Setting up libwww-perl (6.31-1) ...\n",
            "Setting up libxml-parser-perl (2.44-2build3) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1d21581d-77f8-4892-a869-2bdc56a21adc",
        "id": "c5SOZl3GE7_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /home/pyrouge/tools/ROUGE-1.5.5/data/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/pyrouge/tools/ROUGE-1.5.5/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oS6WoMWgE7_r",
        "colab": {}
      },
      "source": [
        "rm WordNet-2.0.exc.db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rlw70vWyE7_s",
        "colab": {}
      },
      "source": [
        "!./WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPhO1bu411g7",
        "colab_type": "code",
        "outputId": "d3acd9ce-94f3-40ab-fcb3-6f74dfc4e90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2604
        }
      },
      "source": [
        "!python -m pyrouge.test"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-02 06:05:09,469 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmpque5f5v9/rouge_conf.xml\n",
            "F2019-06-02 06:05:09,550 [MainThread  ] [INFO ]  Processing files in data/SL2003_models_plain_text.\n",
            "2019-06-02 06:05:09,551 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-19.html.\n",
            "2019-06-02 06:05:09,551 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-05.html.\n",
            "2019-06-02 06:05:09,551 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-18.html.\n",
            "2019-06-02 06:05:09,551 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-06.html.\n",
            "2019-06-02 06:05:09,551 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-16.html.\n",
            "2019-06-02 06:05:09,551 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-20.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-02.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-22.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-10.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-03.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-07.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-25.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-08.html.\n",
            "2019-06-02 06:05:09,552 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-15.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-09.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-13.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-01.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-17.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-12.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-21.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-24.html.\n",
            "2019-06-02 06:05:09,553 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-11.html.\n",
            "2019-06-02 06:05:09,554 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-14.html.\n",
            "2019-06-02 06:05:09,554 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-04.html.\n",
            "2019-06-02 06:05:09,554 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-23.html.\n",
            "2019-06-02 06:05:09,554 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp_68ap5hl.\n",
            ".2019-06-02 06:05:09,566 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmpqrcv89z1/rouge_conf.xml\n",
            "2019-06-02 06:05:09,566 [MainThread  ] [INFO ]  Running ROUGE with command /home/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /home/pyrouge/tools/ROUGE-1.5.5/data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m /tmp/tmpqrcv89z1/rouge_conf.xml\n",
            "F2019-06-02 06:05:11,311 [MainThread  ] [INFO ]  Processing files in data/SL2003_models_rouge_format.\n",
            "2019-06-02 06:05:11,311 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-19.html.\n",
            "/usr/local/bin/pyrouge_convert_rouge_format_to_plain_text:14: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 14 of the file /usr/local/bin/pyrouge_convert_rouge_format_to_plain_text. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(html)\n",
            "2019-06-02 06:05:11,313 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-05.html.\n",
            "2019-06-02 06:05:11,313 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-18.html.\n",
            "2019-06-02 06:05:11,314 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-06.html.\n",
            "2019-06-02 06:05:11,314 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-16.html.\n",
            "2019-06-02 06:05:11,315 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-20.html.\n",
            "2019-06-02 06:05:11,315 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-02.html.\n",
            "2019-06-02 06:05:11,316 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-22.html.\n",
            "2019-06-02 06:05:11,317 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-10.html.\n",
            "2019-06-02 06:05:11,317 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-03.html.\n",
            "2019-06-02 06:05:11,318 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-07.html.\n",
            "2019-06-02 06:05:11,318 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-25.html.\n",
            "2019-06-02 06:05:11,319 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-08.html.\n",
            "2019-06-02 06:05:11,319 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-15.html.\n",
            "2019-06-02 06:05:11,320 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-09.html.\n",
            "2019-06-02 06:05:11,320 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-13.html.\n",
            "2019-06-02 06:05:11,321 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-01.html.\n",
            "2019-06-02 06:05:11,321 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-17.html.\n",
            "2019-06-02 06:05:11,322 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-12.html.\n",
            "2019-06-02 06:05:11,323 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-21.html.\n",
            "2019-06-02 06:05:11,323 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-24.html.\n",
            "2019-06-02 06:05:11,324 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-11.html.\n",
            "2019-06-02 06:05:11,324 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-14.html.\n",
            "2019-06-02 06:05:11,325 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-04.html.\n",
            "2019-06-02 06:05:11,325 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-23.html.\n",
            "2019-06-02 06:05:11,326 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmptwhkdjt2.\n",
            ".E.2019-06-02 06:05:11,427 [MainThread  ] [INFO ]  Writing summaries.\n",
            "2019-06-02 06:05:11,428 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /tmp/tmpizikwzqc/system and model files to /tmp/tmpizikwzqc/model.\n",
            "2019-06-02 06:05:11,428 [MainThread  ] [INFO ]  Processing files in data/systems_plain.\n",
            "2019-06-02 06:05:11,428 [MainThread  ] [INFO ]  Processing D30001.M.100.T.A.\n",
            "2019-06-02 06:05:11,428 [MainThread  ] [INFO ]  Processing D30002.M.100.T.A.\n",
            "2019-06-02 06:05:11,429 [MainThread  ] [INFO ]  Processing D30005.M.100.T.A.\n",
            "2019-06-02 06:05:11,429 [MainThread  ] [INFO ]  Processing D30003.M.100.T.A.\n",
            "2019-06-02 06:05:11,429 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmpizikwzqc/system.\n",
            "2019-06-02 06:05:11,429 [MainThread  ] [INFO ]  Processing files in data/models_plain.\n",
            "2019-06-02 06:05:11,429 [MainThread  ] [INFO ]  Processing D30002.M.100.T.E.\n",
            "2019-06-02 06:05:11,429 [MainThread  ] [INFO ]  Processing D30001.M.100.T.D.\n",
            "2019-06-02 06:05:11,429 [MainThread  ] [INFO ]  Processing D30005.M.100.T.B.\n",
            "2019-06-02 06:05:11,430 [MainThread  ] [INFO ]  Processing D30003.M.100.T.B.\n",
            "2019-06-02 06:05:11,430 [MainThread  ] [INFO ]  Processing D30005.M.100.T.C.\n",
            "2019-06-02 06:05:11,430 [MainThread  ] [INFO ]  Processing D30002.M.100.T.B.\n",
            "2019-06-02 06:05:11,430 [MainThread  ] [INFO ]  Processing D30001.M.100.T.C.\n",
            "2019-06-02 06:05:11,430 [MainThread  ] [INFO ]  Processing D30001.M.100.T.B.\n",
            "2019-06-02 06:05:11,430 [MainThread  ] [INFO ]  Processing D30003.M.100.T.F.\n",
            "2019-06-02 06:05:11,430 [MainThread  ] [INFO ]  Processing D30002.M.100.T.C.\n",
            "2019-06-02 06:05:11,431 [MainThread  ] [INFO ]  Processing D30005.M.100.T.G.\n",
            "2019-06-02 06:05:11,431 [MainThread  ] [INFO ]  Processing D30003.M.100.T.C.\n",
            "2019-06-02 06:05:11,431 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmpizikwzqc/model.\n",
            "2019-06-02 06:05:11,432 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmpjvt9snk8/rouge_conf.xml\n",
            "2019-06-02 06:05:11,432 [MainThread  ] [INFO ]  Running ROUGE with command /home/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /home/pyrouge/tools/ROUGE-1.5.5/data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m /tmp/tmpjvt9snk8/rouge_conf.xml\n",
            "F.E..\n",
            "======================================================================\n",
            "ERROR: test_options (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 218, in test_options\n",
            "    pyrouge_output = check_output_clean(pyrouge_command)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 17, in <lambda>\n",
            "    check_output_clean = lambda c: check_output(c).decode(\"UTF-8\").strip()\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 336, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 403, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 709, in __init__\n",
            "    restore_signals, start_new_session)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyrouge_evaluate_plain_text_files.py': 'pyrouge_evaluate_plain_text_files.py'\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_write_config (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 197, in test_write_config\n",
            "    check_output(command.split())\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 336, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 403, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 709, in __init__\n",
            "    restore_signals, start_new_session)\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 1344, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyrouge_write_config_file.py': 'pyrouge_write_config_file.py'\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_config_file (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 147, in test_config_file\n",
            "    add_data_path(\"ROUGE-test_11.xml\")))\n",
            "AssertionError: False is not true\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_evaluation (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 164, in test_evaluation\n",
            "    self.assertEqual(pyrouge_output, orig_rouge_output)\n",
            "AssertionError: '----[62 chars]R: 0.22552 (95%-conf.int. 0.18148 - 0.27060)\\n[1839 chars]467)' != '----[62 chars]R: 0.77625 (95%-conf.int. 0.76493 - 0.78766)\\n[1839 chars]402)'\n",
            "Diff is 5944 characters long. Set self.maxDiff to None to see it.\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_rouge_for_plain_text (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pyrouge/tests/Rouge155_test.py\", line 183, in test_rouge_for_plain_text\n",
            "    self.assertEqual(pyrouge_output, orig_rouge_output)\n",
            "AssertionError: '----[61 chars]R: 0.39343 (95%-conf.int. 0.37315 - 0.40507)\\n[1816 chars]275)' != '----[61 chars]R: 0.60964 (95%-conf.int. 0.56136 - 0.66437)\\n[1816 chars]163)'\n",
            "Diff is 6000 characters long. Set self.maxDiff to None to see it.\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 11 tests in 4.399s\n",
            "\n",
            "FAILED (failures=3, errors=2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPMqoi-nbW2G",
        "colab_type": "text"
      },
      "source": [
        "## BERTSUM Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NCtuJElbaI_",
        "colab_type": "text"
      },
      "source": [
        "### Load model and sample data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HjaAwgXSp90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dl_gcs(object_fp, dest_fp, bucket_name='nlp-experiment-datasets', project_id='bert-experiments'):\n",
        "    from googleapiclient.discovery import build\n",
        "    gcs_service = build('storage', 'v1')\n",
        "    from apiclient.http import MediaIoBaseDownload\n",
        "    with open(dest_fp, 'wb') as f:\n",
        "        # Download the file from a given Google Cloud Storage bucket.\n",
        "        request = gcs_service.objects().get_media(bucket=bucket_name,\n",
        "                                            object=f'{object_fp}')\n",
        "        media = MediaIoBaseDownload(f, request)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "            _, done = media.next_chunk()        \n",
        "    print('Download complete')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wHkLLz6TaVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS_DIR = '/home/models/'\n",
        "!mkdir {MODELS_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYP8zKcwePNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL = 'model_step_43000.pt'\n",
        "MODEL_FP = f'{MODELS_DIR}{MODEL}'\n",
        "SAMPLE_DATA_FP = '/home/sample_data/cnndm.test.0.bert.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGfZ_jMAEe35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /home/sample_data/ # Source of data\n",
        "!mkdir /home/bert_results/ # Destination of results\n",
        "!mkdir /home/temp/ \n",
        "!mkdir /home/logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfrsJH-rTPI4",
        "colab_type": "code",
        "outputId": "826489b7-0b9c-4726-e8f9-281ea7a187e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Download trained model\n",
        "dl_gcs('bertsum/cnn_dailymail/models/model_step_43000.pt', MODEL_FP)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzYKPnYtEEzv",
        "colab_type": "code",
        "outputId": "193eb832-1f8e-4286-e037-5cd2491ad11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Download sample test data\n",
        "dl_gcs('bertsum/cnn_dailymail/sample_data/cnndm.test.0.bert.pt', SAMPLE_DATA_FP)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQifkIF3UyNR",
        "colab_type": "code",
        "outputId": "2398929f-94c5-4b9e-c2e3-2267b124a379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls {MODELS_DIR}"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_step_43000.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3QDd4C0lWDs",
        "colab_type": "code",
        "outputId": "5713b280-3750-46f0-93a7-be11a6f74637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/sample_data/"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnndm.test.0.bert.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p9OzxbsF832",
        "colab_type": "code",
        "outputId": "efe2c0c7-b8a5-40a3-f73f-22e680d00689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "cat /home/sample_data/cnndm.test.0.bert.pt"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8c36b58c4712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat /home/sample_data/cnndm.test.0.bert.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/alias.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, rest)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m#-----------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0mraw_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PTY_READ_MAX_BYTES_FOR_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0mdecoded_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJB92Mg3rKXX",
        "colab_type": "text"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCUp_r4rrvdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From src/models/optimizers.py\n",
        "\"\"\" Optimizers class \"\"\"\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "# from onmt.utils import use_gpu\n",
        "\n",
        "\n",
        "def use_gpu(opt):\n",
        "    \"\"\"\n",
        "    Creates a boolean if gpu used\n",
        "    \"\"\"\n",
        "    return (hasattr(opt, 'gpu_ranks') and len(opt.gpu_ranks) > 0) or \\\n",
        "           (hasattr(opt, 'gpu') and opt.gpu > -1)\n",
        "\n",
        "def build_optim(model, opt, checkpoint):\n",
        "    \"\"\" Build optimizer \"\"\"\n",
        "    saved_optimizer_state_dict = None\n",
        "\n",
        "    if opt.train_from:\n",
        "        optim = checkpoint['optim']\n",
        "        # We need to save a copy of optim.optimizer.state_dict() for setting\n",
        "        # the, optimizer state later on in Stage 2 in this method, since\n",
        "        # the method optim.set_parameters(model.parameters()) will overwrite\n",
        "        # optim.optimizer, and with ith the values stored in\n",
        "        # optim.optimizer.state_dict()\n",
        "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
        "    else:\n",
        "        optim = Optimizer(\n",
        "            opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
        "            lr_decay=opt.learning_rate_decay,\n",
        "            start_decay_steps=opt.start_decay_steps,\n",
        "            decay_steps=opt.decay_steps,\n",
        "            beta1=opt.adam_beta1,\n",
        "            beta2=opt.adam_beta2,\n",
        "            adagrad_accum=opt.adagrad_accumulator_init,\n",
        "            decay_method=opt.decay_method,\n",
        "            warmup_steps=opt.warmup_steps)\n",
        "\n",
        "    # Stage 1:\n",
        "    # Essentially optim.set_parameters (re-)creates and optimizer using\n",
        "    # model.paramters() as parameters that will be stored in the\n",
        "    # optim.optimizer.param_groups field of the torch optimizer class.\n",
        "    # Importantly, this method does not yet load the optimizer state, as\n",
        "    # essentially it builds a new optimizer with empty optimizer state and\n",
        "    # parameters from the model.\n",
        "    optim.set_parameters(model.named_parameters())\n",
        "\n",
        "    if opt.train_from:\n",
        "        # Stage 2: In this stage, which is only performed when loading an\n",
        "        # optimizer from a checkpoint, we load the saved_optimizer_state_dict\n",
        "        # into the re-created optimizer, to set the optim.optimizer.state\n",
        "        # field, which was previously empty. For this, we use the optimizer\n",
        "        # state saved in the \"saved_optimizer_state_dict\" variable for\n",
        "        # this purpose.\n",
        "        # See also: https://github.com/pytorch/pytorch/issues/2830\n",
        "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
        "        # Convert back the state values to cuda type if applicable\n",
        "        if use_gpu(opt):\n",
        "            for state in optim.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.cuda()\n",
        "\n",
        "        # We want to make sure that indeed we have a non-empty optimizer state\n",
        "        # when we loaded an existing model. This should be at least the case\n",
        "        # for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state\n",
        "        # (Exponential moving average of gradient and squared gradient values)\n",
        "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
        "            raise RuntimeError(\n",
        "                \"Error: loaded Adam optimizer from existing model\" +\n",
        "                \" but optimizer state is empty\")\n",
        "\n",
        "    return optim\n",
        "\n",
        "\n",
        "class MultipleOptimizer(object):\n",
        "    \"\"\" Implement multiple optimizers needed for sparse adam \"\"\"\n",
        "\n",
        "    def __init__(self, op):\n",
        "        \"\"\" ? \"\"\"\n",
        "        self.optimizers = op\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        for op in self.optimizers:\n",
        "            op.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        for op in self.optimizers:\n",
        "            op.step()\n",
        "\n",
        "    @property\n",
        "    def state(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        return {k: v for op in self.optimizers for k, v in op.state.items()}\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        return [op.state_dict() for op in self.optimizers]\n",
        "\n",
        "    def load_state_dict(self, state_dicts):\n",
        "        \"\"\" ? \"\"\"\n",
        "        assert len(state_dicts) == len(self.optimizers)\n",
        "        for i in range(len(state_dicts)):\n",
        "            self.optimizers[i].load_state_dict(state_dicts[i])\n",
        "\n",
        "\n",
        "class Optimizer(object):\n",
        "    \"\"\"\n",
        "    Controller class for optimization. Mostly a thin\n",
        "    wrapper for `optim`, but also useful for implementing\n",
        "    rate scheduling beyond what is currently available.\n",
        "    Also implements necessary methods for training RNNs such\n",
        "    as grad manipulations.\n",
        "    Args:\n",
        "      method (:obj:`str`): one of [sgd, adagrad, adadelta, adam]\n",
        "      lr (float): learning rate\n",
        "      lr_decay (float, optional): learning rate decay multiplier\n",
        "      start_decay_steps (int, optional): step to start learning rate decay\n",
        "      beta1, beta2 (float, optional): parameters for adam\n",
        "      adagrad_accum (float, optional): initialization parameter for adagrad\n",
        "      decay_method (str, option): custom decay options\n",
        "      warmup_steps (int, option): parameter for `noam` decay\n",
        "    We use the default parameters for Adam that are suggested by\n",
        "    the original paper https://arxiv.org/pdf/1412.6980.pdf\n",
        "    These values are also used by other established implementations,\n",
        "    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
        "    https://keras.io/optimizers/\n",
        "    Recently there are slightly different values used in the paper\n",
        "    \"Attention is all you need\"\n",
        "    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n",
        "    was used there however, beta2=0.999 is still arguably the more\n",
        "    established value, so we use that here as well\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, method, learning_rate, max_grad_norm,\n",
        "                 lr_decay=1, start_decay_steps=None, decay_steps=None,\n",
        "                 beta1=0.9, beta2=0.999,\n",
        "                 adagrad_accum=0.0,\n",
        "                 decay_method=None,\n",
        "                 warmup_steps=4000\n",
        "                 ):\n",
        "        self.last_ppl = None\n",
        "        self.learning_rate = learning_rate\n",
        "        self.original_lr = learning_rate\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.method = method\n",
        "        self.lr_decay = lr_decay\n",
        "        self.start_decay_steps = start_decay_steps\n",
        "        self.decay_steps = decay_steps\n",
        "        self.start_decay = False\n",
        "        self._step = 0\n",
        "        self.betas = [beta1, beta2]\n",
        "        self.adagrad_accum = adagrad_accum\n",
        "        self.decay_method = decay_method\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        \"\"\" ? \"\"\"\n",
        "        self.params = []\n",
        "        self.sparse_params = []\n",
        "        for k, p in params:\n",
        "            if p.requires_grad:\n",
        "                if self.method != 'sparseadam' or \"embed\" not in k:\n",
        "                    self.params.append(p)\n",
        "                else:\n",
        "                    self.sparse_params.append(p)\n",
        "        if self.method == 'sgd':\n",
        "            self.optimizer = optim.SGD(self.params, lr=self.learning_rate)\n",
        "        elif self.method == 'adagrad':\n",
        "            self.optimizer = optim.Adagrad(self.params, lr=self.learning_rate)\n",
        "            for group in self.optimizer.param_groups:\n",
        "                for p in group['params']:\n",
        "                    self.optimizer.state[p]['sum'] = self.optimizer\\\n",
        "                        .state[p]['sum'].fill_(self.adagrad_accum)\n",
        "        elif self.method == 'adadelta':\n",
        "            self.optimizer = optim.Adadelta(self.params, lr=self.learning_rate)\n",
        "        elif self.method == 'adam':\n",
        "            self.optimizer = optim.Adam(self.params, lr=self.learning_rate,\n",
        "                                        betas=self.betas, eps=1e-9)\n",
        "        elif self.method == 'sparseadam':\n",
        "            self.optimizer = MultipleOptimizer(\n",
        "                [optim.Adam(self.params, lr=self.learning_rate,\n",
        "                            betas=self.betas, eps=1e-8),\n",
        "                 optim.SparseAdam(self.sparse_params, lr=self.learning_rate,\n",
        "                                  betas=self.betas, eps=1e-8)])\n",
        "        else:\n",
        "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
        "\n",
        "    def _set_rate(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        if self.method != 'sparseadam':\n",
        "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
        "        else:\n",
        "            for op in self.optimizer.optimizers:\n",
        "                op.param_groups[0]['lr'] = self.learning_rate\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Update the model parameters based on current gradients.\n",
        "        Optionally, will employ gradient modification or update learning\n",
        "        rate.\n",
        "        \"\"\"\n",
        "        self._step += 1\n",
        "\n",
        "        # Decay method used in tensor2tensor.\n",
        "        if self.decay_method == \"noam\":\n",
        "            self._set_rate(\n",
        "                self.original_lr *\n",
        "\n",
        "                 min(self._step ** (-0.5),\n",
        "                     self._step * self.warmup_steps**(-1.5)))\n",
        "\n",
        "            # self._set_rate(self.original_lr *self.model_size ** (-0.5) *min(1.0, self._step / self.warmup_steps)*max(self._step, self.warmup_steps)**(-0.5))\n",
        "        # Decay based on start_decay_steps every decay_steps\n",
        "        else:\n",
        "            if ((self.start_decay_steps is not None) and (\n",
        "                     self._step >= self.start_decay_steps)):\n",
        "                self.start_decay = True\n",
        "            if self.start_decay:\n",
        "                if ((self._step - self.start_decay_steps)\n",
        "                   % self.decay_steps == 0):\n",
        "                    self.learning_rate = self.learning_rate * self.lr_decay\n",
        "\n",
        "        if self.method != 'sparseadam':\n",
        "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
        "\n",
        "        if self.max_grad_norm:\n",
        "            clip_grad_norm_(self.params, self.max_grad_norm)\n",
        "        self.optimizer.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5LMTHWzr__F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From src/models/neural.py\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n",
        "    Args:\n",
        "        d_model (int): the size of input for the first-layer of the FFN.\n",
        "        d_ff (int): the hidden layer size of the second-layer\n",
        "            of the FNN.\n",
        "        dropout (float): dropout probability in :math:`[0, 1)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.actv = gelu\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inter = self.dropout_1(self.actv(self.w_1(self.layer_norm(x))))\n",
        "        output = self.dropout_2(self.w_2(inter))\n",
        "        return output + x\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention module from\n",
        "    \"Attention is All You Need\"\n",
        "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n",
        "    Similar to standard `dot` attention but uses\n",
        "    multiple attention distributions simulataneously\n",
        "    to select relevant items.\n",
        "    .. mermaid::\n",
        "       graph BT\n",
        "          A[key]\n",
        "          B[value]\n",
        "          C[query]\n",
        "          O[output]\n",
        "          subgraph Attn\n",
        "            D[Attn 1]\n",
        "            E[Attn 2]\n",
        "            F[Attn N]\n",
        "          end\n",
        "          A --> D\n",
        "          C --> D\n",
        "          A --> E\n",
        "          C --> E\n",
        "          A --> F\n",
        "          C --> F\n",
        "          D --> O\n",
        "          E --> O\n",
        "          F --> O\n",
        "          B --> O\n",
        "    Also includes several additional tricks.\n",
        "    Args:\n",
        "       head_count (int): number of parallel heads\n",
        "       model_dim (int): the dimension of keys/values/queries,\n",
        "           must be divisible by head_count\n",
        "       dropout (float): dropout parameter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
        "        assert model_dim % head_count == 0\n",
        "        self.dim_per_head = model_dim // head_count\n",
        "        self.model_dim = model_dim\n",
        "\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.head_count = head_count\n",
        "\n",
        "        self.linear_keys = nn.Linear(model_dim,\n",
        "                                     head_count * self.dim_per_head)\n",
        "        self.linear_values = nn.Linear(model_dim,\n",
        "                                       head_count * self.dim_per_head)\n",
        "        self.linear_query = nn.Linear(model_dim,\n",
        "                                      head_count * self.dim_per_head)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.use_final_linear = use_final_linear\n",
        "        if (self.use_final_linear):\n",
        "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def forward(self, key, value, query, mask=None,\n",
        "                layer_cache=None, type=None, predefined_graph_1=None):\n",
        "        \"\"\"\n",
        "        Compute the context vector and the attention vectors.\n",
        "        Args:\n",
        "           key (`FloatTensor`): set of `key_len`\n",
        "                key vectors `[batch, key_len, dim]`\n",
        "           value (`FloatTensor`): set of `key_len`\n",
        "                value vectors `[batch, key_len, dim]`\n",
        "           query (`FloatTensor`): set of `query_len`\n",
        "                 query vectors  `[batch, query_len, dim]`\n",
        "           mask: binary mask indicating which keys have\n",
        "                 non-zero attention `[batch, query_len, key_len]`\n",
        "        Returns:\n",
        "           (`FloatTensor`, `FloatTensor`) :\n",
        "           * output context vectors `[batch, query_len, dim]`\n",
        "           * one of the attention vectors `[batch, query_len, key_len]`\n",
        "        \"\"\"\n",
        "\n",
        "        # CHECKS\n",
        "        # batch, k_len, d = key.size()\n",
        "        # batch_, k_len_, d_ = value.size()\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(k_len, k_len_)\n",
        "        # aeq(d, d_)\n",
        "        # batch_, q_len, d_ = query.size()\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(d, d_)\n",
        "        # aeq(self.model_dim % 8, 0)\n",
        "        # if mask is not None:\n",
        "        #    batch_, q_len_, k_len_ = mask.size()\n",
        "        #    aeq(batch_, batch)\n",
        "        #    aeq(k_len_, k_len)\n",
        "        #    aeq(q_len_ == q_len)\n",
        "        # END CHECKS\n",
        "\n",
        "        batch_size = key.size(0)\n",
        "        dim_per_head = self.dim_per_head\n",
        "        head_count = self.head_count\n",
        "        key_len = key.size(1)\n",
        "        query_len = query.size(1)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\"  projection \"\"\"\n",
        "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
        "                .transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            \"\"\"  compute context \"\"\"\n",
        "            return x.transpose(1, 2).contiguous() \\\n",
        "                .view(batch_size, -1, head_count * dim_per_head)\n",
        "\n",
        "        # 1) Project key, value, and query.\n",
        "        if layer_cache is not None:\n",
        "            if type == \"self\":\n",
        "                query, key, value = self.linear_query(query), \\\n",
        "                                    self.linear_keys(query), \\\n",
        "                                    self.linear_values(query)\n",
        "\n",
        "                key = shape(key)\n",
        "                value = shape(value)\n",
        "\n",
        "                if layer_cache is not None:\n",
        "                    device = key.device\n",
        "                    if layer_cache[\"self_keys\"] is not None:\n",
        "                        key = torch.cat(\n",
        "                            (layer_cache[\"self_keys\"].to(device), key),\n",
        "                            dim=2)\n",
        "                    if layer_cache[\"self_values\"] is not None:\n",
        "                        value = torch.cat(\n",
        "                            (layer_cache[\"self_values\"].to(device), value),\n",
        "                            dim=2)\n",
        "                    layer_cache[\"self_keys\"] = key\n",
        "                    layer_cache[\"self_values\"] = value\n",
        "            elif type == \"context\":\n",
        "                query = self.linear_query(query)\n",
        "                if layer_cache is not None:\n",
        "                    if layer_cache[\"memory_keys\"] is None:\n",
        "                        key, value = self.linear_keys(key), \\\n",
        "                                     self.linear_values(value)\n",
        "                        key = shape(key)\n",
        "                        value = shape(value)\n",
        "                    else:\n",
        "                        key, value = layer_cache[\"memory_keys\"], \\\n",
        "                                     layer_cache[\"memory_values\"]\n",
        "                    layer_cache[\"memory_keys\"] = key\n",
        "                    layer_cache[\"memory_values\"] = value\n",
        "                else:\n",
        "                    key, value = self.linear_keys(key), \\\n",
        "                                 self.linear_values(value)\n",
        "                    key = shape(key)\n",
        "                    value = shape(value)\n",
        "        else:\n",
        "            key = self.linear_keys(key)\n",
        "            value = self.linear_values(value)\n",
        "            query = self.linear_query(query)\n",
        "            key = shape(key)\n",
        "            value = shape(value)\n",
        "\n",
        "        query = shape(query)\n",
        "\n",
        "        key_len = key.size(2)\n",
        "        query_len = query.size(2)\n",
        "\n",
        "        # 2) Calculate and scale scores.\n",
        "        query = query / math.sqrt(dim_per_head)\n",
        "        scores = torch.matmul(query, key.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand_as(scores)\n",
        "            scores = scores.masked_fill(mask, -1e18)\n",
        "\n",
        "        # 3) Apply attention dropout and compute context vectors.\n",
        "\n",
        "        attn = self.softmax(scores)\n",
        "\n",
        "        if (not predefined_graph_1 is None):\n",
        "            attn_masked = attn[:, -1] * predefined_graph_1\n",
        "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
        "\n",
        "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
        "\n",
        "        drop_attn = self.dropout(attn)\n",
        "        if (self.use_final_linear):\n",
        "            context = unshape(torch.matmul(drop_attn, value))\n",
        "            output = self.final_linear(context)\n",
        "            return output\n",
        "        else:\n",
        "            context = torch.matmul(drop_attn, value)\n",
        "            return context\n",
        "\n",
        "        # CHECK\n",
        "        # batch_, q_len_, d_ = output.size()\n",
        "        # aeq(q_len, q_len_)\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(d, d_)\n",
        "\n",
        "        # Return one attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNV_julAsEwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From /src/models/rnn.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LayerNormLSTMCell(nn.LSTMCell):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super().__init__(input_size, hidden_size, bias)\n",
        "\n",
        "        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n",
        "        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n",
        "        self.ln_ho = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        self.check_forward_input(input)\n",
        "        if hidden is None:\n",
        "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "        else:\n",
        "            hx, cx = hidden\n",
        "        self.check_forward_hidden(input, hx, '[0]')\n",
        "        self.check_forward_hidden(input, cx, '[1]')\n",
        "\n",
        "        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n",
        "                + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n",
        "        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n",
        "        g = gates[:, (3 * self.hidden_size):].tanh()\n",
        "\n",
        "        cy = (f * cx) + (i * g)\n",
        "        hy = o * self.ln_ho(cy).tanh()\n",
        "        return hy, cy\n",
        "\n",
        "\n",
        "class LayerNormLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.hidden0 = nn.ModuleList([\n",
        "            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
        "                              hidden_size=hidden_size, bias=bias)\n",
        "            for layer in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        if self.bidirectional:\n",
        "            self.hidden1 = nn.ModuleList([\n",
        "                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
        "                                  hidden_size=hidden_size, bias=bias)\n",
        "                for layer in range(num_layers)\n",
        "            ])\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n",
        "        num_directions = 2 if self.bidirectional else 1\n",
        "        if hidden is None:\n",
        "            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
        "            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
        "        else:\n",
        "            hx, cx = hidden\n",
        "\n",
        "        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
        "        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
        "\n",
        "        if self.bidirectional:\n",
        "            xs = input\n",
        "            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n",
        "                l0, l1 = 2 * l, 2 * l + 1\n",
        "                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n",
        "                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n",
        "                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\n",
        "                    h0, c0 = ht[t][l0], ct[t][l0]\n",
        "                    t = seq_len - 1 - t\n",
        "                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\n",
        "                    h1, c1 = ht[t][l1], ct[t][l1]\n",
        "                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\n",
        "            y = torch.stack(xs)\n",
        "            hy = torch.stack(ht[-1])\n",
        "            cy = torch.stack(ct[-1])\n",
        "        else:\n",
        "            h, c = hx, cx\n",
        "            for t, x in enumerate(input):\n",
        "                for l, layer in enumerate(self.hidden0):\n",
        "                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\n",
        "                    x = ht[t][l]\n",
        "                h, c = ht[t], ct[t]\n",
        "            y = torch.stack([h[-1] for h in ht])\n",
        "            hy = torch.stack(ht[-1])\n",
        "            cy = torch.stack(ct[-1])\n",
        "\n",
        "        return y, (hy, cy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCF3pdh6rSyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From /src/models/encoder.py\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from models.neural import MultiHeadedAttention, PositionwiseFeedForward\n",
        "#from models.rnn import LayerNormLSTM\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear1 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask_cls):\n",
        "        h = self.linear1(x).squeeze(-1)\n",
        "        sent_scores = self.sigmoid(h) * mask_cls.float()\n",
        "        return sent_scores\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout, dim, max_len=5000):\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
        "                              -(math.log(10000.0) / dim)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, emb, step=None):\n",
        "        emb = emb * math.sqrt(self.dim)\n",
        "        if (step):\n",
        "            emb = emb + self.pe[:, step][:, None, :]\n",
        "\n",
        "        else:\n",
        "            emb = emb + self.pe[:, :emb.size(1)]\n",
        "        emb = self.dropout(emb)\n",
        "        return emb\n",
        "\n",
        "    def get_emb(self, emb):\n",
        "        return self.pe[:, :emb.size(1)]\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadedAttention(\n",
        "            heads, d_model, dropout=dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, iter, query, inputs, mask):\n",
        "        if (iter != 0):\n",
        "            input_norm = self.layer_norm(inputs)\n",
        "        else:\n",
        "            input_norm = inputs\n",
        "\n",
        "        mask = mask.unsqueeze(1)\n",
        "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
        "                                 mask=mask)\n",
        "        out = self.dropout(context) + inputs\n",
        "        return self.feed_forward(out)\n",
        "\n",
        "\n",
        "class TransformerInterEncoder(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, heads, dropout, num_inter_layers=0):\n",
        "        super(TransformerInterEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_inter_layers = num_inter_layers\n",
        "        self.pos_emb = PositionalEncoding(dropout, d_model)\n",
        "        self.transformer_inter = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(d_model, heads, d_ff, dropout)\n",
        "             for _ in range(num_inter_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.wo = nn.Linear(d_model, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, top_vecs, mask):\n",
        "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
        "\n",
        "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
        "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
        "        x = top_vecs * mask[:, :, None].float()\n",
        "        x = x + pos_emb\n",
        "\n",
        "        for i in range(self.num_inter_layers):\n",
        "            x = self.transformer_inter[i](i, x, x, 1 - mask)  # all_sents * max_tokens * dim\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        sent_scores = self.sigmoid(self.wo(x))\n",
        "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
        "\n",
        "        return sent_scores\n",
        "\n",
        "\n",
        "class RNNEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, bidirectional, num_layers, input_size,\n",
        "                 hidden_size, dropout=0.0):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        assert hidden_size % num_directions == 0\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.rnn = LayerNormLSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional)\n",
        "\n",
        "        self.wo = nn.Linear(num_directions * hidden_size, 1, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"See :func:`EncoderBase.forward()`\"\"\"\n",
        "        x = torch.transpose(x, 1, 0)\n",
        "        memory_bank, _ = self.rnn(x)\n",
        "        memory_bank = self.dropout(memory_bank) + x\n",
        "        memory_bank = torch.transpose(memory_bank, 1, 0)\n",
        "\n",
        "        sent_scores = self.sigmoid(self.wo(memory_bank))\n",
        "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
        "        return sent_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2_pH2IvrOzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From src/models/model_builder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertModel, BertConfig\n",
        "from torch.nn.init import xavier_uniform_\n",
        "\n",
        "#from models.encoder import TransformerInterEncoder, Classifier, RNNEncoder\n",
        "#from models.optimizers import Optimizer\n",
        "\n",
        "\n",
        "def build_optim(args, model, checkpoint):\n",
        "    \"\"\" Build optimizer \"\"\"\n",
        "    saved_optimizer_state_dict = None\n",
        "\n",
        "    if args.train_from != '':\n",
        "        optim = checkpoint['optim']\n",
        "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
        "    else:\n",
        "        optim = Optimizer(\n",
        "            args.optim, args.lr, args.max_grad_norm,\n",
        "            beta1=args.beta1, beta2=args.beta2,\n",
        "            decay_method=args.decay_method,\n",
        "            warmup_steps=args.warmup_steps)\n",
        "\n",
        "    optim.set_parameters(list(model.named_parameters()))\n",
        "\n",
        "    if args.train_from != '':\n",
        "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
        "        if args.visible_gpus != '-1':\n",
        "            for state in optim.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.cuda()\n",
        "\n",
        "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
        "            raise RuntimeError(\n",
        "                \"Error: loaded Adam optimizer from existing model\" +\n",
        "                \" but optimizer state is empty\")\n",
        "\n",
        "    return optim\n",
        "\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, temp_dir, load_pretrained_bert, bert_config):\n",
        "        super(Bert, self).__init__()\n",
        "        if(load_pretrained_bert):\n",
        "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
        "        else:\n",
        "            self.model = BertModel(bert_config)\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        encoded_layers, _ = self.model(x, segs, attention_mask =mask)\n",
        "        top_vec = encoded_layers[-1]\n",
        "        return top_vec\n",
        "\n",
        "\n",
        "\n",
        "class Summarizer(nn.Module):\n",
        "    def __init__(self, args, device, load_pretrained_bert = False, bert_config = None):\n",
        "        super(Summarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.bert = Bert(args.temp_dir, load_pretrained_bert, bert_config)\n",
        "        if (args.encoder == 'classifier'):\n",
        "            self.encoder = Classifier(self.bert.model.config.hidden_size)\n",
        "        elif(args.encoder=='transformer'):\n",
        "            self.encoder = TransformerInterEncoder(self.bert.model.config.hidden_size, args.ff_size, args.heads,\n",
        "                                                   args.dropout, args.inter_layers)\n",
        "        elif(args.encoder=='rnn'):\n",
        "            self.encoder = RNNEncoder(bidirectional=True, num_layers=1,\n",
        "                                      input_size=self.bert.model.config.hidden_size, hidden_size=args.rnn_size,\n",
        "                                      dropout=args.dropout)\n",
        "        elif (args.encoder == 'baseline'):\n",
        "            bert_config = BertConfig(self.bert.model.config.vocab_size, hidden_size=args.hidden_size,\n",
        "                                     num_hidden_layers=6, num_attention_heads=8, intermediate_size=args.ff_size)\n",
        "            self.bert.model = BertModel(bert_config)\n",
        "            self.encoder = Classifier(self.bert.model.config.hidden_size)\n",
        "\n",
        "        if args.param_init != 0.0:\n",
        "            for p in self.encoder.parameters():\n",
        "                p.data.uniform_(-args.param_init, args.param_init)\n",
        "        if args.param_init_glorot:\n",
        "            for p in self.encoder.parameters():\n",
        "                if p.dim() > 1:\n",
        "                    xavier_uniform_(p)\n",
        "\n",
        "        self.to(device)\n",
        "    def load_cp(self, pt):\n",
        "        self.load_state_dict(pt['model'], strict=True)\n",
        "\n",
        "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
        "\n",
        "        top_vec = self.bert(x, segs, mask)\n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores = self.encoder(sents_vec, mask_cls).squeeze(-1)\n",
        "        return sent_scores, mask_cls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtU_pMILbe0N",
        "colab_type": "text"
      },
      "source": [
        "### Inference on sample dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMKQ_-Owfh3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "from argparse import Namespace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfF6gekAq736",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting these as default arguments\n",
        "# These attributes can be edited as needed\n",
        "args = Namespace(\n",
        "    encoder=\"transformer\",\n",
        "    mode=\"test\",\n",
        "    bert_data_path=\"/home/sample_data/cnndm\",\n",
        "    model_path=MODELS_DIR,\n",
        "    result_path=\"/home/bert_results/cnndm\",\n",
        "    temp_dir=\"/home/temp/\",\n",
        "    bert_config_path='/home/BertSum/bert_config_uncased_base.json',\n",
        "    batch_size=1000,\n",
        "    model_fp=MODEL_FP,\n",
        "\n",
        "    use_interval=True,\n",
        "    hidden_size=128,\n",
        "    ff_size=2048, # Size used during training\n",
        "    heads=4,\n",
        "    inter_layers=2,\n",
        "    rnn_size=512,\n",
        "\n",
        "    param_init=0.0,\n",
        "    param_init_glorot=True,\n",
        "    dropout=0.1,\n",
        "    optim='adam',\n",
        "    lr=1,\n",
        "    beta1= 0.9,\n",
        "    beta2=0.999,\n",
        "    decay_method='',\n",
        "    warmup_steps=8000,\n",
        "    max_grad_norm=0,\n",
        "\n",
        "    save_checkpoint_steps=5,\n",
        "    accum_count=1,\n",
        "    world_size=1,\n",
        "    report_every=1,\n",
        "    train_steps=1000,\n",
        "    recall_eval=False,\n",
        "\n",
        "\n",
        "    visible_gpus='-1',\n",
        "    gpu_ranks='0',\n",
        "    log_file='/home/logs/cnndm.log',\n",
        "    dataset='',\n",
        "    seed=666,\n",
        "\n",
        "    test_all=False,\n",
        "    test_from=MODEL,\n",
        "    train_from='',\n",
        "    report_rouge=True,\n",
        "    block_trigram=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_svQJOZ-sse",
        "colab_type": "code",
        "outputId": "09502afa-e6ef-473f-bb0c-649d4141ced2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Have to be in this directory for the pickle file loading to import BERTSUM in the same environment that it was trained in\n",
        "cd /home/BertSum/src"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/BertSum/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB8ZXsTnbln2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pretrained BERTSUM model\n",
        "trained_model = torch.load(MODEL_FP, map_location=lambda storage, loc: storage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLFelfCZj8Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read BERT config file\n",
        "# This contains information about the BERT model (e.g. hidden size for the transformer layers, number of transformer layers, number of self attention heads)\n",
        "config = BertConfig.from_json_file(args.bert_config_path)\n",
        "# Specify device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Instantiate Summarizer and load pretrained model\n",
        "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "model.load_cp(trained_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RJJKNmJp7KX",
        "colab_type": "code",
        "outputId": "ee280c50-a189-41dc-8452-fcb0f8117d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5899
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Summarizer(\n",
              "  (bert): Bert(\n",
              "    (model): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder): TransformerInterEncoder(\n",
              "    (pos_emb): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (transformer_inter): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (softmax): Softmax()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1)\n",
              "          (dropout_2): Dropout(p=0.1)\n",
              "        )\n",
              "        (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (softmax): Softmax()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1)\n",
              "          (dropout_2): Dropout(p=0.1)\n",
              "        )\n",
              "        (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1)\n",
              "    (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6fgE0U_Cl95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_function\n",
        "def test_api(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
        "    \"\"\" Validate model.\n",
        "        valid_iter: validate data iterator\n",
        "    Returns:\n",
        "        :obj:`nmt.Statistics`: validation loss statistics\n",
        "    \"\"\"\n",
        "    # Set model in validating mode.\n",
        "    def _get_ngrams(n, text):\n",
        "        ngram_set = set()\n",
        "        text_length = len(text)\n",
        "        max_index_ngram_start = text_length - n\n",
        "        for i in range(max_index_ngram_start + 1):\n",
        "            ngram_set.add(tuple(text[i:i + n]))\n",
        "        return ngram_set\n",
        "\n",
        "    def _block_tri(c, p):\n",
        "        tri_c = _get_ngrams(3, c.split())\n",
        "        for s in p:\n",
        "            tri_s = _get_ngrams(3, s.split())\n",
        "            if len(tri_c.intersection(tri_s))>0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    if (not cal_lead and not cal_oracle):\n",
        "        # Evaluate without performing backpropagation and dropout\n",
        "        self.model.eval()\n",
        "    _source_article = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "\n",
        "            _source_article += src\n",
        "            pred = []\n",
        "\n",
        "            if (cal_lead):\n",
        "                selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "            elif (cal_oracle):\n",
        "                selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                range(batch.batch_size)]\n",
        "            else:\n",
        "                sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "                sent_scores = sent_scores + mask.float()\n",
        "                sent_scores = sent_scores.cpu().data.numpy()\n",
        "                selected_ids = np.argsort(-sent_scores, 1)\n",
        "            # selected_ids = np.sort(selected_ids,1)\n",
        "            for i, idx in enumerate(selected_ids):\n",
        "                _pred = []\n",
        "                if(len(batch.src_str[i])==0):\n",
        "                    continue\n",
        "                for j in selected_ids[i][:len(batch.src_str[i])]:\n",
        "                    if(j>=len( batch.src_str[i])):\n",
        "                        continue\n",
        "                    candidate = batch.src_str[i][j].strip()\n",
        "                    if(self.args.block_trigram):\n",
        "                        if(not _block_tri(candidate,_pred)):\n",
        "                            _pred.append(candidate)\n",
        "                    else:\n",
        "                        _pred.append(candidate)\n",
        "\n",
        "                    if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == 3):\n",
        "                        break\n",
        "\n",
        "                _pred = '<q>'.join(_pred)\n",
        "                if(self.args.recall_eval):\n",
        "                    _pred = ' '.join(_pred.split()[:len(batch.tgt_str[i].split())])\n",
        "\n",
        "                pred.append(_pred)\n",
        "\n",
        "    results = {'source_article': _source_article, 'predicted_summary': _pred}\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3HuNIL5mKjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from models import data_loader, model_builder\n",
        "from models.data_loader import load_dataset\n",
        "from models.trainer import build_trainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXszR6qWpb4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp = args.test_from\n",
        "step = int(cp.split('.')[-2].split('_')[-1])\n",
        "device_id = 0 if device == \"cuda\" else -1\n",
        "# Read data\n",
        "test_iter = data_loader.Dataloader(args, load_dataset(args, 'test', shuffle=False),\n",
        "                              args.batch_size, device,\n",
        "                              shuffle=False, is_test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wozmRcnHyx1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample is an iterator that contains a batch of data inputs to the \"trainer\"\n",
        "# For our context, this will be for the purpose of doing a forward pass with unseen articles\n",
        "sample = next(iter(test_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS-gANN3y22T",
        "colab_type": "code",
        "outputId": "0c2c6b55-788a-4190-fcea-60a5027ce88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# You'll notice the initial tokenization is performed on the article while separating it into a list of sentences\n",
        "sample.src_str[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .',\n",
              " \"the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders .\",\n",
              " 'on his instagram , he said : ` moving forward !',\n",
              " 'david alaba is looking to get back to fitness for bayern munich now his cast has been removed',\n",
              " 'the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .',\n",
              " 'bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao .',\n",
              " \"having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash .\",\n",
              " 'the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn4WgWGp2YEx",
        "colab_type": "code",
        "outputId": "cfb8b713-01ff-4090-fc22-d079ed8236c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Notice that the target text is abstractive, meaning it's not exactly extracted from the source document\n",
        "sample.tgt_str"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['david alaba posted video on his instagram of the cast being removed<q>the austrian is currently out after suffering knee ligament damage<q>bayern munich face porto in the champions league on wednesday',\n",
              " \"sabrina osterkamp was last seen leaving home in naracoorte at 12pm on sunday<q>the german tourist was driving a friend 's car about 110km south to mount gambier in south australia 's southeast<q>the car was found abandoned on side of road in centre of mount gambier<q>the 25-year-old contacted authorities to say she was safe and well almost 24 hours later\",\n",
              " 'cameron thomas philp vandalised and spat on the vehicle in 2013<q>the forensic officer swabbed the spit and found philp from his dna<q>he pleaded guilty to wilful damage of police property and was fined $ 300<q>mr philp claimed it was out of character but he has similar convictions']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koc1blsOyrN9",
        "colab_type": "code",
        "outputId": "b4ae022c-8187-4de2-ab08-0fd9b03006e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "trainer = build_trainer(args, device_id, model, None)\n",
        "trainer.test(test_iter, step)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu_rank 0\n",
            "2001\n",
            "2001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-05-20 10:43:22,618 [MainThread  ] [INFO ]  Writing summaries.\n",
            "2019-05-20 10:43:22,620 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /home/temp/tmpzh25kbrb/system and model files to /home/temp/tmpzh25kbrb/model.\n",
            "2019-05-20 10:43:22,621 [MainThread  ] [INFO ]  Processing files in /home/temp/rouge-tmp-2019-05-20-10-43-22/candidate/.\n",
            "2019-05-20 10:43:22,905 [MainThread  ] [INFO ]  Saved processed files to /home/temp/tmpzh25kbrb/system.\n",
            "2019-05-20 10:43:22,906 [MainThread  ] [INFO ]  Processing files in /home/temp/rouge-tmp-2019-05-20-10-43-22/reference/.\n",
            "2019-05-20 10:43:23,183 [MainThread  ] [INFO ]  Saved processed files to /home/temp/tmpzh25kbrb/model.\n",
            "2019-05-20 10:43:23,204 [MainThread  ] [INFO ]  Written ROUGE configuration to /home/temp/tmpc8ykd3ra/rouge_conf.xml\n",
            "2019-05-20 10:43:23,205 [MainThread  ] [INFO ]  Running ROUGE with command /home/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /home/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /home/temp/tmpc8ykd3ra/rouge_conf.xml\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.52556 (95%-conf.int. 0.51914 - 0.53207)\n",
            "1 ROUGE-1 Average_P: 0.38347 (95%-conf.int. 0.37746 - 0.38945)\n",
            "1 ROUGE-1 Average_F: 0.42804 (95%-conf.int. 0.42267 - 0.43308)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.24556 (95%-conf.int. 0.23899 - 0.25193)\n",
            "1 ROUGE-2 Average_P: 0.17853 (95%-conf.int. 0.17296 - 0.18372)\n",
            "1 ROUGE-2 Average_F: 0.19937 (95%-conf.int. 0.19401 - 0.20449)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.48173 (95%-conf.int. 0.47519 - 0.48820)\n",
            "1 ROUGE-L Average_P: 0.35203 (95%-conf.int. 0.34625 - 0.35768)\n",
            "1 ROUGE-L Average_F: 0.39274 (95%-conf.int. 0.38751 - 0.39787)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<models.stats.Statistics at 0x7fddf4344b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcwsJZSQG-sm",
        "colab_type": "code",
        "outputId": "6ca2c987-b1ac-4d87-aece-6f84ee49835b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/bert_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnndm_step43000.candidate  cnndm_step43000.gold\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z9JtvE4G8Wl",
        "colab_type": "code",
        "outputId": "2dc3fedd-8de9-4e42-d810-b4b3c2c557dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Predicted summary\n",
        "!head -1 /home/bert_results/cnndm_step43000.candidate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyHZ9hfKFTBK",
        "colab_type": "code",
        "outputId": "5e57eff1-a649-4d24-e113-df8949bb5d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Ground truth summary\n",
        "!head -1 /home/bert_results/cnndm_step43000.gold"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba posted video on his instagram of the cast being removed<q>the austrian is currently out after suffering knee ligament damage<q>bayern munich face porto in the champions league on wednesday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ptvmz-r19LZ",
        "colab_type": "code",
        "outputId": "2b4ade05-ed7a-4a3b-daa1-bdf509a8d2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# First 10 predicted summaries\n",
        "!head -10 /home/bert_results/cnndm_step43000.candidate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\n",
            "sabrina osterkamp was last seen leaving her home in naracoorte at about midday on sunday when she was driving a friend 's car about 110km south to mount gambier in south australia 's southeast .<q>a german tourist , who had been missing for almost 24 hours after her car was discovered abandoned on the side of the road , has been found after surviving the night in a national park .<q>a police spokesman said it was not know why she was in the national park without the car .\n",
            "cameron thomas philp was passing a crime scene at highgate hill in brisbane on may 3 , 2013 when he graffitied and spat on the side of the vehicle .<q>mr philp fronted brisbane magistrates court on friday and pleaded guilty to wilful damage of police property .<q>he was fined $ 300 but avoided a conviction , reports courier mail .\n",
            "the national union of teachers is expected to call for a vote on industrial action this weekend over the prospect of looming funding cuts .<q>tens of thousands of teachers could strike in the autumn , potentially causing temporary school closures .<q>the ballot of its 300,000 members would take place after the election , with strikes possible from september .\n",
            "david luiz could be a doubt for paris saint-germain 's champions league clash with barcelona after picking up a hamstring injury on sunday .<q>the brazilian defender went off after 35 minutes of psg 's 3-2 win over marseille after suffering an injjury to his left leg .<q>reports suggest the 27-year-old will have scans on monday to assess the extent of the injury .\n",
            "a secret meeting between intelligence figures in moscow and washington reportedly revealed putin will consider any attempt to return the crimean peninsula to ukraine as declaration of war and will take any necessary step - including using nuclear weapons - to retain control of the region .<q>notes from the meeting are also said to have revealed that putin is planning imminent ` destabilising actions ' in pro-western baltic states in a direct challenge to nato 's promise to defend the countries from soviet-style russian expansionism .<q>in crimea , russia is understood to have said that any attempt remove the region from russian control would be met ` forcefully including through the use of nuclear force ' .\n",
            "tipper lewis , 43 , has been reaping the benefits of superfoods for twenty years and credits the likes of chia seeds , bee pollen and matcha green tea with her boundless energy and good health<q>health bloggers and celebrities alike sing superfoods ' praises and scientists publish countless studies into their health benefits .<q>indeed , acai berries have been linked to weight loss and cancer prevention , flax seeds are thought to help lower blood pressure and reduce the risk of heart disease , while coconut oil has been hailed as a weight loss aid .\n",
            "harry kane and diego costa are locked on 19 goals in the race to finish as this season 's top scorer but rank fourth and fifth respectively , behind papiss cisse and olivier giroud .<q>glenn murray has scored a goal on average every 91 minutes this season - giving him a better minutes-per-goal ratio than anyone else in the top flight .<q>harry kane has scored 19 goals for tottenham in what has been a breakthrough season for him\n",
            "the 25-year-old ghana international is out of contract in the summer and free to talk to foreign clubs .<q>swansea , meanwhile , have expressed an interest in schalke full-back christian fuchs .<q>swansea have made a contract offer to marseille striker andre ayew .\n",
            "dumbarton boss ian murray believes that rangers will struggle to maintain their pace through the run-in<q>ian murray reckons a supposedly easier run-in can swing second place hibs ' way ahead of rangers in the last month of the championship season .<q>rangers - currently level on points but having played a game less - still have two matches to play against title winners hearts , including sunday 's match at ibrox , while the leith side take on their city rivals just once more .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4qc0yOY1_k5",
        "colab_type": "code",
        "outputId": "3e772a35-7dc3-4d35-eb16-5f43c60385f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# First 10 ground truth summaries\n",
        "!head -10 /home/bert_results/cnndm_step43000.gold"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba posted video on his instagram of the cast being removed<q>the austrian is currently out after suffering knee ligament damage<q>bayern munich face porto in the champions league on wednesday\n",
            "sabrina osterkamp was last seen leaving home in naracoorte at 12pm on sunday<q>the german tourist was driving a friend 's car about 110km south to mount gambier in south australia 's southeast<q>the car was found abandoned on side of road in centre of mount gambier<q>the 25-year-old contacted authorities to say she was safe and well almost 24 hours later\n",
            "cameron thomas philp vandalised and spat on the vehicle in 2013<q>the forensic officer swabbed the spit and found philp from his dna<q>he pleaded guilty to wilful damage of police property and was fined $ 300<q>mr philp claimed it was out of character but he has similar convictions\n",
            "national union of teachers expected to call for a vote on industrial action<q>ballot of its 300,000 members would take place after general election<q>fears funding cuts will lead to redundancies in schools\n",
            "david luiz pulled up in the 35th minute of ligue 1 's le classique<q>luiz was immediately replaced , and club confirmed a pulled hamstring<q>psg defender will have scans on monday , but could be out for weeks<q>barcelona meet psg on april 15 in the first leg of champions league tie\n",
            "russian intelligence chiefs took part in secret meeting with u.s. officials<q>outlined three potential flashpoints that could lead to all-out nuclear war<q>said attempts to return crimea to ukraine will be dealt with as an invasion<q>also demanded nato breaks up so called ` rapid response force ' in the baltic and stops arming those fighting pro-russian separatists in ukraine\n",
            "tipper lewis , 43 , has been consuming superfoods for over 20 years<q>has glowing skin , high energy levels and no doctor<q>would love to see children in schools eating superfoods\n",
            "glenn murray has scored four goals in 364 minutes this season<q>crystal palace striker has best minutes-per-goal ratio in premier league<q>olivier giroud third on the list , harry kane fourth , diego costa fifth\n",
            "andre ayew is free to talk to foreign clubs with his marseille contract expiring at the end of the season<q>ghana international has received offers from swansea , newcastle and everton while wolfsburg and borussia dortmund are interested<q>the swans are also chasing schalke defender christian fuchs\n",
            "dumbarton boss ian murray believes rangers ' tough run-in could mean they miss out on second place in the scottish championship to hibernian<q>rangers face difficult fixtures against hearts and queen of the south<q>murray played for both rangers and hibs during his career\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcJvJmb_dUr6",
        "colab_type": "text"
      },
      "source": [
        "# BERTSUM - Low level implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5uR6mUEdY4-",
        "colab_type": "text"
      },
      "source": [
        "## Prepare sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncuuB0L5dbu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXRJ7SVkdb05",
        "colab_type": "text"
      },
      "source": [
        "## Text pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUHWHKJdeY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr1UV0mUdekO",
        "colab_type": "text"
      },
      "source": [
        "## BERTSUM inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1zeMuPZdhyE",
        "colab_type": "text"
      },
      "source": [
        "### Load model from GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K16GyEW5dm5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9c_lvpMdnLS",
        "colab_type": "text"
      },
      "source": [
        "### Inference on sample dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YorbV8Kd6WkB",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "from argparse import Namespace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bn_zMuP56WkM",
        "colab": {}
      },
      "source": [
        "# Setting these as default arguments\n",
        "# These attributes can be edited as needed\n",
        "args = Namespace(\n",
        "    encoder=\"transformer\",\n",
        "    mode=\"test\",\n",
        "    bert_data_path=\"/home/sample_data/cnndm\",\n",
        "    model_path=MODELS_DIR,\n",
        "    result_path=\"/home/bert_results/cnndm\",\n",
        "    temp_dir=\"/home/temp/\",\n",
        "    bert_config_path='/home/BertSum/bert_config_uncased_base.json',\n",
        "    batch_size=1000,\n",
        "\n",
        "    use_interval=True,\n",
        "    hidden_size=128,\n",
        "    ff_size=2048, # Size used during training\n",
        "    heads=4,\n",
        "    inter_layers=2,\n",
        "    rnn_size=512,\n",
        "\n",
        "    param_init=0.0,\n",
        "    param_init_glorot=True,\n",
        "    dropout=0.1,\n",
        "    optim='adam',\n",
        "    lr=1,\n",
        "    beta1= 0.9,\n",
        "    beta2=0.999,\n",
        "    decay_method='',\n",
        "    warmup_steps=8000,\n",
        "    max_grad_norm=0,\n",
        "\n",
        "    save_checkpoint_steps=5,\n",
        "    accum_count=1,\n",
        "    world_size=1,\n",
        "    report_every=1,\n",
        "    train_steps=1000,\n",
        "    recall_eval=False,\n",
        "\n",
        "\n",
        "    visible_gpus='-1',\n",
        "    gpu_ranks='0',\n",
        "    log_file='/home/logs/cnndm.log',\n",
        "    dataset='',\n",
        "    seed=666,\n",
        "\n",
        "    test_all=False,\n",
        "    test_from=MODEL,\n",
        "    train_from='',\n",
        "    report_rouge=True,\n",
        "    block_trigram=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a1e1402c-9f2f-45ff-aae2-71a3f1ea87b3",
        "id": "NuGc51MY6WkO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Have to be in this directory for the pickle file loading to import BERTSUM in the same environment that it was trained in\n",
        "cd /home/BertSum/src"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/BertSum/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yuHAB6pJ6WkS",
        "colab": {}
      },
      "source": [
        "# Load pretrained BERTSUM model\n",
        "trained_model = torch.load(MODEL_FP, map_location=lambda storage, loc: storage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Sk9ACnG6WkT",
        "colab": {}
      },
      "source": [
        "# Read BERT config file\n",
        "# This contains information about the BERT model (e.g. hidden size for the transformer layers, number of transformer layers, number of self attention heads)\n",
        "config = BertConfig.from_json_file(args.bert_config_path)\n",
        "# Specify device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Instantiate Summarizer and load pretrained model\n",
        "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "model.load_cp(trained_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f4abb4b6-2b5a-49c4-9392-637a4c78490d",
        "id": "8kEYL9fZ6WkW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5899
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Summarizer(\n",
              "  (bert): Bert(\n",
              "    (model): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): BertLayerNorm()\n",
              "                (dropout): Dropout(p=0.1)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder): TransformerInterEncoder(\n",
              "    (pos_emb): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (transformer_inter): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (softmax): Softmax()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1)\n",
              "          (dropout_2): Dropout(p=0.1)\n",
              "        )\n",
              "        (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (softmax): Softmax()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
              "          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "          (dropout_1): Dropout(p=0.1)\n",
              "          (dropout_2): Dropout(p=0.1)\n",
              "        )\n",
              "        (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1)\n",
              "    (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)\n",
              "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SydgwbF26WkY",
        "colab": {}
      },
      "source": [
        "from models import data_loader, model_builder\n",
        "from models.data_loader import load_dataset\n",
        "from models.trainer import build_trainer, Trainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPw4jnO6Obp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59klMZCztvaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_api(self, test_iter, step, top_n_sentences=3, cal_lead=False, cal_oracle=False):\n",
        "    \"\"\" Validate model.\n",
        "        valid_iter: validate data iterator\n",
        "    Returns:\n",
        "        :obj:`nmt.Statistics`: validation loss statistics\n",
        "    \"\"\"\n",
        "    # Set model in validating mode.\n",
        "    def _get_ngrams(n, text):\n",
        "        ngram_set = set()\n",
        "        text_length = len(text)\n",
        "        max_index_ngram_start = text_length - n\n",
        "        for i in range(max_index_ngram_start + 1):\n",
        "            ngram_set.add(tuple(text[i:i + n]))\n",
        "        return ngram_set\n",
        "\n",
        "    def _block_tri(c, p):\n",
        "        tri_c = _get_ngrams(3, c.split())\n",
        "        for s in p:\n",
        "            tri_s = _get_ngrams(3, s.split())\n",
        "            if len(tri_c.intersection(tri_s))>0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    if (not cal_lead and not cal_oracle):\n",
        "        # Evaluate without performing backpropagation and dropout\n",
        "        self.model.eval()\n",
        "    source_article = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "            src_str = batch.src_str\n",
        "\n",
        "            source_article += [' '.join(article) for article in src_str]\n",
        "\n",
        "            if (cal_lead):\n",
        "                selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "            elif (cal_oracle):\n",
        "                selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                range(batch.batch_size)]\n",
        "            else:\n",
        "                sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "                sent_scores = sent_scores + mask.float()\n",
        "                sent_scores = sent_scores.cpu().data.numpy()\n",
        "                # Sort sentence ids in descending order based on sentence scores (representing summary importance)\n",
        "                selected_ids = np.argsort(-sent_scores, 1)\n",
        "            # selected_ids = np.sort(selected_ids,1)\n",
        "            for i, idx in enumerate(selected_ids):\n",
        "                _pred = []\n",
        "                if(len(batch.src_str[i])==0):\n",
        "                    continue\n",
        "                # Loop through each sentence\n",
        "                # len(batch.src_str[i]) refers to the number of sentences in the jth test example\n",
        "                for j in selected_ids[i][:len(batch.src_str[i])]:\n",
        "                    if(j>=len( batch.src_str[i])):\n",
        "                        continue\n",
        "                    candidate = batch.src_str[i][j].strip()\n",
        "                    if(self.args.block_trigram):\n",
        "                        if(not _block_tri(candidate,_pred)):\n",
        "                            _pred.append(candidate)\n",
        "                    else:\n",
        "                        _pred.append(candidate)\n",
        "                    \n",
        "                    # len(_pred) == 3 means that we limit sentences to top top_n_sentences\n",
        "                    if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == top_n_sentences):\n",
        "                        break\n",
        "\n",
        "                _pred = '<q>'.join(_pred)\n",
        "\n",
        "                pred.append(_pred)\n",
        "\n",
        "    results = {'source_article': source_article, 'predicted_summary': pred}\n",
        "    return results\n",
        "\n",
        "def example_api(self, example, step, top_n_sentences=3, device='cpu', cal_lead=False, cal_oracle=False):\n",
        "    \"\"\" \n",
        "    Runs inference on a single test example. Designed for API deployemnt.\n",
        "    \"\"\"\n",
        "    # Set model in validating mode.\n",
        "    def _get_ngrams(n, text):\n",
        "        ngram_set = set()\n",
        "        text_length = len(text)\n",
        "        max_index_ngram_start = text_length - n\n",
        "        for i in range(max_index_ngram_start + 1):\n",
        "            ngram_set.add(tuple(text[i:i + n]))\n",
        "        return ngram_set\n",
        "\n",
        "    def _block_tri(c, p):\n",
        "        tri_c = _get_ngrams(3, c.split())\n",
        "        for s in p:\n",
        "            tri_s = _get_ngrams(3, s.split())\n",
        "            if len(tri_c.intersection(tri_s))>0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    if (not cal_lead and not cal_oracle):\n",
        "        # Evaluate without performing backpropagation and dropout\n",
        "        self.model.eval()\n",
        "    # Set model device (cuda or cpu)\n",
        "    self.model.to(device=device) \n",
        "    source_article = []\n",
        "    pred = []\n",
        "    src = example.src\n",
        "    labels = example.labels\n",
        "    segs = example.segs\n",
        "    clss = example.clss\n",
        "    mask = example.mask\n",
        "    mask_cls = example.mask_cls\n",
        "    src_str = example.src_str\n",
        "\n",
        "    source_article += [' '.join(article) for article in src_str]\n",
        "\n",
        "    if (cal_lead):\n",
        "        selected_ids = [list(range(example.clss.size(1)))] * example.batch_size\n",
        "    elif (cal_oracle):\n",
        "        selected_ids = [[j for j in range(example.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                        range(example.batch_size)]\n",
        "    else:\n",
        "        sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "        sent_scores = sent_scores + mask.float()\n",
        "        sent_scores = sent_scores.cpu().data.numpy()\n",
        "        # Sort sentence ids in descending order based on sentence scores (representing summary importance)\n",
        "        selected_ids = np.argsort(-sent_scores, 1)\n",
        "    # selected_ids = np.sort(selected_ids,1)\n",
        "    for i, idx in enumerate(selected_ids):\n",
        "        _pred = []\n",
        "        if(len(example.src_str[i])==0):\n",
        "            continue\n",
        "        # Loop through each sentence\n",
        "        # len(example.src_str[i]) refers to the number of sentences in the jth test example\n",
        "        for j in selected_ids[i][:len(example.src_str[i])]:\n",
        "            if(j>=len( example.src_str[i])):\n",
        "                continue\n",
        "            candidate = example.src_str[i][j].strip()\n",
        "            if(self.args.block_trigram):\n",
        "                if(not _block_tri(candidate,_pred)):\n",
        "                    _pred.append(candidate)\n",
        "            else:\n",
        "                _pred.append(candidate)\n",
        "\n",
        "            # len(_pred) == 3 means that we limit sentences to top top_n_sentences\n",
        "            if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == top_n_sentences):\n",
        "                break\n",
        "\n",
        "        _pred = '<q>'.join(_pred)\n",
        "\n",
        "        pred.append(_pred)\n",
        "\n",
        "    results = {'source_article': source_article, 'predicted_summary': pred}\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qppmkPcwuJWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Trainer.test_api = test_api\n",
        "Trainer.example_api = example_api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uiTHUxO36WkZ",
        "colab": {}
      },
      "source": [
        "cp = args.test_from\n",
        "step = int(cp.split('.')[-2].split('_')[-1])\n",
        "device_id = 0 if device == \"cuda\" else -1\n",
        "# Read data\n",
        "test_iter = data_loader.Dataloader(args, load_dataset(args, 'test', shuffle=False),\n",
        "                              args.batch_size, device,\n",
        "                              shuffle=False, is_test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xi0OdgVA6Wka",
        "colab": {}
      },
      "source": [
        "# sample is an iterator that contains a batch of data inputs to the \"trainer\"\n",
        "# For our context, this will be for the purpose of doing a forward pass with unseen articles\n",
        "sample = next(iter(test_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHxzSL9Jlj0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d2ed65e3-98bb-4617-9937-aa8b19f943b5",
        "id": "GPlssoqR6Wkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# You'll notice the initial tokenization is performed on the article while separating it into a list of sentences\n",
        "sample.src_str[0]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .',\n",
              " \"the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders .\",\n",
              " 'on his instagram , he said : ` moving forward !',\n",
              " 'david alaba is looking to get back to fitness for bayern munich now his cast has been removed',\n",
              " 'the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .',\n",
              " 'bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao .',\n",
              " \"having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash .\",\n",
              " 'the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "86cd3fad-e31c-47a3-a400-a05a1c012519",
        "id": "eRrHF3TQ6Wke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Notice that the target text is abstractive, meaning it's not exactly extracted from the source document\n",
        "sample.tgt_str"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['david alaba posted video on his instagram of the cast being removed<q>the austrian is currently out after suffering knee ligament damage<q>bayern munich face porto in the champions league on wednesday',\n",
              " \"sabrina osterkamp was last seen leaving home in naracoorte at 12pm on sunday<q>the german tourist was driving a friend 's car about 110km south to mount gambier in south australia 's southeast<q>the car was found abandoned on side of road in centre of mount gambier<q>the 25-year-old contacted authorities to say she was safe and well almost 24 hours later\",\n",
              " 'cameron thomas philp vandalised and spat on the vehicle in 2013<q>the forensic officer swabbed the spit and found philp from his dna<q>he pleaded guilty to wilful damage of police property and was fined $ 300<q>mr philp claimed it was out of character but he has similar convictions']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWyy8gkYFqil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "d19c171f-35b2-47c9-f5ce-2685cd73c475"
      },
      "source": [
        "sample.src[0], sample.labels[0],sample.segs[0]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101,  2585, 21862,  3676,  2038,  2445, 21350,  7469,  2070,  2204,\n",
              "          2739,  2044, 14739,  1037,  2678,  2006,  2010, 16021, 23091,  1997,\n",
              "          1996,  3459,  2108,  3718,  2206,  2010,  3522,  6181, 25641,  4053,\n",
              "          1012,   102,   101,  1996,  6161,  2038,  1050,  1005,  1056,  2018,\n",
              "          2172,  6735,  1999,  3408,  1997,  6441,  2021,  2003,  4415, 15936,\n",
              "          2002,  2064,  2085,  3357,  1011,  2039,  2010,  7233,  1998,  2709,\n",
              "          2005,  1996, 14250,  4177,  1012,   102,   101,  2006,  2010, 16021,\n",
              "         23091,  1010,  2002,  2056,  1024,  1036,  3048,  2830,   999,   102,\n",
              "           101,  2585, 21862,  3676,  2003,  2559,  2000,  2131,  2067,  2000,\n",
              "         10516,  2005, 21350,  7469,  2085,  2010,  3459,  2038,  2042,  3718,\n",
              "           102,   101,  1996, 22979,  2732,  2018,  2010,  3459,  2772,  2011,\n",
              "          2035,  2010,  2136,  1011, 14711,  2040,  3369,  1999,  5978,  2006,\n",
              "          6928,  1010,  3805,  1997,  2037,  3966,  2223,  4284,  1011,  2345,\n",
              "         13249,  2007, 13809,  1012,   102,   101, 21350, 27042,  2098, 21146,\n",
              "         10023,  7559, 29151,  1999,  1996,  2197,  2461,  1998,  2097,  2022,\n",
              "          2559,  2005,  1037,  9377,  2765,  2043,  2027,  2240,  2039,  2006,\n",
              "          9317,  2012,  1996, 14143,  2079,  8011,  7113,  1012,   102,   101,\n",
              "          2383, 13605,  2067,  2013,  2037,  5213,  4154,  2000,  8945,  7946,\n",
              "          8464, 12256, 21043, 27266,  7693,  1010, 27233,  3457, 20282,  2050,\n",
              "          1005,  1055,  2217,  2031,  2180,  2093,  2006,  1996, 19817,  4140,\n",
              "          1998,  2024,  1999,  2204,  2433,  2183,  2046,  1996, 13249,  1012,\n",
              "           102,   101,  1996,  6161,  2248,  2003,  1037,  3145,  2447,  2005,\n",
              "         21350,  1998,  2010, 18601, 18724,  5927,  2032,  2377,  2012,  2440,\n",
              "          2067,  2030,  1999, 23071,   102,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0], device='cuda:0'),\n",
              " tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
              " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLcAjH-GSC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ccb884c8-8436-4e46-f425-2b5d2e3b6ece"
      },
      "source": [
        "sample.clss[0], sample.mask[0], sample.mask_cls[0]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  0,  32,  66,  80, 101, 135, 169, 211,   0,   0,   0],\n",
              "        device='cuda:0'),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0], device='cuda:0', dtype=torch.uint8),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0', dtype=torch.uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "651f8e09-c296-40f4-dadb-9541c5e1b26e",
        "id": "CDiVpZb-6Wkg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# build_trainer returns a Trainer class which now contains our custom test_api method\n",
        "trainer = build_trainer(args, device_id, model, None)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu_rank 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSDr8Tj-uW_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d5273803-0a6d-43fa-da8a-643d6e3a85db"
      },
      "source": [
        "%%time\n",
        "#%debug\n",
        "results = trainer.test_api(test_iter, step, top_n_sentences=4)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 58.9 s, sys: 21.7 s, total: 1min 20s\n",
            "Wall time: 1min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ibss1I4ePB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_df = pd.DataFrame(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSE6wQc1-h1T",
        "colab_type": "code",
        "outputId": "24902c63-5e3f-468e-f2ff-6bca2838142e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "results_df.head()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_article</th>\n",
              "      <th>predicted_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>david alaba has given bayern munich some good ...</td>\n",
              "      <td>david alaba is looking to get back to fitness ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a german tourist , who had been missing for al...</td>\n",
              "      <td>sabrina osterkamp was last seen leaving her ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a queensland man has been dubbed ` the dumbest...</td>\n",
              "      <td>cameron thomas philp was passing a crime scene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pupils could face disruption to classes as tea...</td>\n",
              "      <td>the national union of teachers is expected to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>david luiz could be a doubt for paris saint-ge...</td>\n",
              "      <td>david luiz could be a doubt for paris saint-ge...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      source_article                                  predicted_summary\n",
              "0  david alaba has given bayern munich some good ...  david alaba is looking to get back to fitness ...\n",
              "1  a german tourist , who had been missing for al...  sabrina osterkamp was last seen leaving her ho...\n",
              "2  a queensland man has been dubbed ` the dumbest...  cameron thomas philp was passing a crime scene...\n",
              "3  pupils could face disruption to classes as tea...  the national union of teachers is expected to ...\n",
              "4  david luiz could be a doubt for paris saint-ge...  david luiz could be a doubt for paris saint-ge..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my9ei-wY0zhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccfd602e-8c2f-4195-86bb-9c053c376f58"
      },
      "source": [
        "results_df.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2001, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkESPUd26qg0",
        "colab_type": "code",
        "outputId": "ca4a90cc-713c-44a3-bc0b-f0e3d1029d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results_df.iloc[0, 0]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage . the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders . on his instagram , he said : ` moving forward ! david alaba is looking to get back to fitness for bayern munich now his cast has been removed the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto . bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao . having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash . the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fylZnjk2zoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "33052316-7e75-4726-80d0-6f22b68295ba"
      },
      "source": [
        "results_df.iloc[0, 1]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield<q>the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6ca2c987-b1ac-4d87-aece-6f84ee49835b",
        "id": "oAD4ghRs6Wkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls /home/bert_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnndm_step43000.candidate  cnndm_step43000.gold\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2dc3fedd-8de9-4e42-d810-b4b3c2c557dc",
        "id": "cGqemuVP6Wkl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Predicted summary\n",
        "!head -1 /home/bert_results/cnndm_step43000.candidate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5e57eff1-a649-4d24-e113-df8949bb5d18",
        "id": "na_Ry1do6Wkn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Ground truth summary\n",
        "!head -1 /home/bert_results/cnndm_step43000.gold"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba posted video on his instagram of the cast being removed<q>the austrian is currently out after suffering knee ligament damage<q>bayern munich face porto in the champions league on wednesday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2b4ade05-ed7a-4a3b-daa1-bdf509a8d2ae",
        "id": "RprerSb16Wks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# First 10 predicted summaries\n",
        "!head -10 /home/bert_results/cnndm_step43000.candidate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\n",
            "sabrina osterkamp was last seen leaving her home in naracoorte at about midday on sunday when she was driving a friend 's car about 110km south to mount gambier in south australia 's southeast .<q>a german tourist , who had been missing for almost 24 hours after her car was discovered abandoned on the side of the road , has been found after surviving the night in a national park .<q>a police spokesman said it was not know why she was in the national park without the car .\n",
            "cameron thomas philp was passing a crime scene at highgate hill in brisbane on may 3 , 2013 when he graffitied and spat on the side of the vehicle .<q>mr philp fronted brisbane magistrates court on friday and pleaded guilty to wilful damage of police property .<q>he was fined $ 300 but avoided a conviction , reports courier mail .\n",
            "the national union of teachers is expected to call for a vote on industrial action this weekend over the prospect of looming funding cuts .<q>tens of thousands of teachers could strike in the autumn , potentially causing temporary school closures .<q>the ballot of its 300,000 members would take place after the election , with strikes possible from september .\n",
            "david luiz could be a doubt for paris saint-germain 's champions league clash with barcelona after picking up a hamstring injury on sunday .<q>the brazilian defender went off after 35 minutes of psg 's 3-2 win over marseille after suffering an injjury to his left leg .<q>reports suggest the 27-year-old will have scans on monday to assess the extent of the injury .\n",
            "a secret meeting between intelligence figures in moscow and washington reportedly revealed putin will consider any attempt to return the crimean peninsula to ukraine as declaration of war and will take any necessary step - including using nuclear weapons - to retain control of the region .<q>notes from the meeting are also said to have revealed that putin is planning imminent ` destabilising actions ' in pro-western baltic states in a direct challenge to nato 's promise to defend the countries from soviet-style russian expansionism .<q>in crimea , russia is understood to have said that any attempt remove the region from russian control would be met ` forcefully including through the use of nuclear force ' .\n",
            "tipper lewis , 43 , has been reaping the benefits of superfoods for twenty years and credits the likes of chia seeds , bee pollen and matcha green tea with her boundless energy and good health<q>health bloggers and celebrities alike sing superfoods ' praises and scientists publish countless studies into their health benefits .<q>indeed , acai berries have been linked to weight loss and cancer prevention , flax seeds are thought to help lower blood pressure and reduce the risk of heart disease , while coconut oil has been hailed as a weight loss aid .\n",
            "harry kane and diego costa are locked on 19 goals in the race to finish as this season 's top scorer but rank fourth and fifth respectively , behind papiss cisse and olivier giroud .<q>glenn murray has scored a goal on average every 91 minutes this season - giving him a better minutes-per-goal ratio than anyone else in the top flight .<q>harry kane has scored 19 goals for tottenham in what has been a breakthrough season for him\n",
            "the 25-year-old ghana international is out of contract in the summer and free to talk to foreign clubs .<q>swansea , meanwhile , have expressed an interest in schalke full-back christian fuchs .<q>swansea have made a contract offer to marseille striker andre ayew .\n",
            "dumbarton boss ian murray believes that rangers will struggle to maintain their pace through the run-in<q>ian murray reckons a supposedly easier run-in can swing second place hibs ' way ahead of rangers in the last month of the championship season .<q>rangers - currently level on points but having played a game less - still have two matches to play against title winners hearts , including sunday 's match at ibrox , while the leith side take on their city rivals just once more .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3e772a35-7dc3-4d35-eb16-5f43c60385f4",
        "id": "h5TKs8T-6Wkv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# First 10 ground truth summaries\n",
        "!head -10 /home/bert_results/cnndm_step43000.gold"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david alaba posted video on his instagram of the cast being removed<q>the austrian is currently out after suffering knee ligament damage<q>bayern munich face porto in the champions league on wednesday\n",
            "sabrina osterkamp was last seen leaving home in naracoorte at 12pm on sunday<q>the german tourist was driving a friend 's car about 110km south to mount gambier in south australia 's southeast<q>the car was found abandoned on side of road in centre of mount gambier<q>the 25-year-old contacted authorities to say she was safe and well almost 24 hours later\n",
            "cameron thomas philp vandalised and spat on the vehicle in 2013<q>the forensic officer swabbed the spit and found philp from his dna<q>he pleaded guilty to wilful damage of police property and was fined $ 300<q>mr philp claimed it was out of character but he has similar convictions\n",
            "national union of teachers expected to call for a vote on industrial action<q>ballot of its 300,000 members would take place after general election<q>fears funding cuts will lead to redundancies in schools\n",
            "david luiz pulled up in the 35th minute of ligue 1 's le classique<q>luiz was immediately replaced , and club confirmed a pulled hamstring<q>psg defender will have scans on monday , but could be out for weeks<q>barcelona meet psg on april 15 in the first leg of champions league tie\n",
            "russian intelligence chiefs took part in secret meeting with u.s. officials<q>outlined three potential flashpoints that could lead to all-out nuclear war<q>said attempts to return crimea to ukraine will be dealt with as an invasion<q>also demanded nato breaks up so called ` rapid response force ' in the baltic and stops arming those fighting pro-russian separatists in ukraine\n",
            "tipper lewis , 43 , has been consuming superfoods for over 20 years<q>has glowing skin , high energy levels and no doctor<q>would love to see children in schools eating superfoods\n",
            "glenn murray has scored four goals in 364 minutes this season<q>crystal palace striker has best minutes-per-goal ratio in premier league<q>olivier giroud third on the list , harry kane fourth , diego costa fifth\n",
            "andre ayew is free to talk to foreign clubs with his marseille contract expiring at the end of the season<q>ghana international has received offers from swansea , newcastle and everton while wolfsburg and borussia dortmund are interested<q>the swans are also chasing schalke defender christian fuchs\n",
            "dumbarton boss ian murray believes rangers ' tough run-in could mean they miss out on second place in the scottish championship to hibernian<q>rangers face difficult fixtures against hearts and queen of the south<q>murray played for both rangers and hibs during his career\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ut6RJldN64u",
        "colab_type": "text"
      },
      "source": [
        "### Example level inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCjCc8_eOFt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a30ba5de-8f50-4d50-dd73-66d26cbe20a2"
      },
      "source": [
        "from prepro import data_builder\n",
        "from models import data_loader\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8rRqM6HYQjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing arguments\n",
        "# Setting these as default arguments\n",
        "# These attributes can be edited as needed\n",
        "pp_args = Namespace(\n",
        "    mode=\"\",\n",
        "    oracle_mode='greedy',\n",
        "    shard_size=2000,\n",
        "    min_nsents=3,\n",
        "    max_nsents=100,\n",
        "    min_src_ntokens=5,\n",
        "    max_src_ntokens=200,\n",
        "    lower=True,\n",
        "    dataset='',\n",
        "    n_cpus=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqV2XL5yOroI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_str = \"david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage . the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders . on his instagram , he said : ` moving forward ! david alaba is looking to get back to fitness for bayern munich now his cast has been removed the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto . bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao . having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash . the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\"\n",
        "tgt_str = 'david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield<q>the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SviSlYLQaz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separate documents into list of sentences\n",
        "src = [sent.split() for sent in nltk.tokenize.sent_tokenize(src_str)]\n",
        "tgt = [sent.split() for sent in nltk.tokenize.sent_tokenize(tgt_str)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxrap65rRNHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3298
        },
        "outputId": "b1d01714-d245-4132-8153-9f46921a4266"
      },
      "source": [
        "src"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['david',\n",
              "  'alaba',\n",
              "  'has',\n",
              "  'given',\n",
              "  'bayern',\n",
              "  'munich',\n",
              "  'some',\n",
              "  'good',\n",
              "  'news',\n",
              "  'after',\n",
              "  'posting',\n",
              "  'a',\n",
              "  'video',\n",
              "  'on',\n",
              "  'his',\n",
              "  'instagram',\n",
              "  'of',\n",
              "  'the',\n",
              "  'cast',\n",
              "  'being',\n",
              "  'removed',\n",
              "  'following',\n",
              "  'his',\n",
              "  'recent',\n",
              "  'knee',\n",
              "  'ligament',\n",
              "  'damage',\n",
              "  '.'],\n",
              " ['the',\n",
              "  'austrian',\n",
              "  'has',\n",
              "  \"n't\",\n",
              "  'had',\n",
              "  'much',\n",
              "  'luck',\n",
              "  'in',\n",
              "  'terms',\n",
              "  'of',\n",
              "  'injuries',\n",
              "  'but',\n",
              "  'is',\n",
              "  'clearly',\n",
              "  'delighted',\n",
              "  'he',\n",
              "  'can',\n",
              "  'now',\n",
              "  'step-up',\n",
              "  'his',\n",
              "  'recovery',\n",
              "  'and',\n",
              "  'return',\n",
              "  'for',\n",
              "  'the',\n",
              "  'bundesliga',\n",
              "  'leaders',\n",
              "  '.'],\n",
              " ['on',\n",
              "  'his',\n",
              "  'instagram',\n",
              "  ',',\n",
              "  'he',\n",
              "  'said',\n",
              "  ':',\n",
              "  '`',\n",
              "  'moving',\n",
              "  'forward',\n",
              "  '!'],\n",
              " ['david',\n",
              "  'alaba',\n",
              "  'is',\n",
              "  'looking',\n",
              "  'to',\n",
              "  'get',\n",
              "  'back',\n",
              "  'to',\n",
              "  'fitness',\n",
              "  'for',\n",
              "  'bayern',\n",
              "  'munich',\n",
              "  'now',\n",
              "  'his',\n",
              "  'cast',\n",
              "  'has',\n",
              "  'been',\n",
              "  'removed',\n",
              "  'the',\n",
              "  'versatile',\n",
              "  'star',\n",
              "  'had',\n",
              "  'his',\n",
              "  'cast',\n",
              "  'signed',\n",
              "  'by',\n",
              "  'all',\n",
              "  'his',\n",
              "  'team-mates',\n",
              "  'who',\n",
              "  'arrived',\n",
              "  'in',\n",
              "  'portugal',\n",
              "  'on',\n",
              "  'monday',\n",
              "  ',',\n",
              "  'ahead',\n",
              "  'of',\n",
              "  'their',\n",
              "  'champions',\n",
              "  'league',\n",
              "  'quarter-final',\n",
              "  'clash',\n",
              "  'with',\n",
              "  'porto',\n",
              "  '.'],\n",
              " ['bayern',\n",
              "  'thrashed',\n",
              "  'shakhtar',\n",
              "  'donetsk',\n",
              "  'in',\n",
              "  'the',\n",
              "  'last',\n",
              "  'round',\n",
              "  'and',\n",
              "  'will',\n",
              "  'be',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'a',\n",
              "  'repeat',\n",
              "  'result',\n",
              "  'when',\n",
              "  'they',\n",
              "  'line',\n",
              "  'up',\n",
              "  'on',\n",
              "  'wednesday',\n",
              "  'at',\n",
              "  'the',\n",
              "  'estadio',\n",
              "  'do',\n",
              "  'dragao',\n",
              "  '.'],\n",
              " ['having',\n",
              "  'bounced',\n",
              "  'back',\n",
              "  'from',\n",
              "  'their',\n",
              "  'shock',\n",
              "  'defeat',\n",
              "  'to',\n",
              "  'borussia',\n",
              "  'monchengladbach',\n",
              "  ',',\n",
              "  'pep',\n",
              "  'guardiola',\n",
              "  \"'s\",\n",
              "  'side',\n",
              "  'have',\n",
              "  'won',\n",
              "  'three',\n",
              "  'on',\n",
              "  'the',\n",
              "  'trot',\n",
              "  'and',\n",
              "  'are',\n",
              "  'in',\n",
              "  'good',\n",
              "  'form',\n",
              "  'going',\n",
              "  'into',\n",
              "  'the',\n",
              "  'clash',\n",
              "  '.'],\n",
              " ['the',\n",
              "  'austrian',\n",
              "  'international',\n",
              "  'is',\n",
              "  'a',\n",
              "  'key',\n",
              "  'player',\n",
              "  'for',\n",
              "  'bayern',\n",
              "  'and',\n",
              "  'his',\n",
              "  'versatility',\n",
              "  'sees',\n",
              "  'him',\n",
              "  'play',\n",
              "  'at',\n",
              "  'full',\n",
              "  'back',\n",
              "  'or',\n",
              "  'in',\n",
              "  'midfield']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yjh_m9rOYw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = data_builder.BertData(pp_args)\n",
        "#src_str, tgt_str, oracle_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsNjFI1WSFuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oracle_ids = data_builder.greedy_selection(src, tgt, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGnIhitNSQZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6608bc8-2534-4aa4-fa5b-a4c847120a19"
      },
      "source": [
        "oracle_ids"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 3, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f7PZ8UHObnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_data = bert.preprocess(src, tgt, oracle_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B0G95SyN3Oa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8129
        },
        "outputId": "c32f075e-52c4-4f3c-cc86-5869973b8b6b"
      },
      "source": [
        "b_data"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([101,\n",
              "  2585,\n",
              "  21862,\n",
              "  3676,\n",
              "  2038,\n",
              "  2445,\n",
              "  21350,\n",
              "  7469,\n",
              "  2070,\n",
              "  2204,\n",
              "  2739,\n",
              "  2044,\n",
              "  14739,\n",
              "  1037,\n",
              "  2678,\n",
              "  2006,\n",
              "  2010,\n",
              "  16021,\n",
              "  23091,\n",
              "  1997,\n",
              "  1996,\n",
              "  3459,\n",
              "  2108,\n",
              "  3718,\n",
              "  2206,\n",
              "  2010,\n",
              "  3522,\n",
              "  6181,\n",
              "  25641,\n",
              "  4053,\n",
              "  1012,\n",
              "  102,\n",
              "  101,\n",
              "  1996,\n",
              "  6161,\n",
              "  2038,\n",
              "  1050,\n",
              "  1005,\n",
              "  1056,\n",
              "  2018,\n",
              "  2172,\n",
              "  6735,\n",
              "  1999,\n",
              "  3408,\n",
              "  1997,\n",
              "  6441,\n",
              "  2021,\n",
              "  2003,\n",
              "  4415,\n",
              "  15936,\n",
              "  2002,\n",
              "  2064,\n",
              "  2085,\n",
              "  3357,\n",
              "  1011,\n",
              "  2039,\n",
              "  2010,\n",
              "  7233,\n",
              "  1998,\n",
              "  2709,\n",
              "  2005,\n",
              "  1996,\n",
              "  14250,\n",
              "  4177,\n",
              "  1012,\n",
              "  102,\n",
              "  101,\n",
              "  2006,\n",
              "  2010,\n",
              "  16021,\n",
              "  23091,\n",
              "  1010,\n",
              "  2002,\n",
              "  2056,\n",
              "  1024,\n",
              "  1036,\n",
              "  3048,\n",
              "  2830,\n",
              "  999,\n",
              "  102,\n",
              "  101,\n",
              "  2585,\n",
              "  21862,\n",
              "  3676,\n",
              "  2003,\n",
              "  2559,\n",
              "  2000,\n",
              "  2131,\n",
              "  2067,\n",
              "  2000,\n",
              "  10516,\n",
              "  2005,\n",
              "  21350,\n",
              "  7469,\n",
              "  2085,\n",
              "  2010,\n",
              "  3459,\n",
              "  2038,\n",
              "  2042,\n",
              "  3718,\n",
              "  1996,\n",
              "  22979,\n",
              "  2732,\n",
              "  2018,\n",
              "  2010,\n",
              "  3459,\n",
              "  2772,\n",
              "  2011,\n",
              "  2035,\n",
              "  2010,\n",
              "  2136,\n",
              "  1011,\n",
              "  14711,\n",
              "  2040,\n",
              "  3369,\n",
              "  1999,\n",
              "  5978,\n",
              "  2006,\n",
              "  6928,\n",
              "  1010,\n",
              "  3805,\n",
              "  1997,\n",
              "  2037,\n",
              "  3966,\n",
              "  2223,\n",
              "  4284,\n",
              "  1011,\n",
              "  2345,\n",
              "  13249,\n",
              "  2007,\n",
              "  13809,\n",
              "  1012,\n",
              "  102,\n",
              "  101,\n",
              "  21350,\n",
              "  27042,\n",
              "  2098,\n",
              "  21146,\n",
              "  10023,\n",
              "  7559,\n",
              "  29151,\n",
              "  1999,\n",
              "  1996,\n",
              "  2197,\n",
              "  2461,\n",
              "  1998,\n",
              "  2097,\n",
              "  2022,\n",
              "  2559,\n",
              "  2005,\n",
              "  1037,\n",
              "  9377,\n",
              "  2765,\n",
              "  2043,\n",
              "  2027,\n",
              "  2240,\n",
              "  2039,\n",
              "  2006,\n",
              "  9317,\n",
              "  2012,\n",
              "  1996,\n",
              "  14143,\n",
              "  2079,\n",
              "  8011,\n",
              "  7113,\n",
              "  1012,\n",
              "  102,\n",
              "  101,\n",
              "  2383,\n",
              "  13605,\n",
              "  2067,\n",
              "  2013,\n",
              "  2037,\n",
              "  5213,\n",
              "  4154,\n",
              "  2000,\n",
              "  8945,\n",
              "  7946,\n",
              "  8464,\n",
              "  12256,\n",
              "  21043,\n",
              "  27266,\n",
              "  7693,\n",
              "  1010,\n",
              "  27233,\n",
              "  3457,\n",
              "  20282,\n",
              "  2050,\n",
              "  1005,\n",
              "  1055,\n",
              "  2217,\n",
              "  2031,\n",
              "  2180,\n",
              "  2093,\n",
              "  2006,\n",
              "  1996,\n",
              "  19817,\n",
              "  4140,\n",
              "  1998,\n",
              "  2024,\n",
              "  1999,\n",
              "  2204,\n",
              "  2433,\n",
              "  2183,\n",
              "  2046,\n",
              "  1996,\n",
              "  13249,\n",
              "  1012,\n",
              "  102,\n",
              "  101,\n",
              "  1996,\n",
              "  6161,\n",
              "  2248,\n",
              "  2003,\n",
              "  1037,\n",
              "  3145,\n",
              "  2447,\n",
              "  2005,\n",
              "  21350,\n",
              "  1998,\n",
              "  2010,\n",
              "  18601,\n",
              "  18724,\n",
              "  5927,\n",
              "  2032,\n",
              "  2377,\n",
              "  2012,\n",
              "  2440,\n",
              "  2067,\n",
              "  2030,\n",
              "  1999,\n",
              "  23071,\n",
              "  102],\n",
              " [1, 0, 0, 1, 0, 0, 1],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [0, 32, 66, 80, 133, 167, 209],\n",
              " ['david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .',\n",
              "  \"the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders .\",\n",
              "  'on his instagram , he said : ` moving forward !',\n",
              "  'david alaba is looking to get back to fitness for bayern munich now his cast has been removed the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .',\n",
              "  'bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao .',\n",
              "  \"having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash .\",\n",
              "  'the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield'],\n",
              " 'david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield<q>the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R6i4z8ZZk81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
        "b_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
        "                       'src_str': src_txt, \"tgt_str\": tgt_txt}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6t0mNMOsHph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [[b_dict['src'], b_dict['labels'], b_dict['segs'], b_dict['clss'], b_dict['src_txt'], b_dict['tgt_txt']]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wDAqdUytwPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c574a564-7038-46ce-d02d-5fefecbd7598"
      },
      "source": [
        "# Dimensions of input\n",
        "len(data), len(data[0])"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOMCe_2gf8r0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = data_loader.Batch(data, is_test=True, device='cpu') #cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB4DJkg3uACf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d19b4fa5-0e52-4711-c72a-b7a8592d6402"
      },
      "source": [
        "batch.src_str"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .',\n",
              "  \"the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders .\",\n",
              "  'on his instagram , he said : ` moving forward !',\n",
              "  'david alaba is looking to get back to fitness for bayern munich now his cast has been removed the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .',\n",
              "  'bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao .',\n",
              "  \"having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash .\",\n",
              "  'the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwBxbZa_uuVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b3c7c1c1-5473-41c8-9ef5-9857674f3739"
      },
      "source": [
        "batch.tgt_str"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['david alaba is looking to get back to fitness for bayern munich now his cast has been removed<q>david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield<q>the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsuHyhKblBj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36da61c8-dcee-40c7-a8da-8bf9e9629bbf"
      },
      "source": [
        "# device_id = -1 meaning it's using cpu\n",
        "trainer = build_trainer(args, device_id, model, None)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu_rank 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVKotVpWuOvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "52147c1f-c783-4214-a71c-063e5614bc20"
      },
      "source": [
        "trainer.example_api(batch, step, top_n_sentences=2, device='cpu')"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'predicted_summary': ['david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .<q>david alaba is looking to get back to fitness for bayern munich now his cast has been removed the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto .'],\n",
              " 'source_article': [\"david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage . the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders . on his instagram , he said : ` moving forward ! david alaba is looking to get back to fitness for bayern munich now his cast has been removed the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto . bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao . having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash . the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\"]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ENIfJ61z1Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize_text(src_str, args, pp_args, top_n_sentences=3, tgt_str=''):\n",
        "    '''\n",
        "    Summarizes input text by returning the most important sentences based on the BERT model fine-tuned on CNN and Daily Mail articles\n",
        "    '''\n",
        "    #Separate documents into list of sentences\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    src = [sent.split() for sent in nltk.tokenize.sent_tokenize(src_str)]\n",
        "    tgt = [sent.split() for sent in nltk.tokenize.sent_tokenize(tgt_str)]\n",
        "    bert = data_builder.BertData(pp_args)\n",
        "    oracle_ids = data_builder.greedy_selection(src, tgt, 3)\n",
        "    b_data = bert.preprocess(src, tgt, oracle_ids)\n",
        "    indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
        "    b_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids,\n",
        "              'clss': cls_ids, 'src_str': src_txt, \"tgt_str\": tgt_txt}\n",
        "    data = [[b_dict['src'], b_dict['labels'], b_dict['segs'], b_dict['clss'], b_dict['src_str'], b_dict['tgt_str']]]\n",
        "    batch = data_loader.Batch(data, is_test=True, device=device)\n",
        "    trained_model = torch.load(args.model_fp, map_location=lambda storage, loc: storage)\n",
        "    # Read BERT config file\n",
        "    # This contains information about the BERT model (e.g. hidden size for the transformer layers, number of transformer layers, number of self attention heads)\n",
        "    config = BertConfig.from_json_file(args.bert_config_path)\n",
        "    # Instantiate Summarizer and load pretrained model\n",
        "    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "    model.load_cp(trained_model)\n",
        "    device_id = 0 if device == \"cuda\" else -1\n",
        "    trainer = build_trainer(args, device_id, model, None)\n",
        "    results = trainer.example_api(batch, step, top_n_sentences=top_n_sentences, device=device)\n",
        "    return results\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug-XLSbB2t05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "16f2bf6e-5cf8-44d1-a246-a4989dc05ae6"
      },
      "source": [
        "%%time\n",
        "summarize_text(src_str, args, pp_args, tgt_str=tgt_str, top_n_sentences=1)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu_rank 0\n",
            "CPU times: user 1.83 s, sys: 770 ms, total: 2.6 s\n",
            "Wall time: 2.96 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'predicted_summary': ['david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage .'],\n",
              " 'source_article': [\"david alaba has given bayern munich some good news after posting a video on his instagram of the cast being removed following his recent knee ligament damage . the austrian has n't had much luck in terms of injuries but is clearly delighted he can now step-up his recovery and return for the bundesliga leaders . on his instagram , he said : ` moving forward ! david alaba is looking to get back to fitness for bayern munich now his cast has been removed the versatile star had his cast signed by all his team-mates who arrived in portugal on monday , ahead of their champions league quarter-final clash with porto . bayern thrashed shakhtar donetsk in the last round and will be looking for a repeat result when they line up on wednesday at the estadio do dragao . having bounced back from their shock defeat to borussia monchengladbach , pep guardiola 's side have won three on the trot and are in good form going into the clash . the austrian international is a key player for bayern and his versatility sees him play at full back or in midfield\"]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9s_Iub8uYA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tiger_woods_wiki = '''Woods grew up in Orange County, California. He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods. Earl was a single-digit handicap amateur golfer who also was one of the earliest African-American college baseball players at Kansas State University. Tiger's father was a member of the military and had playing privileges at the Navy golf course beside the Joint Forces Training Base in Los Alamitos, which allowed Tiger to play there. Tiger also played at the par 3 Heartwell golf course in Long Beach, as well as some of the municipals in Long Beach. In 1978, Tiger putted against comedian Bob Hope in a television appearance on The Mike Douglas Show. At age three, he shot a 48 over nine holes at the Navy course. At age five, he appeared in Golf Digest and on ABC's That's Incredible! Before turning seven, Tiger won the Under Age 10 section of the Drive, Pitch, and Putt competition, held at the Navy Golf Course in Cypress, California. In 1984 at the age of eight, he won the 9–10 boys' event, the youngest age group available, at the Junior World Golf Championships. He first broke 80 at age eight. He went on to win the Junior World Championships six times, including four consecutive wins from 1988 to 1991. Woods' father Earl wrote that Tiger first defeated him at the age of 11 years, with Earl trying his best. Earl lost to Tiger every time from then on. Woods first broke 70 on a regulation golf course at age 12. When Woods was 13 years old, he played in the 1989 Big I, which was his first major national junior tournament. In the final round, he was paired with pro John Daly, who was then relatively unknown. The event's format placed a professional with each group of juniors who had qualified. Daly birdied three of the last four holes to beat Woods by only one stroke. As a young teenager, Woods first met Jack Nicklaus in Los Angeles at the Bel-Air Country Club, when Nicklaus was performing a clinic for the club's members. Woods was part of the show, and he impressed Nicklaus and the crowd with his skills and potential. Earl Woods had researched in detail the career accomplishments of Nicklaus and had set his young son the goals of breaking those records. Woods was 15 years old and a student at Western High School in Anaheim when he became the youngest U.S. Junior Amateur champion; this was a record that stood until it was broken by Jim Liu in 2010. He was named 1991's Southern California Amateur Player of the Year (for the second consecutive year) and Golf Digest Junior Amateur Player of the Year. In 1992, he defended his title at the U.S. Junior Amateur Championship, becoming the tournament's first two-time winner. He also competed in his first PGA Tour event, the Nissan Los Angeles Open (he missed the 36-hole cut), and was named Golf Digest Amateur Player of the Year, Golf World Player of the Year, and Golfweek National Amateur of the Year. The following year, Woods won his third consecutive U.S. Junior Amateur Championship; he remains the event's only three-time winner. In 1994, at the TPC at Sawgrass in Florida, he became the youngest winner of the U.S. Amateur Championship, a record he held until 2008 when it was broken by Danny Lee. He was a member of the American team at the 1994 Eisenhower Trophy World Amateur Golf Team Championships (winning), and the 1995 Walker Cup (losing). Woods graduated from Western High School at age 18 in 1994 and was voted \"Most Likely to Succeed\" among the graduating class. He had starred for the high school's golf team under coach Don Crosby. Woods overcame difficulties with stuttering as a boy. This was not known until he wrote a letter to a boy who contemplated suicide. Woods wrote, \"I know what it's like to be different and to sometimes not fit in. I also stuttered as a child and I would talk to my dog and he would sit there and listen until he fell asleep. I also took a class for two years to help me, and I finally learned to stop. '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srNZyuJizuC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3a4e858d-00c6-40b0-f752-b22eff500800"
      },
      "source": [
        "tiger_woods_wiki"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Woods grew up in Orange County, California. He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods. Earl was a single-digit handicap amateur golfer who also was one of the earliest African-American college baseball players at Kansas State University. Tiger\\'s father was a member of the military and had playing privileges at the Navy golf course beside the Joint Forces Training Base in Los Alamitos, which allowed Tiger to play there. Tiger also played at the par 3 Heartwell golf course in Long Beach, as well as some of the municipals in Long Beach. In 1978, Tiger putted against comedian Bob Hope in a television appearance on The Mike Douglas Show. At age three, he shot a 48 over nine holes at the Navy course. At age five, he appeared in Golf Digest and on ABC\\'s That\\'s Incredible! Before turning seven, Tiger won the Under Age 10 section of the Drive, Pitch, and Putt competition, held at the Navy Golf Course in Cypress, California. In 1984 at the age of eight, he won the 9–10 boys\\' event, the youngest age group available, at the Junior World Golf Championships. He first broke 80 at age eight. He went on to win the Junior World Championships six times, including four consecutive wins from 1988 to 1991. Woods\\' father Earl wrote that Tiger first defeated him at the age of 11 years, with Earl trying his best. Earl lost to Tiger every time from then on. Woods first broke 70 on a regulation golf course at age 12. When Woods was 13 years old, he played in the 1989 Big I, which was his first major national junior tournament. In the final round, he was paired with pro John Daly, who was then relatively unknown. The event\\'s format placed a professional with each group of juniors who had qualified. Daly birdied three of the last four holes to beat Woods by only one stroke. As a young teenager, Woods first met Jack Nicklaus in Los Angeles at the Bel-Air Country Club, when Nicklaus was performing a clinic for the club\\'s members. Woods was part of the show, and he impressed Nicklaus and the crowd with his skills and potential. Earl Woods had researched in detail the career accomplishments of Nicklaus and had set his young son the goals of breaking those records. Woods was 15 years old and a student at Western High School in Anaheim when he became the youngest U.S. Junior Amateur champion; this was a record that stood until it was broken by Jim Liu in 2010. He was named 1991\\'s Southern California Amateur Player of the Year (for the second consecutive year) and Golf Digest Junior Amateur Player of the Year. In 1992, he defended his title at the U.S. Junior Amateur Championship, becoming the tournament\\'s first two-time winner. He also competed in his first PGA Tour event, the Nissan Los Angeles Open (he missed the 36-hole cut), and was named Golf Digest Amateur Player of the Year, Golf World Player of the Year, and Golfweek National Amateur of the Year. The following year, Woods won his third consecutive U.S. Junior Amateur Championship; he remains the event\\'s only three-time winner. In 1994, at the TPC at Sawgrass in Florida, he became the youngest winner of the U.S. Amateur Championship, a record he held until 2008 when it was broken by Danny Lee. He was a member of the American team at the 1994 Eisenhower Trophy World Amateur Golf Team Championships (winning), and the 1995 Walker Cup (losing). Woods graduated from Western High School at age 18 in 1994 and was voted \"Most Likely to Succeed\" among the graduating class. He had starred for the high school\\'s golf team under coach Don Crosby. Woods overcame difficulties with stuttering as a boy. This was not known until he wrote a letter to a boy who contemplated suicide. Woods wrote, \"I know what it\\'s like to be different and to sometimes not fit in. I also stuttered as a child and I would talk to my dog and he would sit there and listen until he fell asleep. I also took a class for two years to help me, and I finally learned to stop. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKLR9x7Rzv83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "abe75a54-440d-4a8c-db78-04e03265439a"
      },
      "source": [
        "summarize_text(tiger_woods_wiki, args, pp_args, tgt_str='', top_n_sentences=1)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu_rank 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'predicted_summary': ['He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods.'],\n",
              " 'source_article': ['Woods grew up in Orange County, California. He was a child prodigy who was introduced to golf before the age of two by his athletic father, Earl Woods. Earl was a single-digit handicap amateur golfer who also was one of the earliest African-American college baseball players at Kansas State University. Tiger\\'s father was a member of the military and had playing privileges at the Navy golf course beside the Joint Forces Training Base in Los Alamitos, which allowed Tiger to play there. Tiger also played at the par 3 Heartwell golf course in Long Beach, as well as some of the municipals in Long Beach. In 1978, Tiger putted against comedian Bob Hope in a television appearance on The Mike Douglas Show. At age three, he shot a 48 over nine holes at the Navy course. At age five, he appeared in Golf Digest and on ABC\\'s That\\'s Incredible! Before turning seven, Tiger won the Under Age 10 section of the Drive, Pitch, and Putt competition, held at the Navy Golf Course in Cypress, California. In 1984 at the age of eight, he won the 9–10 boys\\' event, the youngest age group available, at the Junior World Golf Championships. He first broke 80 at age eight. He went on to win the Junior World Championships six times, including four consecutive wins from 1988 to 1991. Woods\\' father Earl wrote that Tiger first defeated him at the age of 11 years, with Earl trying his best. Earl lost to Tiger every time from then on. Woods first broke 70 on a regulation golf course at age 12. When Woods was 13 years old, he played in the 1989 Big I, which was his first major national junior tournament. In the final round, he was paired with pro John Daly, who was then relatively unknown. The event\\'s format placed a professional with each group of juniors who had qualified. Daly birdied three of the last four holes to beat Woods by only one stroke. As a young teenager, Woods first met Jack Nicklaus in Los Angeles at the Bel-Air Country Club, when Nicklaus was performing a clinic for the club\\'s members. Woods was part of the show, and he impressed Nicklaus and the crowd with his skills and potential. Earl Woods had researched in detail the career accomplishments of Nicklaus and had set his young son the goals of breaking those records. Woods was 15 years old and a student at Western High School in Anaheim when he became the youngest U.S. Junior Amateur champion; this was a record that stood until it was broken by Jim Liu in 2010. He was named 1991\\'s Southern California Amateur Player of the Year (for the second consecutive year) and Golf Digest Junior Amateur Player of the Year. In 1992, he defended his title at the U.S. Junior Amateur Championship, becoming the tournament\\'s first two-time winner. He also competed in his first PGA Tour event, the Nissan Los Angeles Open (he missed the 36-hole cut), and was named Golf Digest Amateur Player of the Year, Golf World Player of the Year, and Golfweek National Amateur of the Year. The following year, Woods won his third consecutive U.S. Junior Amateur Championship; he remains the event\\'s only three-time winner. In 1994, at the TPC at Sawgrass in Florida, he became the youngest winner of the U.S. Amateur Championship, a record he held until 2008 when it was broken by Danny Lee. He was a member of the American team at the 1994 Eisenhower Trophy World Amateur Golf Team Championships (winning), and the 1995 Walker Cup (losing). Woods graduated from Western High School at age 18 in 1994 and was voted \"Most Likely to Succeed\" among the graduating class. He had starred for the high school\\'s golf team under coach Don Crosby. Woods overcame difficulties with stuttering as a boy. This was not known until he wrote a letter to a boy who contemplated suicide. Woods wrote, \"I know what it\\'s like to be different and to sometimes not fit in. I also stuttered as a child and I would talk to my dog and he would sit there and listen until he fell asleep. I also took a class for two years to help me, and I finally learned to stop.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy8sEU0T3sv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}